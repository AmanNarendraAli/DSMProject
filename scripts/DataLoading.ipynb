{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Dataset: Data Loading and Neo4j Import\n",
                "\n",
                "This notebook processes Yelp dataset JSON files (Business, User, Review, Tip), cleans the data, transforms it into CSV format suitable for Neo4j's `neo4j-admin import` tool, and imports it into a Neo4j database."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup: Imports and Constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core Libraries\n",
                "import json\n",
                "import csv\n",
                "import uuid\n",
                "import os\n",
                "import regex as re\n",
                "import subprocess\n",
                "import datetime\n",
                "import shutil\n",
                "from pathlib import Path\n",
                "from time import sleep\n",
                "from typing import List, Tuple, Set\n",
                "\n",
                "# Data Handling & Graph\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import reverse_geocoder as rg\n",
                "from py2neo import Graph\n",
                "# Import base Neo4jError and specific retryable errors\n",
                "from neo4j.exceptions import ServiceUnavailable, TransientError, Neo4jError"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Constants: Node Labels ---\n",
                "BUSINESS_NODE = \"Business\"\n",
                "USER_NODE = \"User\"\n",
                "REVIEW_NODE = \"Review\"\n",
                "TIP_NODE = \"Tip\" # Added for tips\n",
                "CATEGORY_NODE = \"Category\"\n",
                "CITY_NODE = \"City\"\n",
                "STATE_NODE = \"State\"  # Changed from Area\n",
                "COUNTRY_NODE = \"Country\"\n",
                "\n",
                "# --- Constants: Relationship Types ---\n",
                "IN_CATEGORY = \"IN_CATEGORY\"\n",
                "IN_CITY = \"IN_CITY\"\n",
                "IN_STATE = \"IN_STATE\"    # Changed from IN_AREA\n",
                "IN_COUNTRY = \"IN_COUNTRY\"\n",
                "FRIENDS = \"FRIENDS\"\n",
                "REVIEWS = \"REVIEWS\" # Business <-[:REVIEWS]- Review/Tip\n",
                "WROTE = \"WROTE\"     # User -[:WROTE]-> Review/Tip"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration: File Paths and Neo4j Connection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Input Data Files ---\n",
                "# Adjusted path to point one level up and into 'yelp_dataset'\n",
                "data_folder = \"../cleaned_yelp_data\" \n",
                "business_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_business.json\")\n",
                "review_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_review.json\")\n",
                "user_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_user.json\")\n",
                "tip_json_file = os.path.join(data_folder, \"cleaned_yelp_academic_dataset_tip.json\") # Added tip file\n",
                "\n",
                "# List of raw input files\n",
                "list_raw_files = [business_json_file, review_json_file, user_json_file, tip_json_file] # Added tip file\n",
                "\n",
                "# --- Intermediate Fixed Data Files ---\n",
                "# These will be created in the same directory as the raw files\n",
                "fixed_business_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_business.json\")\n",
                "fixed_review_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_review.json\")\n",
                "fixed_user_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_user.json\")\n",
                "fixed_tip_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_tip.json\") # Added fixed tip file\n",
                "\n",
                "# List of fixed intermediate files\n",
                "list_fixed_data = [fixed_business_json_file, fixed_review_json_file, fixed_user_json_file, fixed_tip_json_file] # Added fixed tip file\n",
                "\n",
                "# --- Output CSV Files for Neo4j Import ---\n",
                "# These will be generated in the current working directory (assumed project root)\n",
                "business_nodes_csv_file = \"business_nodes.csv\"\n",
                "category_nodes_csv_file = \"category_nodes.csv\"\n",
                "city_nodes_csv_file = \"city_nodes.csv\"\n",
                "state_nodes_csv_file = \"state_nodes.csv\"  # Changed from area_nodes.csv\n",
                "country_nodes_csv_file = \"country_nodes.csv\"\n",
                "user_nodes_csv_file = \"user_nodes.csv\"\n",
                "review_nodes_csv_file = \"review_nodes.csv\"\n",
                "tip_nodes_csv_file = \"tip_nodes.csv\" # Added tip nodes file\n",
                "relationship_csv_file = \"relationships.csv\"\n",
                "\n",
                "# List of node CSV files (updated)\n",
                "nodes_files = [\n",
                "    business_nodes_csv_file, category_nodes_csv_file, city_nodes_csv_file, \n",
                "    state_nodes_csv_file, country_nodes_csv_file, user_nodes_csv_file, \n",
                "    review_nodes_csv_file, tip_nodes_csv_file # Added tip nodes file\n",
                "]\n",
                "\n",
                "# --- Neo4j Configuration ---\n",
                "graph_name = \"neo4j\" # Default graph name\n",
                "SERVER_ADDRESS = \"bolt://localhost:7687\"\n",
                "SERVER_AUTH = (\"neo4j\",\"password\") # Replace with your Neo4j credentials\n",
                "\n",
                "# --- Neo4j Installation Path (Manual Configuration Required for Neo4j 5+) ---\n",
                "# !! IMPORTANT !! Set this variable to the root directory of your Neo4j installation.\n",
                "# Example for Windows: neo4j_home = r\"C:\\Users\\YourUser\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-abc12345-def67890\"\n",
                "# Example for Linux/macOS: neo4j_home = \"/path/to/neo4j-community-5.x.x\"\n",
                "neo4j_home = \"C:/Users/wiztu/.Neo4jDesktop/relate-data/dbmss/dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\" \n",
                "\n",
                "if neo4j_home is None:\n",
                "    print(\"WARNING: 'neo4j_home' variable is not set.\")\n",
                "    print(\"         Please edit this cell and set it to your Neo4j installation path.\")\n",
                "    print(\"         The script needs this path to find 'neo4j-admin' and 'neo4j' commands.\")\n",
                "\n",
                "# --- Utility Function ---\n",
                "def delete_files(files: List[str]):\n",
                "    \"\"\"Safely deletes a list of files if they exist.\"\"\"\n",
                "    for one_file in files:\n",
                "        one_path = Path(one_file)\n",
                "        if one_path.is_file():\n",
                "            try:\n",
                "                one_path.unlink()\n",
                "                # print(f\"Deleted file: {one_file}\")\n",
                "            except OSError as e:\n",
                "                print(f\"Error deleting file {one_file}: {e}\")\n",
                "\n",
                "# --- Initial Check: Raw Data Files ---\n",
                "if not all(Path(f).is_file() for f in list_raw_files):\n",
                "    missing = [f for f in list_raw_files if not Path(f).is_file()]\n",
                "    raise FileNotFoundError(f\"Missing raw Yelp JSON files: {missing}. Expected in '{data_folder}'.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing Raw Data\n",
                "\n",
                "**Data Challenges Addressed:**\n",
                "1.  **Non-Unique IDs:** `business_id`, `user_id`, `review_id`, and potentially tip identifiers (if they existed) can overlap across files. Prefixes (`b-`, `u-`, `r-`, `t-`) are added to ensure global uniqueness for Neo4j import. Tips require UUID generation as they lack a source ID.\n",
                "2.  **Dangling Friendships:** Users might list friends who don't exist in the dataset. These non-existent friend references are removed.\n",
                "3.  **Duplicate Categories:** Businesses might list the same category multiple times. Duplicates are removed.\n",
                "4.  **String vs. Array:** `friends` (in User) and `categories` (in Business) are stored as comma-separated strings instead of proper JSON arrays. While fixing IDs/categories, these are handled appropriately."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Preprocessing Functions ---\n",
                "\n",
                "def remove_unknown_friends(raw_user_path: str, output_path: str) -> None:\n",
                "    \"\"\"Reads raw user data, identifies all valid user IDs, \n",
                "       and writes a new file containing only users with valid friend lists.\"\"\"\n",
                "    print(f\"Reading user IDs from {raw_user_path}...\")\n",
                "    user_ids = set()\n",
                "    try:\n",
                "        with open(raw_user_path, \"r\", encoding=\"utf-8\") as rf:\n",
                "            for i, line in enumerate(rf):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        user_ids.add(json_node[\"user_id\"])\n",
                "                    except json.JSONDecodeError:\n",
                "                        print(f\"Warning: Skipping invalid JSON on line {i+1} in {raw_user_path}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {raw_user_path}\")\n",
                "        raise\n",
                "    print(f\"Found {len(user_ids)} unique user IDs.\")\n",
                "\n",
                "    print(f\"Writing users with cleaned friend lists to {output_path}...\")\n",
                "    try:\n",
                "        with open(raw_user_path, \"r\", encoding=\"utf-8\") as rf, open(output_path, mode=\"w\", encoding=\"utf-8\") as of:\n",
                "            for i, line in enumerate(rf):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        friends_str = json_node.get(\"friends\", \"\")\n",
                "                        if friends_str and friends_str.lower() != 'none' and len(friends_str.strip()) > 0:\n",
                "                            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
                "                            # Filter out friends not present in the dataset\n",
                "                            friends_exist_arr = [f for f in friends_arr if f in user_ids]\n",
                "                            json_node[\"friends\"] = ', '.join(friends_exist_arr)\n",
                "                        else:\n",
                "                            json_node[\"friends\"] = \"\" # Ensure field exists but is empty\n",
                "                        of.write(f'{json.dumps(json_node)}\\n')\n",
                "                    except json.JSONDecodeError:\n",
                "                        # Warning already printed during ID collection\n",
                "                        pass \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {raw_user_path} during writing phase.\")\n",
                "        raise\n",
                "    print(\"Finished cleaning friend lists.\")\n",
                "\n",
                "def make_ids_unique(input_path: str, output_path: str, func_to_modify) -> None:\n",
                "    \"\"\"Applies a modification function (e.g., adding ID prefixes) \n",
                "       to each JSON object in the input file and writes to the output file.\"\"\"\n",
                "    print(f\"Applying ID modifications to {input_path} -> {output_path}...\")\n",
                "    temp_output_file = output_path + \".tmp-\" + str(uuid.uuid4())\n",
                "    try:\n",
                "        with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
                "            for i, line in enumerate(in_f):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        json_node = func_to_modify(json_node)\n",
                "                        ou_f.write(f'{json.dumps(json_node)}\\n')\n",
                "                    except json.JSONDecodeError:\n",
                "                         print(f\"Warning: Skipping invalid JSON on line {i+1} in {input_path}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {input_path}\")\n",
                "        # Clean up temp file if it exists\n",
                "        if Path(temp_output_file).is_file():\n",
                "            Path(temp_output_file).unlink()\n",
                "        raise\n",
                "        \n",
                "    # Replace original file with modified temp file\n",
                "    try:\n",
                "        os.replace(temp_output_file, output_path)\n",
                "        print(f\"Successfully updated {output_path}.\")\n",
                "    except OSError as e:\n",
                "        print(f\"Error replacing file {output_path} with {temp_output_file}: {e}\")\n",
                "        # Attempt cleanup again\n",
                "        if Path(temp_output_file).is_file():\n",
                "            try: Path(temp_output_file).unlink()\n",
                "            except OSError: pass\n",
                "        raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fixed data files not found. Starting preprocessing...\n",
                        "Reading user IDs from ../cleaned_yelp_data\\cleaned_yelp_academic_dataset_user.json...\n",
                        "Found 1677963 unique user IDs.\n",
                        "Writing users with cleaned friend lists to ../cleaned_yelp_data\\fixed_yelp_academic_dataset_user.json...\n",
                        "Finished cleaning friend lists.\n",
                        "Applying ID modifications to ../cleaned_yelp_data\\fixed_yelp_academic_dataset_user.json -> ../cleaned_yelp_data\\fixed_yelp_academic_dataset_user.json...\n",
                        "Successfully updated ../cleaned_yelp_data\\fixed_yelp_academic_dataset_user.json.\n",
                        "Applying ID modifications to ../cleaned_yelp_data\\cleaned_yelp_academic_dataset_business.json -> ../cleaned_yelp_data\\fixed_yelp_academic_dataset_business.json...\n",
                        "Successfully updated ../cleaned_yelp_data\\fixed_yelp_academic_dataset_business.json.\n",
                        "Applying ID modifications to ../cleaned_yelp_data\\cleaned_yelp_academic_dataset_review.json -> ../cleaned_yelp_data\\fixed_yelp_academic_dataset_review.json...\n",
                        "Successfully updated ../cleaned_yelp_data\\fixed_yelp_academic_dataset_review.json.\n",
                        "Applying ID modifications to ../cleaned_yelp_data\\cleaned_yelp_academic_dataset_tip.json -> ../cleaned_yelp_data\\fixed_yelp_academic_dataset_tip.json...\n",
                        "Successfully updated ../cleaned_yelp_data\\fixed_yelp_academic_dataset_tip.json.\n",
                        "Preprocessing complete. Fixed data files created.\n"
                    ]
                }
            ],
            "source": [
                "# --- Preprocessing Execution ---\n",
                "\n",
                "# Check if fixed files already exist. If not, create them.\n",
                "if not all(Path(f).is_file() for f in list_fixed_data):\n",
                "    print(\"Fixed data files not found. Starting preprocessing...\")\n",
                "    # Remove any potentially incomplete fixed files first\n",
                "    delete_files(list_fixed_data)\n",
                "\n",
                "    # --- Define Modification Functions ---\n",
                "    def fix_user(json_node):\n",
                "        # Prefix user_id\n",
                "        original_user_id = json_node[\"user_id\"]\n",
                "        json_node[\"user_id\"] = \"u-\" + original_user_id\n",
                "        # Prefix friend_ids (already cleaned in remove_unknown_friends)\n",
                "        friends_str = json_node.get(\"friends\", \"\")\n",
                "        if friends_str and len(friends_str.strip()) > 0:\n",
                "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
                "            # Prefix each valid friend ID\n",
                "            friends_arr = [\"u-\" + f_id for f_id in friends_arr]\n",
                "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
                "        else:\n",
                "             json_node[\"friends\"] = \"\"\n",
                "        return json_node\n",
                "\n",
                "    def fix_business(json_node):\n",
                "        # Prefix business_id\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        # Deduplicate and clean categories\n",
                "        categories_str = json_node.get(\"categories\", None)\n",
                "        if categories_str and len(categories_str.strip()) > 0:\n",
                "            categories_arr = re.split(r\"\\s*,\\s*\", categories_str.strip())\n",
                "            # Filter out empty strings and trim whitespace, then deduplicate\n",
                "            categories_set = set(cat.strip() for cat in categories_arr if cat and len(cat.strip()) > 0)\n",
                "            json_node[\"categories\"] = ', '.join(sorted(list(categories_set))) # Sort for consistency\n",
                "        else:\n",
                "            json_node[\"categories\"] = \"\"\n",
                "        return json_node\n",
                "\n",
                "    def fix_review(json_node):\n",
                "        # Prefix review_id, user_id, business_id\n",
                "        json_node[\"review_id\"] = \"r-\" + json_node[\"review_id\"]\n",
                "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        return json_node\n",
                "    \n",
                "    def fix_tip(json_node):\n",
                "        # Prefix user_id, business_id and generate tip_id\n",
                "        json_node[\"tip_id\"] = \"t-\" + str(uuid.uuid4()) # Generate unique ID for tip\n",
                "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        return json_node\n",
                "\n",
                "    # --- Apply Fixes ---\n",
                "    # 1. Clean user friends (reads raw, writes to fixed path)\n",
                "    remove_unknown_friends(user_json_file, fixed_user_json_file)\n",
                "    # 2. Make user IDs unique (reads fixed, modifies in-place)\n",
                "    make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)\n",
                "    # 3. Make business IDs unique and clean categories (reads raw, writes to fixed path)\n",
                "    make_ids_unique(business_json_file, fixed_business_json_file, fix_business)\n",
                "    # 4. Make review IDs unique (reads raw, writes to fixed path)\n",
                "    make_ids_unique(review_json_file, fixed_review_json_file, fix_review)\n",
                "    # 5. Make tip IDs unique (reads raw, writes to fixed path)\n",
                "    make_ids_unique(tip_json_file, fixed_tip_json_file, fix_tip) # Added tip processing\n",
                "    \n",
                "    print(\"Preprocessing complete. Fixed data files created.\")\n",
                "else:\n",
                "    print(\"Fixed data files already exist. Skipping preprocessing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generating CSV Files for Neo4j Import Tool\n",
                "\n",
                "This section reads the preprocessed (`fixed_...json`) files and generates CSV files formatted according to the requirements of `neo4j-admin import`.\n",
                "- Node files contain headers like `nodeId:ID`, `propertyName`, `:LABEL`.\n",
                "- The relationship file contains headers `:START_ID`, `:END_ID`, `:TYPE`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CSV Generation Functions ---\n",
                "\n",
                "# Global sets to store unique nodes and relationships across functions\n",
                "business_lat_lon = {}\n",
                "state_nodes: Set[Tuple[str, str]] = set() # Changed from area_nodes\n",
                "city_nodes: Set[Tuple[str, str]] = set()\n",
                "country_nodes: Set[str] = set()\n",
                "categories_nodes: Set[str] = set()\n",
                "valid_user_ids: Set[str] = set() # Added to track valid users for tips\n",
                "valid_business_ids: Set[str] = set() # Added to track valid businesses for tips\n",
                "\n",
                "# Relationship sets (tuples: start_id, end_id, type)\n",
                "in_city_relationships: Set[Tuple[str, str, str]] = set()\n",
                "in_state_relationships: Set[Tuple[str, str, str]] = set() # Changed from in_area\n",
                "in_country_relationships: Set[Tuple[str, str, str]] = set()\n",
                "in_category_relationships: Set[Tuple[str, str, str]] = set()\n",
                "friend_relationships: Set[Tuple[str, str, str]] = set()\n",
                "wrote_relationships: Set[Tuple[str, str, str]] = set() # User->Review\n",
                "reviews_relationships: Set[Tuple[str, str, str]] = set() # Review->Business\n",
                "tip_wrote_relationships: Set[Tuple[str, str, str]] = set() # User->Tip (Added)\n",
                "tip_reviews_relationships: Set[Tuple[str, str, str]] = set() # Tip->Business (Added)\n",
                "\n",
                "def process_business_data():\n",
                "    \"\"\"Reads fixed business data, populates business nodes, categories, \n",
                "       lat/lon mapping, IN_CATEGORY relationships, and valid_business_ids set.\"\"\"\n",
                "    print(f\"Processing business data from {fixed_business_json_file}...\")\n",
                "    global categories_nodes, business_lat_lon, in_category_relationships, valid_business_ids\n",
                "    processed_count = 0\n",
                "    valid_business_ids.clear() # Ensure set is empty before processing\n",
                "    try:\n",
                "        with open(fixed_business_json_file, \"r\", encoding=\"utf-8\") as bjf:\n",
                "            # Prepare writer for business nodes CSV\n",
                "            with open(business_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as business_csv:\n",
                "                fieldnames = [\"business_id:ID\", \"name\", \"address\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(business_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in bjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            business_id = json_node[\"business_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Write business node\n",
                "                            writer.writerow({\n",
                "                                \"business_id:ID\": business_id,\n",
                "                                \"name\": json_node.get(\"name\", \"\"),\n",
                "                                \"address\": json_node.get(\"address\", \"\"),\n",
                "                                \":LABEL\": BUSINESS_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Add to valid IDs set\n",
                "                            valid_business_ids.add(business_id)\n",
                "                            \n",
                "                            # Store lat/lon for location processing\n",
                "                            if json_node.get(\"latitude\") is not None and json_node.get(\"longitude\") is not None:\n",
                "                                business_lat_lon[business_id] = (json_node[\"latitude\"], json_node[\"longitude\"])\n",
                "                            \n",
                "                            # Process categories\n",
                "                            categories_str = json_node.get(\"categories\", \"\")\n",
                "                            if categories_str and len(categories_str.strip()) > 0:\n",
                "                                cur_categories = re.split(r\"\\s*,\\s*\", categories_str.strip()) # Already cleaned/deduplicated\n",
                "                                categories_nodes.update(cur_categories)\n",
                "                                for category in cur_categories:\n",
                "                                    in_category_relationships.add((business_id, category, IN_CATEGORY))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_business_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in business record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_business_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} businesses. Found {len(valid_business_ids)} valid business IDs.\")\n",
                "\n",
                "def process_location_data():\n",
                "    \"\"\"Uses reverse_geocoder on business lat/lons to create City, State, Country nodes \n",
                "       and IN_CITY, IN_STATE, IN_COUNTRY relationships.\"\"\"\n",
                "    print(\"Processing location data using reverse geocoding...\")\n",
                "    global city_nodes, state_nodes, country_nodes, in_city_relationships, in_state_relationships, in_country_relationships\n",
                "    \n",
                "    if not business_lat_lon:\n",
                "        print(\"Warning: No business latitude/longitude data found. Skipping location processing.\")\n",
                "        return\n",
                "        \n",
                "    # Prepare coordinates for batch reverse geocoding\n",
                "    business_ids = list(business_lat_lon.keys())\n",
                "    coordinates = list(business_lat_lon.values())\n",
                "    \n",
                "    print(f\"Performing reverse geocoding for {len(coordinates)} coordinates...\")\n",
                "    try:\n",
                "        location_results = rg.search(coordinates)\n",
                "    except Exception as e:\n",
                "        print(f\"Error during reverse geocoding: {e}\")\n",
                "        print(\"Ensure 'reverse_geocoder' library and its data are installed correctly.\")\n",
                "        raise\n",
                "        \n",
                "    print(\"Processing geocoding results...\")\n",
                "    # Process results and build node/relationship sets\n",
                "    for business_id, loc_info in zip(business_ids, location_results):\n",
                "        city = loc_info.get('name', 'UnknownCity')\n",
                "        state = loc_info.get('admin1', 'UnknownState') # admin1 is typically state/province\n",
                "        country = loc_info.get('cc', 'UnknownCountry') # cc is country code\n",
                "        \n",
                "        # Create unique IDs\n",
                "        # Ensure IDs don't clash if names are identical across different levels\n",
                "        country_id = country\n",
                "        state_id = f\"{state}-{country_id}\"\n",
                "        city_id = f\"{city}-{state_id}\"\n",
                "        \n",
                "        # Add nodes (sets handle uniqueness)\n",
                "        country_nodes.add(country_id)\n",
                "        state_nodes.add((state_id, state))\n",
                "        city_nodes.add((city_id, city))\n",
                "        \n",
                "        # Add relationships (sets handle uniqueness)\n",
                "        in_city_relationships.add((business_id, city_id, IN_CITY))\n",
                "        in_state_relationships.add((city_id, state_id, IN_STATE))\n",
                "        in_country_relationships.add((state_id, country_id, IN_COUNTRY))\n",
                "        \n",
                "    print(f\"Finished processing locations: {len(city_nodes)} cities, {len(state_nodes)} states, {len(country_nodes)} countries.\")\n",
                "\n",
                "def process_user_data():\n",
                "    \"\"\"Reads fixed user data, populates user nodes, FRIEND relationships, and valid_user_ids set.\"\"\"\n",
                "    print(f\"Processing user data from {fixed_user_json_file}...\")\n",
                "    global friend_relationships, valid_user_ids\n",
                "    processed_count = 0\n",
                "    valid_user_ids.clear() # Ensure set is empty before processing\n",
                "    try:\n",
                "        with open(fixed_user_json_file, \"r\", encoding=\"utf-8\") as ujf:\n",
                "            # Prepare writer for user nodes CSV\n",
                "            with open(user_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as user_csv:\n",
                "                fieldnames = [\"user_id:ID\", \"name\", \"yelping_since\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(user_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in ujf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            user_id = json_node[\"user_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Write user node\n",
                "                            writer.writerow({\n",
                "                                \"user_id:ID\": user_id,\n",
                "                                \"name\": json_node.get(\"name\", \"\"),\n",
                "                                \"yelping_since\": json_node.get(\"yelping_since\", \"\"),\n",
                "                                \":LABEL\": USER_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Add to valid IDs set\n",
                "                            valid_user_ids.add(user_id)\n",
                "                            \n",
                "                            # Process friends\n",
                "                            friends_str = json_node.get(\"friends\", \"\")\n",
                "                            if friends_str and len(friends_str.strip()) > 0:\n",
                "                                friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip()) # Already cleaned and prefixed\n",
                "                                for friend_id in friends_arr:\n",
                "                                    # Ensure relationship uniqueness (u1 < u2)\n",
                "                                    u1 = min(user_id, friend_id)\n",
                "                                    u2 = max(user_id, friend_id)\n",
                "                                    if u1 != u2: # Avoid self-loops if data error exists\n",
                "                                        friend_relationships.add((u1, u2, FRIENDS))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_user_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in user record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_user_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} users. Found {len(valid_user_ids)} valid user IDs.\")\n",
                "\n",
                "def process_review_data():\n",
                "    \"\"\"Reads fixed review data, populates review nodes, WROTE and REVIEWS relationships.\"\"\"\n",
                "    print(f\"Processing review data from {fixed_review_json_file}...\")\n",
                "    global wrote_relationships, reviews_relationships\n",
                "    processed_count = 0\n",
                "    try:\n",
                "        with open(fixed_review_json_file, \"r\", encoding=\"utf-8\") as rjf:\n",
                "            # Prepare writer for review nodes CSV\n",
                "            with open(review_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as review_csv:\n",
                "                # Note: 'text' can contain quotes/commas, ensure proper quoting\n",
                "                fieldnames = [\"review_id:ID\", \"stars\", \"date\", \"text\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(review_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL) # QUOTE_ALL for safety\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in rjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            review_id = json_node[\"review_id\"] # Already prefixed\n",
                "                            user_id = json_node[\"user_id\"]     # Already prefixed\n",
                "                            business_id = json_node[\"business_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Write review node\n",
                "                            writer.writerow({\n",
                "                                \"review_id:ID\": review_id,\n",
                "                                \"stars\": json_node.get(\"stars\", 0),\n",
                "                                \"date\": json_node.get(\"date\", \"\"),\n",
                "                                \"text\": json_node.get(\"text\", \"\"),\n",
                "                                \":LABEL\": REVIEW_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Add relationships\n",
                "                            wrote_relationships.add((user_id, review_id, WROTE))\n",
                "                            reviews_relationships.add((review_id, business_id, REVIEWS))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_review_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in review record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_review_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} reviews.\")\n",
                "\n",
                "def process_tip_data():\n",
                "    \"\"\"Reads fixed tip data, populates tip nodes, WROTE and REVIEWS relationships, \n",
                "       only including tips where user and business exist.\"\"\"\n",
                "    print(f\"Processing tip data from {fixed_tip_json_file}...\")\n",
                "    global tip_wrote_relationships, tip_reviews_relationships\n",
                "    processed_count = 0\n",
                "    skipped_count = 0\n",
                "    \n",
                "    # Ensure valid ID sets are populated before running this\n",
                "    if not valid_user_ids or not valid_business_ids:\n",
                "        raise RuntimeError(\"Valid user and business ID sets are empty. Ensure process_user_data() and process_business_data() run first.\")\n",
                "        \n",
                "    try:\n",
                "        with open(fixed_tip_json_file, \"r\", encoding=\"utf-8\") as tjf:\n",
                "            # Prepare writer for tip nodes CSV\n",
                "            with open(tip_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as tip_csv:\n",
                "                fieldnames = [\"tip_id:ID\", \"text\", \"date\", \"compliment_count\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(tip_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL) # QUOTE_ALL for safety\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in tjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            tip_id = json_node[\"tip_id\"]         # Already generated & prefixed\n",
                "                            user_id = json_node[\"user_id\"]       # Already prefixed\n",
                "                            business_id = json_node[\"business_id\"] # Already prefixed\n",
                "                            \n",
                "                            # --- Validation Check ---\n",
                "                            if user_id in valid_user_ids and business_id in valid_business_ids:\n",
                "                                # Write tip node\n",
                "                                writer.writerow({\n",
                "                                    \"tip_id:ID\": tip_id,\n",
                "                                    \"text\": json_node.get(\"text\", \"\"),\n",
                "                                    \"date\": json_node.get(\"date\", \"\"),\n",
                "                                    \"compliment_count\": json_node.get(\"compliment_count\", 0),\n",
                "                                    \":LABEL\": TIP_NODE\n",
                "                                })\n",
                "                                \n",
                "                                # Add relationships\n",
                "                                tip_wrote_relationships.add((user_id, tip_id, WROTE))\n",
                "                                tip_reviews_relationships.add((tip_id, business_id, REVIEWS))\n",
                "                                processed_count += 1\n",
                "                            else:\n",
                "                                skipped_count += 1\n",
                "                                # Optionally log skipped tips if needed for debugging\n",
                "                                # print(f\"Skipping tip {tip_id} due to missing user ({user_id not in valid_user_ids}) or business ({business_id not in valid_business_ids})\")\n",
                "                                \n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_tip_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in tip record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_tip_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing tips. Added {processed_count} tips, skipped {skipped_count} due to missing user/business.\")\n",
                "\n",
                "def write_category_nodes_to_file():\n",
                "    \"\"\"Writes the collected unique category nodes to a CSV file.\"\"\"\n",
                "    print(f\"Writing {len(categories_nodes)} category nodes to {category_nodes_csv_file}...\")\n",
                "    with open(category_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as category_csv:\n",
                "        fieldnames = [\"category_id:ID\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(category_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for category_id in sorted(list(categories_nodes)):\n",
                "            writer.writerow({\"category_id:ID\": category_id, \":LABEL\": CATEGORY_NODE})\n",
                "    print(\"Finished writing category nodes.\")\n",
                "\n",
                "def write_location_nodes_to_file():\n",
                "    \"\"\"Writes the collected unique City, State, Country nodes to CSV files.\"\"\"\n",
                "    # Write City Nodes\n",
                "    print(f\"Writing {len(city_nodes)} city nodes to {city_nodes_csv_file}...\")\n",
                "    with open(city_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as city_csv:\n",
                "        fieldnames = [\"city_id:ID\", \"name\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(city_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for city_id, city_name in sorted(list(city_nodes)):\n",
                "            writer.writerow({\"city_id:ID\": city_id, \"name\": city_name, \":LABEL\": CITY_NODE})\n",
                "            \n",
                "    # Write State Nodes (Changed from Area)\n",
                "    print(f\"Writing {len(state_nodes)} state nodes to {state_nodes_csv_file}...\")\n",
                "    with open(state_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as state_csv:\n",
                "        fieldnames = [\"state_id:ID\", \"name\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(state_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for state_id, state_name in sorted(list(state_nodes)):\n",
                "            writer.writerow({\"state_id:ID\": state_id, \"name\": state_name, \":LABEL\": STATE_NODE})\n",
                "            \n",
                "    # Write Country Nodes\n",
                "    print(f\"Writing {len(country_nodes)} country nodes to {country_nodes_csv_file}...\")\n",
                "    with open(country_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as country_csv:\n",
                "        fieldnames = [\"country_id:ID\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(country_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for country_id in sorted(list(country_nodes)):\n",
                "            writer.writerow({\"country_id:ID\": country_id, \":LABEL\": COUNTRY_NODE})\n",
                "    print(\"Finished writing location nodes.\")\n",
                "\n",
                "def write_relationships_to_file():\n",
                "    \"\"\"Writes all collected relationships to a single CSV file.\"\"\"\n",
                "    all_relationships = (\n",
                "        in_category_relationships | in_city_relationships | \n",
                "        in_state_relationships | in_country_relationships | \n",
                "        friend_relationships | wrote_relationships | reviews_relationships | \n",
                "        tip_wrote_relationships | tip_reviews_relationships # Added tip relationships\n",
                "    )\n",
                "    print(f\"Writing {len(all_relationships)} relationships to {relationship_csv_file}...\")\n",
                "    with open(relationship_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as rel_csv:\n",
                "        fieldnames = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
                "        writer = csv.DictWriter(rel_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        # Sort for deterministic output (optional but good practice)\n",
                "        for start_id, end_id, rel_type in sorted(list(all_relationships)):\n",
                "            writer.writerow({\":START_ID\": start_id, \":END_ID\": end_id, \":TYPE\": rel_type})\n",
                "    print(\"Finished writing relationships.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Import CSV files not found or incomplete. Starting generation...\n",
                        "Processing user data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_user.json...\n",
                        "Finished processing 1677963 users. Found 1677963 valid user IDs.\n",
                        "Processing business data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_business.json...\n",
                        "Finished processing 68696 businesses. Found 68696 valid business IDs.\n",
                        "Processing location data using reverse geocoding...\n",
                        "Performing reverse geocoding for 68696 coordinates...\n",
                        "Loading formatted geocoded file...\n",
                        "Processing geocoding results...\n",
                        "Finished processing locations: 620 cities, 14 states, 2 countries.\n",
                        "Processing review data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_review.json...\n",
                        "Finished processing 5084370 reviews.\n",
                        "Processing tip data from ../cleaned_yelp_data\\fixed_yelp_academic_dataset_tip.json...\n",
                        "Finished processing tips. Added 741599 tips, skipped 0 due to missing user/business.\n",
                        "Writing 878 category nodes to category_nodes.csv...\n",
                        "Finished writing category nodes.\n",
                        "Writing 620 city nodes to city_nodes.csv...\n",
                        "Writing 14 state nodes to state_nodes.csv...\n",
                        "Writing 2 country nodes to country_nodes.csv...\n",
                        "Finished writing location nodes.\n",
                        "Writing 19333704 relationships to relationships.csv...\n",
                        "Finished writing relationships.\n",
                        "CSV file generation complete.\n"
                    ]
                }
            ],
            "source": [
                "# --- CSV Generation Execution ---\n",
                "\n",
                "# Check if CSV files already exist. If not, generate them.\n",
                "if not all(Path(f).is_file() for f in nodes_files) or not Path(relationship_csv_file).is_file():\n",
                "    print(\"Import CSV files not found or incomplete. Starting generation...\")\n",
                "    # Delete any existing CSV files first\n",
                "    delete_files(nodes_files) # Now includes tip_nodes_csv_file\n",
                "    delete_files([relationship_csv_file])\n",
                "    \n",
                "    # Process data and populate node/relationship sets\n",
                "    # IMPORTANT: Process Users and Businesses FIRST to populate valid ID sets\n",
                "    process_user_data() \n",
                "    process_business_data()\n",
                "    process_location_data() # Depends on business_lat_lon\n",
                "    process_review_data()\n",
                "    process_tip_data() # Process tips AFTER users/businesses\n",
                "    \n",
                "    # Write node sets to their respective CSV files\n",
                "    write_category_nodes_to_file()\n",
                "    write_location_nodes_to_file()\n",
                "    # Business, User, Review, Tip nodes written during processing steps\n",
                "    \n",
                "    # Write all relationships to the relationship CSV file\n",
                "    write_relationships_to_file()\n",
                "    \n",
                "    # Optional: Clean up large intermediate sets to free memory\n",
                "    del business_lat_lon, categories_nodes, city_nodes, state_nodes, country_nodes\n",
                "    del valid_user_ids, valid_business_ids # Added cleanup\n",
                "    del in_category_relationships, in_city_relationships, in_state_relationships, in_country_relationships\n",
                "    del friend_relationships, wrote_relationships, reviews_relationships\n",
                "    del tip_wrote_relationships, tip_reviews_relationships # Added cleanup\n",
                "    \n",
                "    print(\"CSV file generation complete.\")\n",
                "else:\n",
                "    print(\"Import CSV files already exist. Skipping generation.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Integrity Check\n",
                "\n",
                "Verify basic integrity of the generated CSV files before attempting import:\n",
                "- Check for duplicate IDs within each node file.\n",
                "- Check for duplicate relationships (same start, end, and type)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking integrity of generated CSV files...\n",
                        "  Checking node file: business_nodes.csv...\n",
                        "    Found 68696 unique nodes.\n",
                        "  Checking node file: category_nodes.csv...\n",
                        "    Found 878 unique nodes.\n",
                        "  Checking node file: city_nodes.csv...\n",
                        "    Found 620 unique nodes.\n",
                        "  Checking node file: state_nodes.csv...\n",
                        "    Found 14 unique nodes.\n",
                        "  Checking node file: country_nodes.csv...\n",
                        "    Found 2 unique nodes.\n",
                        "  Checking node file: user_nodes.csv...\n",
                        "    Found 1677963 unique nodes.\n",
                        "  Checking node file: review_nodes.csv...\n",
                        "    Found 5084370 unique nodes.\n",
                        "  Checking node file: tip_nodes.csv...\n",
                        "    Found 741599 unique nodes.\n",
                        "  Checking relationship file: relationships.csv...\n",
                        "    Found 19333704 unique relationships.\n",
                        "Integrity check passed. Total unique nodes expected: 7574142\n"
                    ]
                }
            ],
            "source": [
                "num_nodes_total = 0\n",
                "\n",
                "def check_nodes_relationships_csv_files_integrity():\n",
                "    \"\"\"Validates uniqueness of node IDs and relationships in generated CSVs.\"\"\"\n",
                "    global num_nodes_total\n",
                "    num_nodes_total = 0 # Reset count\n",
                "    print(\"Checking integrity of generated CSV files...\")\n",
                "    \n",
                "    # Check Node Files\n",
                "    for one_node_file in nodes_files:\n",
                "        print(f\"  Checking node file: {one_node_file}...\")\n",
                "        try:\n",
                "            temp_df = pd.read_csv(one_node_file, header=\"infer\", sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
                "            ids = temp_df.iloc[:, 0] # First column should be the ID (e.g., 'node_id:ID')\n",
                "            if len(ids) != len(ids.unique()):\n",
                "                duplicates = ids[ids.duplicated()].unique()\n",
                "                raise ValueError(f\"Duplicate node IDs found in [{one_node_file}]: {list(duplicates)[:5]}...\")\n",
                "            num_nodes_total += len(ids)\n",
                "            print(f\"    Found {len(ids)} unique nodes.\")\n",
                "        except FileNotFoundError:\n",
                "            print(f\"Error: Node file not found: {one_node_file}\")\n",
                "            raise\n",
                "        except Exception as e:\n",
                "            print(f\"Error checking node file {one_node_file}: {e}\")\n",
                "            raise\n",
                "            \n",
                "    # Check Relationship File\n",
                "    print(f\"  Checking relationship file: {relationship_csv_file}...\")\n",
                "    try:\n",
                "        temp_df = pd.read_csv(relationship_csv_file, header=\"infer\", sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
                "        # Combine start, end, type for uniqueness check\n",
                "        # Ensure columns are treated as strings to avoid type issues\n",
                "        rel_signatures = temp_df.iloc[:, 0].astype(str) + \"|\" + temp_df.iloc[:, 1].astype(str) + \"|\" + temp_df.iloc[:, 2].astype(str)\n",
                "        if len(rel_signatures) != len(rel_signatures.unique()):\n",
                "            duplicates = rel_signatures[rel_signatures.duplicated()].unique()\n",
                "            raise ValueError(f\"Duplicate relationships found in [{relationship_csv_file}]: {list(duplicates)[:5]}...\")\n",
                "        print(f\"    Found {len(rel_signatures)} unique relationships.\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: Relationship file not found: {relationship_csv_file}\")\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        print(f\"Error checking relationship file {relationship_csv_file}: {e}\")\n",
                "        raise\n",
                "        \n",
                "    print(f\"Integrity check passed. Total unique nodes expected: {num_nodes_total}\")\n",
                "\n",
                "# Run the check\n",
                "try:\n",
                "    check_nodes_relationships_csv_files_integrity()\n",
                "except Exception as e:\n",
                "    print(f\"\\n--- DATA INTEGRITY CHECK FAILED ---: {e}\")\n",
                "    print(\"Import aborted. Please check the CSV generation process and input data.\")\n",
                "    # Optionally raise to stop notebook execution\n",
                "    # raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Importing Data into Neo4j\n",
                "\n",
                "This section uses the `neo4j-admin import` tool to load the generated CSV files into the Neo4j database. \n",
                "\n",
                "**Process:**\n",
                "1.  Stop the Neo4j Database Service.\n",
                "2.  Delete the existing database files (effectively resetting the graph).\n",
                "3.  Run `neo4j-admin import` using the generated CSV files.\n",
                "4.  Start the Neo4j Database Service.\n",
                "5.  Verify the import by checking the node count.\n",
                "\n",
                "**Important Notes:**\n",
                "- Requires `neo4j_home` to be correctly identified earlier.\n",
                "- Requires sufficient permissions to stop/start the Neo4j service and modify its data directories.\n",
                "- This will **completely overwrite** the target Neo4j database (`graph_name`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Neo4j Import Functions ---\n",
                "\n",
                "def command_neo4j_database_service(cmd: str):\n",
                "    \"\"\"Executes start/stop commands for the Neo4j service.\"\"\"\n",
                "    if neo4j_home is None or not Path(neo4j_home).is_dir():\n",
                "        raise ValueError(\"Neo4j home directory ('neo4j_home') is not set or invalid. Please configure it in cell [2].\")\n",
                "        \n",
                "    neo4j_cmd_path = os.path.join(neo4j_home, \"bin\", \"neo4j.bat\") # Assuming Windows .bat\n",
                "    if not Path(neo4j_cmd_path).is_file():\n",
                "         neo4j_cmd_path = os.path.join(neo4j_home, \"bin\", \"neo4j\") # Try Linux/macOS script\n",
                "         if not Path(neo4j_cmd_path).is_file():\n",
                "             raise FileNotFoundError(f\"Neo4j command script not found in {os.path.join(neo4j_home, 'bin')}\")\n",
                "             \n",
                "    if cmd in [\"stop\", \"start\"]:\n",
                "        print(f\"Attempting to {cmd} Neo4j service...\")\n",
                "        try:\n",
                "            # Use shell=True cautiously, ensure neo4j_cmd_path is safe.\n",
                "            # Capture output to check success.\n",
                "            cmd_res = subprocess.run([neo4j_cmd_path, cmd], capture_output=True, text=True, check=False, shell=True)\n",
                "            \n",
                "            print(f\"  STDOUT: {cmd_res.stdout.strip()}\")\n",
                "            print(f\"  STDERR: {cmd_res.stderr.strip()}\")\n",
                "            \n",
                "            # Check output for success indicators (these might vary slightly by Neo4j version/OS)\n",
                "            if cmd == \"stop\":\n",
                "                # Check if it's already stopped or successfully stopped\n",
                "                if \"stopped\" not in cmd_res.stdout.lower() and \"not running\" not in cmd_res.stderr.lower():\n",
                "                     # If stop failed, maybe it needs more time or manual intervention\n",
                "                     print(f\"Warning: Neo4j stop command finished, but output doesn't confirm stop. Check service status manually.\")\n",
                "                     # Allow script to continue, but import might fail if service still running\n",
                "                else:\n",
                "                    print(\"Neo4j service appears stopped.\")\n",
                "                    sleep(5) # Give some time for files to be released\n",
                "            elif cmd == \"start\":\n",
                "                if \"started\" not in cmd_res.stdout.lower():\n",
                "                    raise RuntimeError(f\"Failed to start Neo4j service. Check logs. STDERR: {cmd_res.stderr.strip()}\")\n",
                "                else:\n",
                "                    print(\"Neo4j service appears started. Waiting for it to become available...\")\n",
                "                    sleep(15) # Give Neo4j time to initialize\n",
                "                    \n",
                "        except FileNotFoundError:\n",
                "            raise FileNotFoundError(f\"Neo4j command '{neo4j_cmd_path}' not found.\")\n",
                "        except subprocess.CalledProcessError as e:\n",
                "            raise RuntimeError(f\"Error executing Neo4j {cmd} command: {e}. STDOUT: {e.stdout}. STDERR: {e.stderr}\")\n",
                "        except Exception as e:\n",
                "            raise RuntimeError(f\"An unexpected error occurred during Neo4j {cmd}: {e}\")\n",
                "    else:\n",
                "        raise ValueError(f'Unknown command for Neo4j service: [{cmd}]')\n",
                "\n",
                "def reset_neo4j_database():\n",
                "    \"\"\"Deletes the data directories for the target graph.\"\"\"\n",
                "    if neo4j_home is None or not Path(neo4j_home).is_dir():\n",
                "        raise ValueError(\"Neo4j home directory ('neo4j_home') is not set or invalid. Please configure it in cell [2].\")\n",
                "        \n",
                "    # Construct paths based on standard Neo4j directory structure\n",
                "    neo4j_db_dir = Path(neo4j_home) / \"data\" / \"databases\" / graph_name\n",
                "    neo4j_tx_dir = Path(neo4j_home) / \"data\" / \"transactions\" / graph_name\n",
                "    \n",
                "    print(f\"Attempting to delete database directory: {neo4j_db_dir}\")\n",
                "    if neo4j_db_dir.exists() and neo4j_db_dir.is_dir():\n",
                "        try:\n",
                "            shutil.rmtree(neo4j_db_dir)\n",
                "            print(\"  Database directory deleted.\")\n",
                "        except OSError as e:\n",
                "            raise OSError(f\"Error deleting database directory {neo4j_db_dir}: {e}. Check permissions and ensure Neo4j is stopped.\")\n",
                "    else:\n",
                "        print(\"  Database directory does not exist, skipping deletion.\")\n",
                "        \n",
                "    print(f\"Attempting to delete transactions directory: {neo4j_tx_dir}\")\n",
                "    if neo4j_tx_dir.exists() and neo4j_tx_dir.is_dir():\n",
                "        try:\n",
                "            shutil.rmtree(neo4j_tx_dir)\n",
                "            print(\"  Transactions directory deleted.\")\n",
                "        except OSError as e:\n",
                "            # This might sometimes fail if logs are held; often okay if db dir was removed.\n",
                "            print(f\"Warning: Could not delete transactions directory {neo4j_tx_dir}: {e}. Import might still succeed.\")\n",
                "    else:\n",
                "        print(\"  Transactions directory does not exist, skipping deletion.\")\n",
                "\n",
                "import os\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Assume neo4j_home, graph_name, nodes_files, relationship_csv_file are defined correctly\n",
                "\n",
                "def import_data():\n",
                "    # Ensure neo4j_home is valid before proceeding\n",
                "    if not neo4j_home or not Path(neo4j_home).is_dir():\n",
                "         raise ValueError(f\"Neo4j home directory ('neo4j_home') is not set or invalid: {neo4j_home}\")\n",
                "\n",
                "    import_tool = Path(neo4j_home) / \"bin\" / (\"neo4j-admin.bat\" if os.name == \"nt\" else \"neo4j-admin\")\n",
                "    if not import_tool.is_file():\n",
                "        raise FileNotFoundError(f\"neo4j-admin executable not found at: {import_tool}\")\n",
                "\n",
                "    csv_dir = Path.cwd() # where the notebook generated the CSVs\n",
                "\n",
                "    args = [\n",
                "        str(import_tool),\n",
                "        \"database\", \"import\", \"full\",\n",
                "        # Removed --database=graph_name from here\n",
                "        \"--overwrite-destination=true\",\n",
                "        \"--multiline-fields=true\",       # Keep this if your text fields might have newlines\n",
                "        \"--skip-duplicate-nodes=true\",\n",
                "        \"--skip-bad-relationships=true\",\n",
                "        # Add other flags as needed, BEFORE nodes/relationships\n",
                "    ]\n",
                "\n",
                "    # Add each node file with an **absolute** path\n",
                "    for f in nodes_files:\n",
                "        # Ensure the path is absolute and correctly formatted for the OS\n",
                "        node_file_path = (csv_dir / f).resolve()\n",
                "        args.append(f\"--nodes={node_file_path}\")\n",
                "\n",
                "    # Add the relationship file with an **absolute** path\n",
                "    rel_file_path = (csv_dir / relationship_csv_file).resolve()\n",
                "    args.append(f\"--relationships={rel_file_path}\")\n",
                "\n",
                "    # --- FIX ---\n",
                "    # Add the database name as the VERY LAST positional argument\n",
                "    # args.append(graph_name)\n",
                "    # --- END FIX ---\n",
                "\n",
                "    print(\"Running:\", \" \".join(map(str, args))) # Use map(str,..) in case paths are Path objects\n",
                "\n",
                "    env = os.environ.copy()\n",
                "    # Ensure JAVA_HOME is correctly set if needed by your environment/neo4j-admin script\n",
                "    # Double-check this path is correct for your system where the JDK is installed\n",
                "    java_home_path = \"C:/Program Files/Eclipse Adoptium/jdk-21.0.7.6-hotspot\"\n",
                "    if Path(java_home_path).is_dir():\n",
                "         env[\"JAVA_HOME\"] = java_home_path\n",
                "         print(f\"Setting JAVA_HOME to: {java_home_path}\")\n",
                "    else:\n",
                "         print(f\"Warning: Specified JAVA_HOME path not found: {java_home_path}. neo4j-admin might fail.\")\n",
                "         # You might remove the JAVA_HOME setting if neo4j-admin finds it automatically,\n",
                "         # but explicitly setting it is often more reliable if needed.\n",
                "\n",
                "    # Use shell=False (safer and often handles arguments better)\n",
                "    # Ensure neo4j service is STOPPED before running this\n",
                "    try:\n",
                "        res = subprocess.run(args, text=True, capture_output=True, env=env, check=False, shell=False) # Use check=False to manually check returncode\n",
                "\n",
                "        if res.returncode != 0:\n",
                "            # Print command again for easier debugging on failure\n",
                "            print(\"\\n--- FAILED COMMAND ---\")\n",
                "            print(\" \".join(map(str, args)))\n",
                "            print(\"----------------------\\n\")\n",
                "            raise RuntimeError(f\"Import failed ({res.returncode})\\nSTDOUT:\\n{res.stdout}\\nSTDERR:\\n{res.stderr}\")\n",
                "        else:\n",
                "             print(\"\\n--- Import Command Output ---\")\n",
                "             print(res.stdout)\n",
                "             print(\"---------------------------\\n\")\n",
                "             if res.stderr: # Print stderr even on success, might contain warnings\n",
                "                 print(\"--- Import Command STDERR (Warnings/Info) ---\")\n",
                "                 print(res.stderr)\n",
                "                 print(\"---------------------------------------------\\n\")\n",
                "\n",
                "    except FileNotFoundError as fnf_error:\n",
                "         print(f\"Error: Command not found. Ensure neo4j-admin path is correct. Details: {fnf_error}\")\n",
                "         raise\n",
                "    except Exception as e:\n",
                "         print(f\"An unexpected error occurred running the import command: {e}\")\n",
                "         raise # Re-raise the caught exception"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking current graph 'neo4j' node count...\n",
                        "An unexpected error occurred checking node count: Cannot open connection to ConnectionProfile('bolt://localhost:7687'). Assuming import is needed.\n",
                        "\n",
                        "--- Starting Neo4j Import Process ---\n",
                        "Attempting to delete database directory: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\databases\\neo4j\n",
                        "  Database directory deleted.\n",
                        "Attempting to delete transactions directory: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\transactions\\neo4j\n",
                        "  Transactions directory deleted.\n",
                        "Running: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\bin\\neo4j-admin.bat database import full --overwrite-destination=true --multiline-fields=true --skip-duplicate-nodes=true --skip-bad-relationships=true --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\business_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\category_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\city_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\state_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\country_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\user_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\review_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\tip_nodes.csv --relationships=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\relationships.csv\n",
                        "Setting JAVA_HOME to: C:/Program Files/Eclipse Adoptium/jdk-21.0.7.6-hotspot\n",
                        "\n",
                        "--- Import Command Output ---\n",
                        "Neo4j version: 5.24.0\n",
                        "Importing the contents of these files into C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\databases\\neo4j:\n",
                        "Nodes:\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\business_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\category_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\city_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\state_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\country_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\user_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\review_nodes.csv\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\tip_nodes.csv\n",
                        "\n",
                        "Relationships:\n",
                        "  C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\relationships.csv\n",
                        "\n",
                        "\n",
                        "Available resources:\n",
                        "  Total machine memory: 15.84GiB\n",
                        "  Free machine memory: 8.857GiB\n",
                        "  Max heap memory : 910.5MiB\n",
                        "  Max worker threads: 8\n",
                        "  Configured max memory: 7.334GiB\n",
                        "  High parallel IO: true\n",
                        "\n",
                        "Import starting\n",
                        "  Page cache size: 1.992GiB\n",
                        "  Number of worker threads: 8\n",
                        "  Estimated number of nodes: 7.635.327\n",
                        "  Estimated number of relationships: 22.680.988\n",
                        "\n",
                        "Importing nodes\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†2s 274ms [2s 274ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†598ms [2s 872ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†570ms [3s 443ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†449ms [3s 892ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†1s 407ms [5s 299ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†1s 748ms [7s 48ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†1s 879ms [8s 927ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†1s 289ms [10s 217ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†1s 375ms [11s 592ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†1s 234ms [12s 827ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†1s 180ms [14s 7ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†1s 389ms [15s 397ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†1s 232ms [16s 630ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†6s 449ms [23s 79ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†4s 123ms [27s 202ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†3s 624ms [30s 826ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†2s 909ms [33s 736ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†3s 625ms [37s 361ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†10s 850ms [48s 212ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†2s 997ms [51s 210ms]\n",
                        "Imported 7,574,142 nodes in 51s 240ms\n",
                        "Prepare ID mapper\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†78ms [78ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†31ms [109ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†28ms [138ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†28ms [166ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†26ms [193ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†26ms [220ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†502ms [722ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†122ms [845ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†113ms [958ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†142ms [1s 100ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†182ms [1s 282ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†188ms [1s 471ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†194ms [1s 665ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†181ms [1s 846ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†172ms [2s 19ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†134ms [2s 154ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†25ms [2s 180ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†25ms [2s 205ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†24ms [2s 230ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†24ms [2s 254ms]\n",
                        "Importing relationships\n",
                        "Converting to intermediary format\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†2s 464ms [2s 464ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†806ms [3s 270ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†836ms [4s 107ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†964ms [5s 72ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†856ms [5s 929ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†736ms [6s 665ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†756ms [7s 421ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†672ms [8s 94ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†652ms [8s 746ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†661ms [9s 408ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†747ms [10s 155ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†601ms [10s 757ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†814ms [11s 571ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†614ms [12s 186ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†955ms [13s 142ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†827ms [13s 969ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†620ms [14s 590ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†38ms [14s 628ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†0ms [14s 628ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†0ms [14s 628ms]\n",
                        "  using configuration:Configuration[numberOfWorkers=8, temporaryPath=C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\databases\\neo4j\\temp, numberOfTrackedDense=10000, applyBatchSize=64]\n",
                        "Importing relationships\n",
                        ".......... .......... .......... .......... ..........   5% âˆ†939ms [939ms]\n",
                        ".......... .......... .......... .......... ..........  10% âˆ†475ms [1s 415ms]\n",
                        ".......... .......... .......... .......... ..........  15% âˆ†443ms [1s 859ms]\n",
                        ".......... .......... .......... .......... ..........  20% âˆ†367ms [2s 227ms]\n",
                        ".......... .......... .......... .......... ..........  25% âˆ†361ms [2s 589ms]\n",
                        ".......... .......... .......... .......... ..........  30% âˆ†361ms [2s 950ms]\n",
                        ".......... .......... .......... .......... ..........  35% âˆ†293ms [3s 244ms]\n",
                        ".......... .......... .......... .......... ..........  40% âˆ†1s 675ms [4s 919ms]\n",
                        ".......... .......... .......... .......... ..........  45% âˆ†972ms [5s 892ms]\n",
                        ".......... .......... .......... .......... ..........  50% âˆ†622ms [6s 515ms]\n",
                        ".......... .......... .......... .......... ..........  55% âˆ†1s 356ms [7s 872ms]\n",
                        ".......... .......... .......... .......... ..........  60% âˆ†1s 317ms [9s 189ms]\n",
                        ".......... .......... .......... .......... ..........  65% âˆ†4s 237ms [13s 427ms]\n",
                        ".......... .......... .......... .......... ..........  70% âˆ†14s 91ms [27s 518ms]\n",
                        ".......... .......... .......... .......... ..........  75% âˆ†255ms [27s 773ms]\n",
                        ".......... .......... .......... .......... ..........  80% âˆ†1s 6ms [28s 780ms]\n",
                        ".......... .......... .......... .......... ..........  85% âˆ†1s 280ms [30s 60ms]\n",
                        ".......... .......... .......... .......... ..........  90% âˆ†59ms [30s 120ms]\n",
                        ".......... .......... .......... .......... ..........  95% âˆ†0ms [30s 120ms]\n",
                        ".......... .......... .......... .......... .......... 100% âˆ†0ms [30s 120ms]\n",
                        "Imported 26,907,635 relationships in 45s 517ms\n",
                        "Flushing stores\n",
                        "Flush completed in 6s 411ms\n",
                        "IMPORT DONE in 1m 46s 688ms.\n",
                        "\n",
                        "---------------------------\n",
                        "\n",
                        "--- Neo4j Import Process Finished ---\n"
                    ]
                }
            ],
            "source": [
                "# --- Import Execution ---\n",
                "\n",
                "# Check if import is necessary by comparing expected node count with current graph\n",
                "run_import = False\n",
                "try:\n",
                "    # Ensure num_nodes_total was calculated\n",
                "    if 'num_nodes_total' not in globals() or num_nodes_total == 0:\n",
                "         print(\"Warning: Expected node count not calculated. Running integrity check first.\")\n",
                "         check_nodes_relationships_csv_files_integrity()\n",
                "         \n",
                "    print(f\"Checking current graph '{graph_name}' node count...\")\n",
                "    graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH) # Reconnect\n",
                "    current_node_count = graph.nodes.match().count()\n",
                "    print(f\"  Current node count: {current_node_count}\")\n",
                "    print(f\"  Expected node count from CSVs: {num_nodes_total}\")\n",
                "    \n",
                "    if current_node_count != num_nodes_total or num_nodes_total == 0:\n",
                "        print(\"Node count mismatch or expected count is zero. Proceeding with import.\")\n",
                "        run_import = True\n",
                "    else:\n",
                "        print(\"Node count matches expected count. Skipping import.\")\n",
                "        \n",
                "# Catch specific retryable errors, general driver errors, and OS-level connection errors\n",
                "except (ServiceUnavailable, TransientError, Neo4jError, OSError) as e:\n",
                "    print(f\"Could not connect to Neo4j to check node count ({e}). Assuming import is needed.\")\n",
                "    run_import = True\n",
                "except NameError:\n",
                "     print(\"Expected node count variable 'num_nodes_total' not defined. Assuming import is needed.\")\n",
                "     run_import = True\n",
                "except Exception as e:\n",
                "    print(f\"An unexpected error occurred checking node count: {e}. Assuming import is needed.\")\n",
                "    run_import = True\n",
                "\n",
                "# Execute the import process if needed\n",
                "if run_import:\n",
                "    try:\n",
                "        print(\"\\n--- Starting Neo4j Import Process ---\")\n",
                "        #command_neo4j_database_service(\"stop\")\n",
                "        reset_neo4j_database()\n",
                "        import_data()\n",
                "        #command_neo4j_database_service(\"start\")\n",
                "        print(\"--- Neo4j Import Process Finished ---\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\n--- IMPORT PROCESS FAILED ---: {e}\")\n",
                "        print(\"Attempting to start Neo4j service if it was stopped...\")\n",
                "        try:\n",
                "            # Try to start it anyway, might fail if already running or other issues\n",
                "            #command_neo4j_database_service(\"start\") \n",
                "            print(\"whatever\")\n",
                "        except Exception as start_err:\n",
                "            print(f\"Could not restart Neo4j service after failed import: {start_err}\")\n",
                "        # Re-raise the original exception\n",
                "        # raise e \n",
                "else:\n",
                "    print(\"\\nImport skipped as node count matches.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Final Verification\n",
                "\n",
                "Attempt to connect to the Neo4j database and verify the node count again after the import process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Verifying import results (expected nodes: 7574142)...\n",
                        "Attempt 1/30: Connecting to Neo4j...\n",
                        "  An unexpected error occurred during verification: Cannot open connection to ConnectionProfile('bolt://localhost:7687')\n",
                        "\n",
                        "Verification failed after 30 attempts. Could not confirm successful import.\n",
                        "Last recorded node count: -1\n",
                        "\n",
                        "--- FINAL VERIFICATION FAILED ---\n"
                    ]
                }
            ],
            "source": [
                "yelp_graph_ready = False\n",
                "final_node_count = -1\n",
                "\n",
                "def check_if_importing_is_successful():\n",
                "    \"\"\"Tries to connect to Neo4j and checks if the node count matches the expected count.\"\"\"\n",
                "    global graph, yelp_graph_ready, final_node_count\n",
                "    \n",
                "    # Ensure expected count is available\n",
                "    if 'num_nodes_total' not in globals() or num_nodes_total == 0:\n",
                "        try:\n",
                "             print(\"Recalculating expected node count for final verification...\")\n",
                "             check_nodes_relationships_csv_files_integrity()\n",
                "        except Exception as e:\n",
                "             print(f\"Error recalculating expected node count: {e}. Cannot verify import accurately.\")\n",
                "             return # Cannot verify\n",
                "             \n",
                "    print(f\"\\nVerifying import results (expected nodes: {num_nodes_total})...\")\n",
                "    num_tries = 30 # Try for 30 seconds (adjust as needed)\n",
                "    wait_interval = 2 # Wait 2 seconds between tries\n",
                "    \n",
                "    for one_try in range(num_tries):\n",
                "        try:\n",
                "            print(f\"Attempt {one_try + 1}/{num_tries}: Connecting to Neo4j...\")\n",
                "            graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)\n",
                "            final_node_count = graph.nodes.match().count()\n",
                "            print(f\"  Successfully connected. Found {final_node_count} nodes.\")\n",
                "            \n",
                "            if final_node_count == 0 and num_nodes_total > 0:\n",
                "                print(\"  Warning: Graph is empty, but expected nodes. Import might have failed silently or service is still starting.\")\n",
                "                # Continue trying for a bit longer\n",
                "            elif final_node_count != num_nodes_total:\n",
                "                print(f\"  Warning: Node count mismatch! Expected {num_nodes_total}, found {final_node_count}.\")\n",
                "                # Consider this potentially successful but log warning\n",
                "                yelp_graph_ready = True # Mark as ready but with a warning\n",
                "                return\n",
                "            else: # Counts match\n",
                "                print(\"  Node count matches expected count. Import successful!\")\n",
                "                yelp_graph_ready = True\n",
                "                return # Success\n",
                "                \n",
                "        # Catch specific retryable errors, general driver errors, and OS-level connection errors\n",
                "        except (ServiceUnavailable, TransientError, Neo4jError, OSError) as e:\n",
                "            print(f\"  Connection failed ({e}). Neo4j might still be starting. Retrying in {wait_interval}s...\")\n",
                "        except Exception as e:\n",
                "            print(f\"  An unexpected error occurred during verification: {e}\")\n",
                "            # Stop trying on unexpected errors\n",
                "            break \n",
                "            \n",
                "        # Wait before next try only if not successful yet\n",
                "        if not yelp_graph_ready:\n",
                "             sleep(wait_interval)\n",
                "             \n",
                "    # If loop finishes without success\n",
                "    if not yelp_graph_ready:\n",
                "        print(f\"\\nVerification failed after {num_tries} attempts. Could not confirm successful import.\")\n",
                "        print(f\"Last recorded node count: {final_node_count}\")\n",
                "\n",
                "# Run the final check\n",
                "check_if_importing_is_successful()\n",
                "\n",
                "if not yelp_graph_ready:\n",
                "    print(\"\\n--- FINAL VERIFICATION FAILED ---\")\n",
                "    # Optional: raise Exception(\"Failed to verify Neo4j import success.\")\n",
                "else:\n",
                "     print(\"\\n--- FINAL VERIFICATION COMPLETE --- \")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
