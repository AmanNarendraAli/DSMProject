{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Dataset: Data Loading and Neo4j Import\n",
                "\n",
                "This notebook processes Yelp dataset JSON files (Business, User, Review), cleans the data, transforms it into CSV format suitable for Neo4j's `neo4j-admin import` tool, and imports it into a Neo4j database."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup: Imports and Constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core Libraries\n",
                "import json\n",
                "import csv\n",
                "import uuid\n",
                "import os\n",
                "import regex as re\n",
                "import subprocess\n",
                "import datetime\n",
                "import shutil\n",
                "from pathlib import Path\n",
                "from time import sleep\n",
                "from typing import List, Tuple, Set\n",
                "\n",
                "# Data Handling & Graph\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import reverse_geocoder as rg\n",
                "from py2neo import Graph\n",
                "# Import base Neo4jError and specific retryable errors\n",
                "from neo4j.exceptions import ServiceUnavailable, TransientError, Neo4jError"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Constants: Node Labels ---\n",
                "BUSINESS_NODE = \"Business\"\n",
                "USER_NODE = \"User\"\n",
                "REVIEW_NODE = \"Review\"\n",
                "CATEGORY_NODE = \"Category\"\n",
                "CITY_NODE = \"City\"\n",
                "STATE_NODE = \"State\"  # Changed from Area\n",
                "COUNTRY_NODE = \"Country\"\n",
                "\n",
                "# --- Constants: Relationship Types ---\n",
                "IN_CATEGORY = \"IN_CATEGORY\"\n",
                "IN_CITY = \"IN_CITY\"\n",
                "IN_STATE = \"IN_STATE\"    # Changed from IN_AREA\n",
                "IN_COUNTRY = \"IN_COUNTRY\"\n",
                "FRIENDS = \"FRIENDS\"\n",
                "REVIEWS = \"REVIEWS\"\n",
                "WROTE = \"WROTE\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration: File Paths and Neo4j Connection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Input Data Files ---\n",
                "# Adjusted path to point one level up and into 'yelp_dataset'\n",
                "data_folder = \"../yelp_dataset\" \n",
                "business_json_file = os.path.join(data_folder, \"yelp_academic_dataset_business.json\")\n",
                "review_json_file = os.path.join(data_folder, \"yelp_academic_dataset_review.json\")\n",
                "user_json_file = os.path.join(data_folder, \"yelp_academic_dataset_user.json\")\n",
                "\n",
                "# List of raw input files\n",
                "list_raw_files = [business_json_file, review_json_file, user_json_file]\n",
                "\n",
                "# --- Intermediate Fixed Data Files ---\n",
                "# These will be created in the same directory as the raw files\n",
                "fixed_business_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_business.json\")\n",
                "fixed_review_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_review.json\")\n",
                "fixed_user_json_file = os.path.join(data_folder, \"fixed_yelp_academic_dataset_user.json\")\n",
                "\n",
                "# List of fixed intermediate files\n",
                "list_fixed_data = [fixed_business_json_file, fixed_review_json_file, fixed_user_json_file]\n",
                "\n",
                "# --- Output CSV Files for Neo4j Import ---\n",
                "# These will be generated in the current working directory (assumed project root)\n",
                "business_nodes_csv_file = \"business_nodes.csv\"\n",
                "category_nodes_csv_file = \"category_nodes.csv\"\n",
                "city_nodes_csv_file = \"city_nodes.csv\"\n",
                "state_nodes_csv_file = \"state_nodes.csv\"  # Changed from area_nodes.csv\n",
                "country_nodes_csv_file = \"country_nodes.csv\"\n",
                "user_nodes_csv_file = \"user_nodes.csv\"\n",
                "review_nodes_csv_file = \"review_nodes.csv\"\n",
                "relationship_csv_file = \"relationships.csv\"\n",
                "\n",
                "# List of node CSV files (updated)\n",
                "nodes_files = [\n",
                "    business_nodes_csv_file, category_nodes_csv_file, city_nodes_csv_file, \n",
                "    state_nodes_csv_file, country_nodes_csv_file, user_nodes_csv_file, \n",
                "    review_nodes_csv_file\n",
                "]\n",
                "\n",
                "# --- Neo4j Configuration ---\n",
                "graph_name = \"neo4j\" # Default graph name\n",
                "SERVER_ADDRESS = \"bolt://localhost:7687\"\n",
                "SERVER_AUTH = (\"neo4j\",\"password\") # Replace with your Neo4j credentials\n",
                "\n",
                "# --- Neo4j Installation Path (Manual Configuration Required for Neo4j 5+) ---\n",
                "# !! IMPORTANT !! Set this variable to the root directory of your Neo4j installation.\n",
                "# Example for Windows: neo4j_home = r\"C:\\Users\\YourUser\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-abc12345-def67890\"\n",
                "# Example for Linux/macOS: neo4j_home = \"/path/to/neo4j-community-5.x.x\"\n",
                "neo4j_home = \"C:/Users/wiztu/.Neo4jDesktop/relate-data/dbmss/dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\" \n",
                "\n",
                "if neo4j_home is None:\n",
                "    print(\"WARNING: 'neo4j_home' variable is not set.\")\n",
                "    print(\"         Please edit this cell and set it to your Neo4j installation path.\")\n",
                "    print(\"         The script needs this path to find 'neo4j-admin' and 'neo4j' commands.\")\n",
                "\n",
                "# --- Utility Function ---\n",
                "def delete_files(files: List[str]):\n",
                "    \"\"\"Safely deletes a list of files if they exist.\"\"\"\n",
                "    for one_file in files:\n",
                "        one_path = Path(one_file)\n",
                "        if one_path.is_file():\n",
                "            try:\n",
                "                one_path.unlink()\n",
                "                # print(f\"Deleted file: {one_file}\")\n",
                "            except OSError as e:\n",
                "                print(f\"Error deleting file {one_file}: {e}\")\n",
                "\n",
                "# --- Initial Check: Raw Data Files ---\n",
                "if not all(Path(f).is_file() for f in list_raw_files):\n",
                "    missing = [f for f in list_raw_files if not Path(f).is_file()]\n",
                "    raise FileNotFoundError(f\"Missing raw Yelp JSON files: {missing}. Expected in '{data_folder}'.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Preprocessing Raw Data\n",
                "\n",
                "**Data Challenges Addressed:**\n",
                "1.  **Non-Unique IDs:** `business_id`, `user_id`, and `review_id` can overlap across files. Prefixes (`b-`, `u-`, `r-`) are added to ensure global uniqueness for Neo4j import.\n",
                "2.  **Dangling Friendships:** Users might list friends who don't exist in the dataset. These non-existent friend references are removed.\n",
                "3.  **Duplicate Categories:** Businesses might list the same category multiple times. Duplicates are removed.\n",
                "4.  **String vs. Array:** `friends` (in User) and `categories` (in Business) are stored as comma-separated strings instead of proper JSON arrays. While fixing IDs/categories, these are handled appropriately."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Preprocessing Functions ---\n",
                "\n",
                "def remove_unknown_friends(raw_user_path: str, output_path: str) -> None:\n",
                "    \"\"\"Reads raw user data, identifies all valid user IDs, \n",
                "       and writes a new file containing only users with valid friend lists.\"\"\"\n",
                "    print(f\"Reading user IDs from {raw_user_path}...\")\n",
                "    user_ids = set()\n",
                "    try:\n",
                "        with open(raw_user_path, \"r\", encoding=\"utf-8\") as rf:\n",
                "            for i, line in enumerate(rf):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        user_ids.add(json_node[\"user_id\"])\n",
                "                    except json.JSONDecodeError:\n",
                "                        print(f\"Warning: Skipping invalid JSON on line {i+1} in {raw_user_path}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {raw_user_path}\")\n",
                "        raise\n",
                "    print(f\"Found {len(user_ids)} unique user IDs.\")\n",
                "\n",
                "    print(f\"Writing users with cleaned friend lists to {output_path}...\")\n",
                "    try:\n",
                "        with open(raw_user_path, \"r\", encoding=\"utf-8\") as rf, open(output_path, mode=\"w\", encoding=\"utf-8\") as of:\n",
                "            for i, line in enumerate(rf):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        friends_str = json_node.get(\"friends\", \"\")\n",
                "                        if friends_str and friends_str.lower() != 'none' and len(friends_str.strip()) > 0:\n",
                "                            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
                "                            # Filter out friends not present in the dataset\n",
                "                            friends_exist_arr = [f for f in friends_arr if f in user_ids]\n",
                "                            json_node[\"friends\"] = ', '.join(friends_exist_arr)\n",
                "                        else:\n",
                "                            json_node[\"friends\"] = \"\" # Ensure field exists but is empty\n",
                "                        of.write(f'{json.dumps(json_node)}\\n')\n",
                "                    except json.JSONDecodeError:\n",
                "                        # Warning already printed during ID collection\n",
                "                        pass \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {raw_user_path} during writing phase.\")\n",
                "        raise\n",
                "    print(\"Finished cleaning friend lists.\")\n",
                "\n",
                "def make_ids_unique(input_path: str, output_path: str, func_to_modify) -> None:\n",
                "    \"\"\"Applies a modification function (e.g., adding ID prefixes) \n",
                "       to each JSON object in the input file and writes to the output file.\"\"\"\n",
                "    print(f\"Applying ID modifications to {input_path} -> {output_path}...\")\n",
                "    temp_output_file = output_path + \".tmp-\" + str(uuid.uuid4())\n",
                "    try:\n",
                "        with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
                "            for i, line in enumerate(in_f):\n",
                "                if len(line.strip()) > 0:\n",
                "                    try:\n",
                "                        json_node = json.loads(line)\n",
                "                        json_node = func_to_modify(json_node)\n",
                "                        ou_f.write(f'{json.dumps(json_node)}\\n')\n",
                "                    except json.JSONDecodeError:\n",
                "                         print(f\"Warning: Skipping invalid JSON on line {i+1} in {input_path}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {input_path}\")\n",
                "        # Clean up temp file if it exists\n",
                "        if Path(temp_output_file).is_file():\n",
                "            Path(temp_output_file).unlink()\n",
                "        raise\n",
                "        \n",
                "    # Replace original file with modified temp file\n",
                "    try:\n",
                "        os.replace(temp_output_file, output_path)\n",
                "        print(f\"Successfully updated {output_path}.\")\n",
                "    except OSError as e:\n",
                "        print(f\"Error replacing file {output_path} with {temp_output_file}: {e}\")\n",
                "        # Attempt cleanup again\n",
                "        if Path(temp_output_file).is_file():\n",
                "            try: Path(temp_output_file).unlink()\n",
                "            except OSError: pass\n",
                "        raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fixed data files already exist. Skipping preprocessing.\n"
                    ]
                }
            ],
            "source": [
                "# --- Preprocessing Execution ---\n",
                "\n",
                "# Check if fixed files already exist. If not, create them.\n",
                "if not all(Path(f).is_file() for f in list_fixed_data):\n",
                "    print(\"Fixed data files not found. Starting preprocessing...\")\n",
                "    # Remove any potentially incomplete fixed files first\n",
                "    delete_files(list_fixed_data)\n",
                "\n",
                "    # --- Define Modification Functions ---\n",
                "    def fix_user(json_node):\n",
                "        # Prefix user_id\n",
                "        original_user_id = json_node[\"user_id\"]\n",
                "        json_node[\"user_id\"] = \"u-\" + original_user_id\n",
                "        # Prefix friend_ids (already cleaned in remove_unknown_friends)\n",
                "        friends_str = json_node.get(\"friends\", \"\")\n",
                "        if friends_str and len(friends_str.strip()) > 0:\n",
                "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
                "            # Prefix each valid friend ID\n",
                "            friends_arr = [\"u-\" + f_id for f_id in friends_arr]\n",
                "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
                "        else:\n",
                "             json_node[\"friends\"] = \"\"\n",
                "        return json_node\n",
                "\n",
                "    def fix_business(json_node):\n",
                "        # Prefix business_id\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        # Deduplicate and clean categories\n",
                "        categories_str = json_node.get(\"categories\", None)\n",
                "        if categories_str and len(categories_str.strip()) > 0:\n",
                "            categories_arr = re.split(r\"\\s*,\\s*\", categories_str.strip())\n",
                "            # Filter out empty strings and trim whitespace, then deduplicate\n",
                "            categories_set = set(cat.strip() for cat in categories_arr if cat and len(cat.strip()) > 0)\n",
                "            json_node[\"categories\"] = ', '.join(sorted(list(categories_set))) # Sort for consistency\n",
                "        else:\n",
                "            json_node[\"categories\"] = \"\"\n",
                "        return json_node\n",
                "\n",
                "    def fix_review(json_node):\n",
                "        # Prefix review_id, user_id, business_id\n",
                "        json_node[\"review_id\"] = \"r-\" + json_node[\"review_id\"]\n",
                "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
                "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
                "        return json_node\n",
                "\n",
                "    # --- Apply Fixes ---\n",
                "    # 1. Clean user friends (reads raw, writes to fixed path)\n",
                "    remove_unknown_friends(user_json_file, fixed_user_json_file)\n",
                "    # 2. Make user IDs unique (reads fixed, modifies in-place)\n",
                "    make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)\n",
                "    # 3. Make business IDs unique and clean categories (reads raw, writes to fixed path)\n",
                "    make_ids_unique(business_json_file, fixed_business_json_file, fix_business)\n",
                "    # 4. Make review IDs unique (reads raw, writes to fixed path)\n",
                "    make_ids_unique(review_json_file, fixed_review_json_file, fix_review)\n",
                "    \n",
                "    print(\"Preprocessing complete. Fixed data files created.\")\n",
                "else:\n",
                "    print(\"Fixed data files already exist. Skipping preprocessing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generating CSV Files for Neo4j Import Tool\n",
                "\n",
                "This section reads the preprocessed (`fixed_...json`) files and generates CSV files formatted according to the requirements of `neo4j-admin import`.\n",
                "- Node files contain headers like `nodeId:ID`, `propertyName`, `:LABEL`.\n",
                "- The relationship file contains headers `:START_ID`, `:END_ID`, `:TYPE`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CSV Generation Functions ---\n",
                "\n",
                "# Global sets to store unique nodes and relationships across functions\n",
                "business_lat_lon = {}\n",
                "state_nodes: Set[Tuple[str, str]] = set() # Changed from area_nodes\n",
                "city_nodes: Set[Tuple[str, str]] = set()\n",
                "country_nodes: Set[str] = set()\n",
                "categories_nodes: Set[str] = set()\n",
                "\n",
                "# Relationship sets (tuples: start_id, end_id, type)\n",
                "in_city_relationships: Set[Tuple[str, str, str]] = set()\n",
                "in_state_relationships: Set[Tuple[str, str, str]] = set() # Changed from in_area\n",
                "in_country_relationships: Set[Tuple[str, str, str]] = set()\n",
                "in_category_relationships: Set[Tuple[str, str, str]] = set()\n",
                "friend_relationships: Set[Tuple[str, str, str]] = set()\n",
                "wrote_relationships: Set[Tuple[str, str, str]] = set()\n",
                "reviews_relationships: Set[Tuple[str, str, str]] = set()\n",
                "\n",
                "def process_business_data():\n",
                "    \"\"\"Reads fixed business data, populates business nodes, categories, \n",
                "       lat/lon mapping, and IN_CATEGORY relationships.\"\"\"\n",
                "    print(f\"Processing business data from {fixed_business_json_file}...\")\n",
                "    global categories_nodes, business_lat_lon, in_category_relationships\n",
                "    processed_count = 0\n",
                "    try:\n",
                "        with open(fixed_business_json_file, \"r\", encoding=\"utf-8\") as bjf:\n",
                "            # Prepare writer for business nodes CSV\n",
                "            with open(business_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as business_csv:\n",
                "                fieldnames = [\"business_id:ID\", \"name\", \"address\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(business_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in bjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            business_id = json_node[\"business_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Write business node\n",
                "                            writer.writerow({\n",
                "                                \"business_id:ID\": business_id,\n",
                "                                \"name\": json_node.get(\"name\", \"\"),\n",
                "                                \"address\": json_node.get(\"address\", \"\"),\n",
                "                                \":LABEL\": BUSINESS_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Store lat/lon for location processing\n",
                "                            if json_node.get(\"latitude\") is not None and json_node.get(\"longitude\") is not None:\n",
                "                                business_lat_lon[business_id] = (json_node[\"latitude\"], json_node[\"longitude\"])\n",
                "                            \n",
                "                            # Process categories\n",
                "                            categories_str = json_node.get(\"categories\", \"\")\n",
                "                            if categories_str and len(categories_str.strip()) > 0:\n",
                "                                cur_categories = re.split(r\"\\s*,\\s*\", categories_str.strip()) # Already cleaned/deduplicated\n",
                "                                categories_nodes.update(cur_categories)\n",
                "                                for category in cur_categories:\n",
                "                                    in_category_relationships.add((business_id, category, IN_CATEGORY))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_business_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in business record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_business_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} businesses.\")\n",
                "\n",
                "def process_location_data():\n",
                "    \"\"\"Uses reverse_geocoder on business lat/lons to create City, State, Country nodes \n",
                "       and IN_CITY, IN_STATE, IN_COUNTRY relationships.\"\"\"\n",
                "    print(\"Processing location data using reverse geocoding...\")\n",
                "    global city_nodes, state_nodes, country_nodes, in_city_relationships, in_state_relationships, in_country_relationships\n",
                "    \n",
                "    if not business_lat_lon:\n",
                "        print(\"Warning: No business latitude/longitude data found. Skipping location processing.\")\n",
                "        return\n",
                "        \n",
                "    # Prepare coordinates for batch reverse geocoding\n",
                "    business_ids = list(business_lat_lon.keys())\n",
                "    coordinates = list(business_lat_lon.values())\n",
                "    \n",
                "    print(f\"Performing reverse geocoding for {len(coordinates)} coordinates...\")\n",
                "    try:\n",
                "        location_results = rg.search(coordinates)\n",
                "    except Exception as e:\n",
                "        print(f\"Error during reverse geocoding: {e}\")\n",
                "        print(\"Ensure 'reverse_geocoder' library and its data are installed correctly.\")\n",
                "        raise\n",
                "        \n",
                "    print(\"Processing geocoding results...\")\n",
                "    # Process results and build node/relationship sets\n",
                "    for business_id, loc_info in zip(business_ids, location_results):\n",
                "        city = loc_info.get('name', 'UnknownCity')\n",
                "        state = loc_info.get('admin1', 'UnknownState') # admin1 is typically state/province\n",
                "        country = loc_info.get('cc', 'UnknownCountry') # cc is country code\n",
                "        \n",
                "        # Create unique IDs\n",
                "        # Ensure IDs don't clash if names are identical across different levels\n",
                "        country_id = country\n",
                "        state_id = f\"{state}-{country_id}\"\n",
                "        city_id = f\"{city}-{state_id}\"\n",
                "        \n",
                "        # Add nodes (sets handle uniqueness)\n",
                "        country_nodes.add(country_id)\n",
                "        state_nodes.add((state_id, state))\n",
                "        city_nodes.add((city_id, city))\n",
                "        \n",
                "        # Add relationships (sets handle uniqueness)\n",
                "        in_city_relationships.add((business_id, city_id, IN_CITY))\n",
                "        in_state_relationships.add((city_id, state_id, IN_STATE))\n",
                "        in_country_relationships.add((state_id, country_id, IN_COUNTRY))\n",
                "        \n",
                "    print(f\"Finished processing locations: {len(city_nodes)} cities, {len(state_nodes)} states, {len(country_nodes)} countries.\")\n",
                "\n",
                "def process_user_data():\n",
                "    \"\"\"Reads fixed user data, populates user nodes and FRIEND relationships.\"\"\"\n",
                "    print(f\"Processing user data from {fixed_user_json_file}...\")\n",
                "    global friend_relationships\n",
                "    processed_count = 0\n",
                "    try:\n",
                "        with open(fixed_user_json_file, \"r\", encoding=\"utf-8\") as ujf:\n",
                "            # Prepare writer for user nodes CSV\n",
                "            with open(user_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as user_csv:\n",
                "                fieldnames = [\"user_id:ID\", \"name\", \"yelping_since\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(user_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in ujf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            user_id = json_node[\"user_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Write user node\n",
                "                            writer.writerow({\n",
                "                                \"user_id:ID\": user_id,\n",
                "                                \"name\": json_node.get(\"name\", \"\"),\n",
                "                                \"yelping_since\": json_node.get(\"yelping_since\", \"\"),\n",
                "                                \":LABEL\": USER_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Process friends\n",
                "                            friends_str = json_node.get(\"friends\", \"\")\n",
                "                            if friends_str and len(friends_str.strip()) > 0:\n",
                "                                friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip()) # Already cleaned and prefixed\n",
                "                                for friend_id in friends_arr:\n",
                "                                    # Ensure relationship uniqueness (u1 < u2)\n",
                "                                    u1 = min(user_id, friend_id)\n",
                "                                    u2 = max(user_id, friend_id)\n",
                "                                    if u1 != u2: # Avoid self-loops if data error exists\n",
                "                                        friend_relationships.add((u1, u2, FRIENDS))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_user_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in user record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_user_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} users.\")\n",
                "\n",
                "def process_review_data():\n",
                "    \"\"\"Reads fixed review data, populates review nodes, WROTE and REVIEWS relationships.\"\"\"\n",
                "    print(f\"Processing review data from {fixed_review_json_file}...\")\n",
                "    global wrote_relationships, reviews_relationships\n",
                "    processed_count = 0\n",
                "    try:\n",
                "        with open(fixed_review_json_file, \"r\", encoding=\"utf-8\") as rjf:\n",
                "            # Prepare writer for review nodes CSV\n",
                "            with open(review_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as review_csv:\n",
                "                # Note: 'text' can contain quotes/commas, ensure proper quoting\n",
                "                fieldnames = [\"review_id:ID\", \"stars\", \"date\", \"text\", \":LABEL\"]\n",
                "                writer = csv.DictWriter(review_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_ALL) # QUOTE_ALL for safety\n",
                "                writer.writeheader()\n",
                "                \n",
                "                for line in rjf:\n",
                "                    line = line.strip()\n",
                "                    if len(line) > 0:\n",
                "                        try:\n",
                "                            json_node = json.loads(line)\n",
                "                            review_id = json_node[\"review_id\"] # Already prefixed\n",
                "                            user_id = json_node[\"user_id\"]     # Already prefixed\n",
                "                            business_id = json_node[\"business_id\"] # Already prefixed\n",
                "                            \n",
                "                            # Write review node\n",
                "                            writer.writerow({\n",
                "                                \"review_id:ID\": review_id,\n",
                "                                \"stars\": json_node.get(\"stars\", 0),\n",
                "                                \"date\": json_node.get(\"date\", \"\"),\n",
                "                                \"text\": json_node.get(\"text\", \"\"),\n",
                "                                \":LABEL\": REVIEW_NODE\n",
                "                            })\n",
                "                            \n",
                "                            # Add relationships\n",
                "                            wrote_relationships.add((user_id, review_id, WROTE))\n",
                "                            reviews_relationships.add((review_id, business_id, REVIEWS))\n",
                "                            processed_count += 1\n",
                "                        except json.JSONDecodeError:\n",
                "                            print(f\"Warning: Skipping invalid JSON line in {fixed_review_json_file}\")\n",
                "                        except KeyError as e:\n",
                "                            print(f\"Warning: Missing key {e} in review record: {line[:100]}...\")\n",
                "                            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: File not found {fixed_review_json_file}\")\n",
                "        raise\n",
                "    print(f\"Finished processing {processed_count} reviews.\")\n",
                "\n",
                "def write_category_nodes_to_file():\n",
                "    \"\"\"Writes the collected unique category nodes to a CSV file.\"\"\"\n",
                "    print(f\"Writing {len(categories_nodes)} category nodes to {category_nodes_csv_file}...\")\n",
                "    with open(category_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as category_csv:\n",
                "        fieldnames = [\"category_id:ID\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(category_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for category_id in sorted(list(categories_nodes)):\n",
                "            writer.writerow({\"category_id:ID\": category_id, \":LABEL\": CATEGORY_NODE})\n",
                "    print(\"Finished writing category nodes.\")\n",
                "\n",
                "def write_location_nodes_to_file():\n",
                "    \"\"\"Writes the collected unique City, State, Country nodes to CSV files.\"\"\"\n",
                "    # Write City Nodes\n",
                "    print(f\"Writing {len(city_nodes)} city nodes to {city_nodes_csv_file}...\")\n",
                "    with open(city_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as city_csv:\n",
                "        fieldnames = [\"city_id:ID\", \"name\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(city_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for city_id, city_name in sorted(list(city_nodes)):\n",
                "            writer.writerow({\"city_id:ID\": city_id, \"name\": city_name, \":LABEL\": CITY_NODE})\n",
                "            \n",
                "    # Write State Nodes (Changed from Area)\n",
                "    print(f\"Writing {len(state_nodes)} state nodes to {state_nodes_csv_file}...\")\n",
                "    with open(state_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as state_csv:\n",
                "        fieldnames = [\"state_id:ID\", \"name\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(state_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for state_id, state_name in sorted(list(state_nodes)):\n",
                "            writer.writerow({\"state_id:ID\": state_id, \"name\": state_name, \":LABEL\": STATE_NODE})\n",
                "            \n",
                "    # Write Country Nodes\n",
                "    print(f\"Writing {len(country_nodes)} country nodes to {country_nodes_csv_file}...\")\n",
                "    with open(country_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as country_csv:\n",
                "        fieldnames = [\"country_id:ID\", \":LABEL\"]\n",
                "        writer = csv.DictWriter(country_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        for country_id in sorted(list(country_nodes)):\n",
                "            writer.writerow({\"country_id:ID\": country_id, \":LABEL\": COUNTRY_NODE})\n",
                "    print(\"Finished writing location nodes.\")\n",
                "\n",
                "def write_relationships_to_file():\n",
                "    \"\"\"Writes all collected relationships to a single CSV file.\"\"\"\n",
                "    all_relationships = (\n",
                "        in_category_relationships | in_city_relationships | \n",
                "        in_state_relationships | in_country_relationships | \n",
                "        friend_relationships | wrote_relationships | reviews_relationships\n",
                "    )\n",
                "    print(f\"Writing {len(all_relationships)} relationships to {relationship_csv_file}...\")\n",
                "    with open(relationship_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\") as rel_csv:\n",
                "        fieldnames = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
                "        writer = csv.DictWriter(rel_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
                "        writer.writeheader()\n",
                "        # Sort for deterministic output (optional but good practice)\n",
                "        for start_id, end_id, rel_type in sorted(list(all_relationships)):\n",
                "            writer.writerow({\":START_ID\": start_id, \":END_ID\": end_id, \":TYPE\": rel_type})\n",
                "    print(\"Finished writing relationships.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Import CSV files already exist. Skipping generation.\n"
                    ]
                }
            ],
            "source": [
                "# --- CSV Generation Execution ---\n",
                "\n",
                "# Check if CSV files already exist. If not, generate them.\n",
                "if not all(Path(f).is_file() for f in nodes_files) or not Path(relationship_csv_file).is_file():\n",
                "    print(\"Import CSV files not found or incomplete. Starting generation...\")\n",
                "    # Delete any existing CSV files first\n",
                "    delete_files(nodes_files)\n",
                "    delete_files([relationship_csv_file])\n",
                "    \n",
                "    # Process data and populate node/relationship sets\n",
                "    process_business_data()\n",
                "    process_location_data() # Depends on business_lat_lon\n",
                "    process_user_data()\n",
                "    process_review_data()\n",
                "    \n",
                "    # Write node sets to their respective CSV files\n",
                "    write_category_nodes_to_file()\n",
                "    write_location_nodes_to_file()\n",
                "    # Business, User, Review nodes written during processing steps\n",
                "    \n",
                "    # Write all relationships to the relationship CSV file\n",
                "    write_relationships_to_file()\n",
                "    \n",
                "    # Optional: Clean up large intermediate sets to free memory\n",
                "    del business_lat_lon, categories_nodes, city_nodes, state_nodes, country_nodes\n",
                "    del in_category_relationships, in_city_relationships, in_state_relationships, in_country_relationships\n",
                "    del friend_relationships, wrote_relationships, reviews_relationships\n",
                "    \n",
                "    print(\"CSV file generation complete.\")\n",
                "else:\n",
                "    print(\"Import CSV files already exist. Skipping generation.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Integrity Check\n",
                "\n",
                "Verify basic integrity of the generated CSV files before attempting import:\n",
                "- Check for duplicate IDs within each node file.\n",
                "- Check for duplicate relationships (same start, end, and type)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking integrity of generated CSV files...\n",
                        "  Checking node file: business_nodes.csv...\n",
                        "    Found 150346 unique nodes.\n",
                        "  Checking node file: category_nodes.csv...\n",
                        "    Found 1311 unique nodes.\n",
                        "  Checking node file: city_nodes.csv...\n",
                        "    Found 642 unique nodes.\n",
                        "  Checking node file: state_nodes.csv...\n",
                        "    Found 16 unique nodes.\n",
                        "  Checking node file: country_nodes.csv...\n",
                        "    Found 2 unique nodes.\n",
                        "  Checking node file: user_nodes.csv...\n",
                        "    Found 1987897 unique nodes.\n",
                        "  Checking node file: review_nodes.csv...\n",
                        "    Found 6990280 unique nodes.\n",
                        "  Checking relationship file: relationships.csv...\n",
                        "    Found 22105987 unique relationships.\n",
                        "Integrity check passed. Total unique nodes expected: 9130494\n"
                    ]
                }
            ],
            "source": [
                "num_nodes_total = 0\n",
                "\n",
                "def check_nodes_relationships_csv_files_integrity():\n",
                "    \"\"\"Validates uniqueness of node IDs and relationships in generated CSVs.\"\"\"\n",
                "    global num_nodes_total\n",
                "    num_nodes_total = 0 # Reset count\n",
                "    print(\"Checking integrity of generated CSV files...\")\n",
                "    \n",
                "    # Check Node Files\n",
                "    for one_node_file in nodes_files:\n",
                "        print(f\"  Checking node file: {one_node_file}...\")\n",
                "        try:\n",
                "            temp_df = pd.read_csv(one_node_file, header=\"infer\", sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
                "            ids = temp_df.iloc[:, 0] # First column should be the ID (e.g., 'node_id:ID')\n",
                "            if len(ids) != len(ids.unique()):\n",
                "                duplicates = ids[ids.duplicated()].unique()\n",
                "                raise ValueError(f\"Duplicate node IDs found in [{one_node_file}]: {list(duplicates)[:5]}...\")\n",
                "            num_nodes_total += len(ids)\n",
                "            print(f\"    Found {len(ids)} unique nodes.\")\n",
                "        except FileNotFoundError:\n",
                "            print(f\"Error: Node file not found: {one_node_file}\")\n",
                "            raise\n",
                "        except Exception as e:\n",
                "            print(f\"Error checking node file {one_node_file}: {e}\")\n",
                "            raise\n",
                "            \n",
                "    # Check Relationship File\n",
                "    print(f\"  Checking relationship file: {relationship_csv_file}...\")\n",
                "    try:\n",
                "        temp_df = pd.read_csv(relationship_csv_file, header=\"infer\", sep=\",\", encoding=\"utf-8\", low_memory=False)\n",
                "        # Combine start, end, type for uniqueness check\n",
                "        # Ensure columns are treated as strings to avoid type issues\n",
                "        rel_signatures = temp_df.iloc[:, 0].astype(str) + \"|\" + temp_df.iloc[:, 1].astype(str) + \"|\" + temp_df.iloc[:, 2].astype(str)\n",
                "        if len(rel_signatures) != len(rel_signatures.unique()):\n",
                "            duplicates = rel_signatures[rel_signatures.duplicated()].unique()\n",
                "            raise ValueError(f\"Duplicate relationships found in [{relationship_csv_file}]: {list(duplicates)[:5]}...\")\n",
                "        print(f\"    Found {len(rel_signatures)} unique relationships.\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: Relationship file not found: {relationship_csv_file}\")\n",
                "        raise\n",
                "    except Exception as e:\n",
                "        print(f\"Error checking relationship file {relationship_csv_file}: {e}\")\n",
                "        raise\n",
                "        \n",
                "    print(f\"Integrity check passed. Total unique nodes expected: {num_nodes_total}\")\n",
                "\n",
                "# Run the check\n",
                "try:\n",
                "    check_nodes_relationships_csv_files_integrity()\n",
                "except Exception as e:\n",
                "    print(f\"\\n--- DATA INTEGRITY CHECK FAILED ---: {e}\")\n",
                "    print(\"Import aborted. Please check the CSV generation process and input data.\")\n",
                "    # Optionally raise to stop notebook execution\n",
                "    # raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Importing Data into Neo4j\n",
                "\n",
                "This section uses the `neo4j-admin import` tool to load the generated CSV files into the Neo4j database. \n",
                "\n",
                "**Process:**\n",
                "1.  Stop the Neo4j Database Service.\n",
                "2.  Delete the existing database files (effectively resetting the graph).\n",
                "3.  Run `neo4j-admin import` using the generated CSV files.\n",
                "4.  Start the Neo4j Database Service.\n",
                "5.  Verify the import by checking the node count.\n",
                "\n",
                "**Important Notes:**\n",
                "- Requires `neo4j_home` to be correctly identified earlier.\n",
                "- Requires sufficient permissions to stop/start the Neo4j service and modify its data directories.\n",
                "- This will **completely overwrite** the target Neo4j database (`graph_name`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Neo4j Import Functions ---\n",
                "\n",
                "def command_neo4j_database_service(cmd: str):\n",
                "    \"\"\"Executes start/stop commands for the Neo4j service.\"\"\"\n",
                "    if neo4j_home is None or not Path(neo4j_home).is_dir():\n",
                "        raise ValueError(\"Neo4j home directory ('neo4j_home') is not set or invalid. Please configure it in cell [2].\")\n",
                "        \n",
                "    neo4j_cmd_path = os.path.join(neo4j_home, \"bin\", \"neo4j.bat\") # Assuming Windows .bat\n",
                "    if not Path(neo4j_cmd_path).is_file():\n",
                "         neo4j_cmd_path = os.path.join(neo4j_home, \"bin\", \"neo4j\") # Try Linux/macOS script\n",
                "         if not Path(neo4j_cmd_path).is_file():\n",
                "             raise FileNotFoundError(f\"Neo4j command script not found in {os.path.join(neo4j_home, 'bin')}\")\n",
                "             \n",
                "    if cmd in [\"stop\", \"start\"]:\n",
                "        print(f\"Attempting to {cmd} Neo4j service...\")\n",
                "        try:\n",
                "            # Use shell=True cautiously, ensure neo4j_cmd_path is safe.\n",
                "            # Capture output to check success.\n",
                "            cmd_res = subprocess.run([neo4j_cmd_path, cmd], capture_output=True, text=True, check=False, shell=True)\n",
                "            \n",
                "            print(f\"  STDOUT: {cmd_res.stdout.strip()}\")\n",
                "            print(f\"  STDERR: {cmd_res.stderr.strip()}\")\n",
                "            \n",
                "            # Check output for success indicators (these might vary slightly by Neo4j version/OS)\n",
                "            if cmd == \"stop\":\n",
                "                # Check if it's already stopped or successfully stopped\n",
                "                if \"stopped\" not in cmd_res.stdout.lower() and \"not running\" not in cmd_res.stderr.lower():\n",
                "                     # If stop failed, maybe it needs more time or manual intervention\n",
                "                     print(f\"Warning: Neo4j stop command finished, but output doesn't confirm stop. Check service status manually.\")\n",
                "                     # Allow script to continue, but import might fail if service still running\n",
                "                else:\n",
                "                    print(\"Neo4j service appears stopped.\")\n",
                "                    sleep(5) # Give some time for files to be released\n",
                "            elif cmd == \"start\":\n",
                "                if \"started\" not in cmd_res.stdout.lower():\n",
                "                    raise RuntimeError(f\"Failed to start Neo4j service. Check logs. STDERR: {cmd_res.stderr.strip()}\")\n",
                "                else:\n",
                "                    print(\"Neo4j service appears started. Waiting for it to become available...\")\n",
                "                    sleep(15) # Give Neo4j time to initialize\n",
                "                    \n",
                "        except FileNotFoundError:\n",
                "            raise FileNotFoundError(f\"Neo4j command '{neo4j_cmd_path}' not found.\")\n",
                "        except subprocess.CalledProcessError as e:\n",
                "            raise RuntimeError(f\"Error executing Neo4j {cmd} command: {e}. STDOUT: {e.stdout}. STDERR: {e.stderr}\")\n",
                "        except Exception as e:\n",
                "            raise RuntimeError(f\"An unexpected error occurred during Neo4j {cmd}: {e}\")\n",
                "    else:\n",
                "        raise ValueError(f'Unknown command for Neo4j service: [{cmd}]')\n",
                "\n",
                "def reset_neo4j_database():\n",
                "    \"\"\"Deletes the data directories for the target graph.\"\"\"\n",
                "    if neo4j_home is None or not Path(neo4j_home).is_dir():\n",
                "        raise ValueError(\"Neo4j home directory ('neo4j_home') is not set or invalid. Please configure it in cell [2].\")\n",
                "        \n",
                "    # Construct paths based on standard Neo4j directory structure\n",
                "    neo4j_db_dir = Path(neo4j_home) / \"data\" / \"databases\" / graph_name\n",
                "    neo4j_tx_dir = Path(neo4j_home) / \"data\" / \"transactions\" / graph_name\n",
                "    \n",
                "    print(f\"Attempting to delete database directory: {neo4j_db_dir}\")\n",
                "    if neo4j_db_dir.exists() and neo4j_db_dir.is_dir():\n",
                "        try:\n",
                "            shutil.rmtree(neo4j_db_dir)\n",
                "            print(\"  Database directory deleted.\")\n",
                "        except OSError as e:\n",
                "            raise OSError(f\"Error deleting database directory {neo4j_db_dir}: {e}. Check permissions and ensure Neo4j is stopped.\")\n",
                "    else:\n",
                "        print(\"  Database directory does not exist, skipping deletion.\")\n",
                "        \n",
                "    print(f\"Attempting to delete transactions directory: {neo4j_tx_dir}\")\n",
                "    if neo4j_tx_dir.exists() and neo4j_tx_dir.is_dir():\n",
                "        try:\n",
                "            shutil.rmtree(neo4j_tx_dir)\n",
                "            print(\"  Transactions directory deleted.\")\n",
                "        except OSError as e:\n",
                "            # This might sometimes fail if logs are held; often okay if db dir was removed.\n",
                "            print(f\"Warning: Could not delete transactions directory {neo4j_tx_dir}: {e}. Import might still succeed.\")\n",
                "    else:\n",
                "        print(\"  Transactions directory does not exist, skipping deletion.\")\n",
                "\n",
                "import os\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Assume neo4j_home, graph_name, nodes_files, relationship_csv_file are defined correctly\n",
                "\n",
                "def import_data():\n",
                "    # Ensure neo4j_home is valid before proceeding\n",
                "    if not neo4j_home or not Path(neo4j_home).is_dir():\n",
                "         raise ValueError(f\"Neo4j home directory ('neo4j_home') is not set or invalid: {neo4j_home}\")\n",
                "\n",
                "    import_tool = Path(neo4j_home) / \"bin\" / (\"neo4j-admin.bat\" if os.name == \"nt\" else \"neo4j-admin\")\n",
                "    if not import_tool.is_file():\n",
                "        raise FileNotFoundError(f\"neo4j-admin executable not found at: {import_tool}\")\n",
                "\n",
                "    csv_dir = Path.cwd() # where the notebook generated the CSVs\n",
                "\n",
                "    args = [\n",
                "        str(import_tool),\n",
                "        \"database\", \"import\", \"full\",\n",
                "        # Removed --database=graph_name from here\n",
                "        \"--overwrite-destination=true\",\n",
                "        \"--multiline-fields=true\",       # Keep this if your text fields might have newlines\n",
                "        \"--skip-duplicate-nodes=true\",\n",
                "        \"--skip-bad-relationships=true\",\n",
                "        # Add other flags as needed, BEFORE nodes/relationships\n",
                "    ]\n",
                "\n",
                "    # Add each node file with an **absolute** path\n",
                "    for f in nodes_files:\n",
                "        # Ensure the path is absolute and correctly formatted for the OS\n",
                "        node_file_path = (csv_dir / f).resolve()\n",
                "        args.append(f\"--nodes={node_file_path}\")\n",
                "\n",
                "    # Add the relationship file with an **absolute** path\n",
                "    rel_file_path = (csv_dir / relationship_csv_file).resolve()\n",
                "    args.append(f\"--relationships={rel_file_path}\")\n",
                "\n",
                "    # --- FIX ---\n",
                "    # Add the database name as the VERY LAST positional argument\n",
                "    # args.append(graph_name)\n",
                "    # --- END FIX ---\n",
                "\n",
                "    print(\"Running:\", \" \".join(map(str, args))) # Use map(str,..) in case paths are Path objects\n",
                "\n",
                "    env = os.environ.copy()\n",
                "    # Ensure JAVA_HOME is correctly set if needed by your environment/neo4j-admin script\n",
                "    # Double-check this path is correct for your system where the JDK is installed\n",
                "    java_home_path = \"C:/Program Files/Eclipse Adoptium/jdk-21.0.7.6-hotspot\"\n",
                "    if Path(java_home_path).is_dir():\n",
                "         env[\"JAVA_HOME\"] = java_home_path\n",
                "         print(f\"Setting JAVA_HOME to: {java_home_path}\")\n",
                "    else:\n",
                "         print(f\"Warning: Specified JAVA_HOME path not found: {java_home_path}. neo4j-admin might fail.\")\n",
                "         # You might remove the JAVA_HOME setting if neo4j-admin finds it automatically,\n",
                "         # but explicitly setting it is often more reliable if needed.\n",
                "\n",
                "    # Use shell=False (safer and often handles arguments better)\n",
                "    # Ensure neo4j service is STOPPED before running this\n",
                "    try:\n",
                "        res = subprocess.run(args, text=True, capture_output=True, env=env, check=False, shell=False) # Use check=False to manually check returncode\n",
                "\n",
                "        if res.returncode != 0:\n",
                "            # Print command again for easier debugging on failure\n",
                "            print(\"\\n--- FAILED COMMAND ---\")\n",
                "            print(\" \".join(map(str, args)))\n",
                "            print(\"----------------------\\n\")\n",
                "            raise RuntimeError(f\"Import failed ({res.returncode})\\nSTDOUT:\\n{res.stdout}\\nSTDERR:\\n{res.stderr}\")\n",
                "        else:\n",
                "             print(\"\\n--- Import Command Output ---\")\n",
                "             print(res.stdout)\n",
                "             print(\"---------------------------\\n\")\n",
                "             if res.stderr: # Print stderr even on success, might contain warnings\n",
                "                 print(\"--- Import Command STDERR (Warnings/Info) ---\")\n",
                "                 print(res.stderr)\n",
                "                 print(\"---------------------------------------------\\n\")\n",
                "\n",
                "    except FileNotFoundError as fnf_error:\n",
                "         print(f\"Error: Command not found. Ensure neo4j-admin path is correct. Details: {fnf_error}\")\n",
                "         raise\n",
                "    except Exception as e:\n",
                "         print(f\"An unexpected error occurred running the import command: {e}\")\n",
                "         raise # Re-raise the caught exception"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking current graph 'neo4j' node count...\n",
                        "An unexpected error occurred checking node count: Cannot open connection to ConnectionProfile('bolt://localhost:7687'). Assuming import is needed.\n",
                        "\n",
                        "--- Starting Neo4j Import Process ---\n",
                        "Attempting to delete database directory: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\databases\\neo4j\n",
                        "  Database directory does not exist, skipping deletion.\n",
                        "Attempting to delete transactions directory: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\data\\transactions\\neo4j\n",
                        "  Transactions directory does not exist, skipping deletion.\n",
                        "Running: C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\bin\\neo4j-admin.bat database import full --overwrite-destination=true --multiline-fields=true --skip-duplicate-nodes=true --skip-bad-relationships=true --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\business_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\category_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\city_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\state_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\country_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\user_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\review_nodes.csv --relationships=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\relationships.csv neo4j\n",
                        "Setting JAVA_HOME to: C:/Program Files/Eclipse Adoptium/jdk-21.0.7.6-hotspot\n",
                        "\n",
                        "--- FAILED COMMAND ---\n",
                        "C:\\Users\\wiztu\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-d9cecb50-c260-4dd0-b3e0-6b387c58d5ec\\bin\\neo4j-admin.bat database import full --overwrite-destination=true --multiline-fields=true --skip-duplicate-nodes=true --skip-bad-relationships=true --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\business_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\category_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\city_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\state_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\country_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\user_nodes.csv --nodes=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\review_nodes.csv --relationships=C:\\Users\\wiztu\\OneDrive\\Documents\\Data Science and Management\\Project\\scripts\\relationships.csv neo4j\n",
                        "----------------------\n",
                        "\n",
                        "An unexpected error occurred running the import command: Import failed (70)\n",
                        "STDOUT:\n",
                        "\n",
                        "STDERR:\n",
                        "\u001b[31m\u001b[1mjava.lang.IllegalArgumentException: File 'neo4j' doesn't exist\u001b[21m\u001b[39m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.kernel.impl.util.Validators.matchingFiles(Validators.java:52)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.kernel.impl.util.Converters.lambda$regexFiles$2(Converters.java:49)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.kernel.impl.util.Converters.lambda$toFiles$3(Converters.java:65)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand.parseFilesList(ImportCommand.java:938)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand$InputFilesGroup.toPaths(ImportCommand.java:905)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand$Base.doExecute(ImportCommand.java:488)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand$Full.execute(ImportCommand.java:710)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AbstractCommand.call(AbstractCommand.java:92)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AbstractCommand.call(AbstractCommand.java:37)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine.executeUserObject(CommandLine.java:2045)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine.access$1500(CommandLine.java:148)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2465)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2457)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2419)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2277)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.execute(CommandLine.java:2421)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine.execute(CommandLine.java:2174)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AdminTool.execute(AdminTool.java:94)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AdminTool.main(AdminTool.java:82)\u001b[23m\u001b[0m\n",
                        "\n",
                        "\n",
                        "--- IMPORT PROCESS FAILED ---: Import failed (70)\n",
                        "STDOUT:\n",
                        "\n",
                        "STDERR:\n",
                        "\u001b[31m\u001b[1mjava.lang.IllegalArgumentException: File 'neo4j' doesn't exist\u001b[21m\u001b[39m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.kernel.impl.util.Validators.matchingFiles(Validators.java:52)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.kernel.impl.util.Converters.lambda$regexFiles$2(Converters.java:49)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.kernel.impl.util.Converters.lambda$toFiles$3(Converters.java:65)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand.parseFilesList(ImportCommand.java:938)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand$InputFilesGroup.toPaths(ImportCommand.java:905)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand$Base.doExecute(ImportCommand.java:488)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.importer.ImportCommand$Full.execute(ImportCommand.java:710)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AbstractCommand.call(AbstractCommand.java:92)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AbstractCommand.call(AbstractCommand.java:37)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine.executeUserObject(CommandLine.java:2045)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine.access$1500(CommandLine.java:148)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.executeUserObjectOfLastSubcommandWithSameParent(CommandLine.java:2465)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2457)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.handle(CommandLine.java:2419)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$AbstractParseResultHandler.execute(CommandLine.java:2277)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine$RunLast.execute(CommandLine.java:2421)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat picocli.CommandLine.execute(CommandLine.java:2174)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AdminTool.execute(AdminTool.java:94)\u001b[23m\u001b[0m\n",
                        "\u001b[3m\tat org.neo4j.cli.AdminTool.main(AdminTool.java:82)\u001b[23m\u001b[0m\n",
                        "\n",
                        "Attempting to start Neo4j service if it was stopped...\n",
                        "whatever\n"
                    ]
                }
            ],
            "source": [
                "# --- Import Execution ---\n",
                "\n",
                "# Check if import is necessary by comparing expected node count with current graph\n",
                "run_import = False\n",
                "try:\n",
                "    # Ensure num_nodes_total was calculated\n",
                "    if 'num_nodes_total' not in globals() or num_nodes_total == 0:\n",
                "         print(\"Warning: Expected node count not calculated. Running integrity check first.\")\n",
                "         check_nodes_relationships_csv_files_integrity()\n",
                "         \n",
                "    print(f\"Checking current graph '{graph_name}' node count...\")\n",
                "    graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH) # Reconnect\n",
                "    current_node_count = graph.nodes.match().count()\n",
                "    print(f\"  Current node count: {current_node_count}\")\n",
                "    print(f\"  Expected node count from CSVs: {num_nodes_total}\")\n",
                "    \n",
                "    if current_node_count != num_nodes_total or num_nodes_total == 0:\n",
                "        print(\"Node count mismatch or expected count is zero. Proceeding with import.\")\n",
                "        run_import = True\n",
                "    else:\n",
                "        print(\"Node count matches expected count. Skipping import.\")\n",
                "        \n",
                "# Catch specific retryable errors, general driver errors, and OS-level connection errors\n",
                "except (ServiceUnavailable, TransientError, Neo4jError, OSError) as e:\n",
                "    print(f\"Could not connect to Neo4j to check node count ({e}). Assuming import is needed.\")\n",
                "    run_import = True\n",
                "except NameError:\n",
                "     print(\"Expected node count variable 'num_nodes_total' not defined. Assuming import is needed.\")\n",
                "     run_import = True\n",
                "except Exception as e:\n",
                "    print(f\"An unexpected error occurred checking node count: {e}. Assuming import is needed.\")\n",
                "    run_import = True\n",
                "\n",
                "# Execute the import process if needed\n",
                "if run_import:\n",
                "    try:\n",
                "        print(\"\\n--- Starting Neo4j Import Process ---\")\n",
                "        #command_neo4j_database_service(\"stop\")\n",
                "        reset_neo4j_database()\n",
                "        import_data()\n",
                "        #command_neo4j_database_service(\"start\")\n",
                "        print(\"--- Neo4j Import Process Finished ---\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\n--- IMPORT PROCESS FAILED ---: {e}\")\n",
                "        print(\"Attempting to start Neo4j service if it was stopped...\")\n",
                "        try:\n",
                "            # Try to start it anyway, might fail if already running or other issues\n",
                "            #command_neo4j_database_service(\"start\") \n",
                "            print(\"whatever\")\n",
                "        except Exception as start_err:\n",
                "            print(f\"Could not restart Neo4j service after failed import: {start_err}\")\n",
                "        # Re-raise the original exception\n",
                "        # raise e \n",
                "else:\n",
                "    print(\"\\nImport skipped as node count matches.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Final Verification\n",
                "\n",
                "Attempt to connect to the Neo4j database and verify the node count again after the import process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Verifying import results (expected nodes: 9130494)...\n",
                        "Attempt 1/30: Connecting to Neo4j...\n",
                        "  An unexpected error occurred during verification: Cannot open connection to ConnectionProfile('bolt://localhost:7687')\n",
                        "\n",
                        "Verification failed after 30 attempts. Could not confirm successful import.\n",
                        "Last recorded node count: -1\n",
                        "\n",
                        "--- FINAL VERIFICATION FAILED ---\n"
                    ]
                }
            ],
            "source": [
                "yelp_graph_ready = False\n",
                "final_node_count = -1\n",
                "\n",
                "def check_if_importing_is_successful():\n",
                "    \"\"\"Tries to connect to Neo4j and checks if the node count matches the expected count.\"\"\"\n",
                "    global graph, yelp_graph_ready, final_node_count\n",
                "    \n",
                "    # Ensure expected count is available\n",
                "    if 'num_nodes_total' not in globals() or num_nodes_total == 0:\n",
                "        try:\n",
                "             print(\"Recalculating expected node count for final verification...\")\n",
                "             check_nodes_relationships_csv_files_integrity()\n",
                "        except Exception as e:\n",
                "             print(f\"Error recalculating expected node count: {e}. Cannot verify import accurately.\")\n",
                "             return # Cannot verify\n",
                "             \n",
                "    print(f\"\\nVerifying import results (expected nodes: {num_nodes_total})...\")\n",
                "    num_tries = 30 # Try for 30 seconds (adjust as needed)\n",
                "    wait_interval = 2 # Wait 2 seconds between tries\n",
                "    \n",
                "    for one_try in range(num_tries):\n",
                "        try:\n",
                "            print(f\"Attempt {one_try + 1}/{num_tries}: Connecting to Neo4j...\")\n",
                "            graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)\n",
                "            final_node_count = graph.nodes.match().count()\n",
                "            print(f\"  Successfully connected. Found {final_node_count} nodes.\")\n",
                "            \n",
                "            if final_node_count == 0 and num_nodes_total > 0:\n",
                "                print(\"  Warning: Graph is empty, but expected nodes. Import might have failed silently or service is still starting.\")\n",
                "                # Continue trying for a bit longer\n",
                "            elif final_node_count != num_nodes_total:\n",
                "                print(f\"  Warning: Node count mismatch! Expected {num_nodes_total}, found {final_node_count}.\")\n",
                "                # Consider this potentially successful but log warning\n",
                "                yelp_graph_ready = True # Mark as ready but with a warning\n",
                "                return\n",
                "            else: # Counts match\n",
                "                print(\"  Node count matches expected count. Import successful!\")\n",
                "                yelp_graph_ready = True\n",
                "                return # Success\n",
                "                \n",
                "        # Catch specific retryable errors, general driver errors, and OS-level connection errors\n",
                "        except (ServiceUnavailable, TransientError, Neo4jError, OSError) as e:\n",
                "            print(f\"  Connection failed ({e}). Neo4j might still be starting. Retrying in {wait_interval}s...\")\n",
                "        except Exception as e:\n",
                "            print(f\"  An unexpected error occurred during verification: {e}\")\n",
                "            # Stop trying on unexpected errors\n",
                "            break \n",
                "            \n",
                "        # Wait before next try only if not successful yet\n",
                "        if not yelp_graph_ready:\n",
                "             sleep(wait_interval)\n",
                "             \n",
                "    # If loop finishes without success\n",
                "    if not yelp_graph_ready:\n",
                "        print(f\"\\nVerification failed after {num_tries} attempts. Could not confirm successful import.\")\n",
                "        print(f\"Last recorded node count: {final_node_count}\")\n",
                "\n",
                "# Run the final check\n",
                "check_if_importing_is_successful()\n",
                "\n",
                "if not yelp_graph_ready:\n",
                "    print(\"\\n--- FINAL VERIFICATION FAILED ---\")\n",
                "    # Optional: raise Exception(\"Failed to verify Neo4j import success.\")\n",
                "else:\n",
                "     print(\"\\n--- FINAL VERIFICATION COMPLETE --- \")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Conclusion\n",
                "\n",
                "This notebook processed the Yelp Business, User, and Review datasets, addressed data quality issues (ID uniqueness, dangling friendships, duplicate categories), generated CSV files suitable for Neo4j's bulk import tool, and executed the import process.\n",
                "\n",
                "The resulting graph structure connects Businesses to Categories and Locations (City -> State -> Country), Users to Friends and Reviews, and Reviews back to Businesses.\n",
                "\n",
                "You can now explore the graph using Neo4j Browser or Cypher queries. Try `CALL db.schema.visualization()` in the Neo4j Browser to see the schema."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
