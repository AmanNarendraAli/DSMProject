{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Dataset Exploratory Data Analysis (EDA)\n",
                "\n",
                "This notebook performs EDA on the Yelp dataset, focusing on businesses, reviews, users, check-ins, and tips.\n",
                "\n",
                "**Objectives:**\n",
                "- Load and inspect the different JSON files.\n",
                "- Perform initial exploration and basic cleaning.\n",
                "- Analyze relationships between different data entities.\n",
                "- Conduct basic text analysis on reviews and tips.\n",
                "- Visualize key findings.\n",
                "- Summarize observations and suggest next steps for cleaning and analysis."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup\n",
                "\n",
                "Import necessary libraries and define file paths."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Business file exists: True\n",
                        "Review file exists: True\n",
                        "User file exists: True\n",
                        "Checkin file exists: True\n",
                        "Tip file exists: True\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "\n",
                "# Set plot style\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "# Define the base path to the dataset\n",
                "base_path = '../yelp_dataset/' # Adjust if your notebook is in a different location relative to the data\n",
                "\n",
                "# Define file paths\n",
                "business_path = os.path.join(base_path, 'yelp_academic_dataset_business.json')\n",
                "review_path = os.path.join(base_path, 'yelp_academic_dataset_review.json')\n",
                "user_path = os.path.join(base_path, 'yelp_academic_dataset_user.json')\n",
                "checkin_path = os.path.join(base_path, 'yelp_academic_dataset_checkin.json')\n",
                "tip_path = os.path.join(base_path, 'yelp_academic_dataset_tip.json')\n",
                "\n",
                "# Verify paths exist (optional check)\n",
                "print(f\"Business file exists: {os.path.exists(business_path)}\")\n",
                "print(f\"Review file exists: {os.path.exists(review_path)}\")\n",
                "print(f\"User file exists: {os.path.exists(user_path)}\")\n",
                "print(f\"Checkin file exists: {os.path.exists(checkin_path)}\")\n",
                "print(f\"Tip file exists: {os.path.exists(tip_path)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "\n",
                "Load each JSON file into a pandas DataFrame. Since the files can be very large, we might load them line by line or use chunking if memory becomes an issue. For initial inspection, loading a subset might be sufficient."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Business data loaded successfully.\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 150346 entries, 0 to 150345\n",
                        "Data columns (total 14 columns):\n",
                        " #   Column        Non-Null Count   Dtype  \n",
                        "---  ------        --------------   -----  \n",
                        " 0   business_id   150346 non-null  object \n",
                        " 1   name          150346 non-null  object \n",
                        " 2   address       150346 non-null  object \n",
                        " 3   city          150346 non-null  object \n",
                        " 4   state         150346 non-null  object \n",
                        " 5   postal_code   150346 non-null  object \n",
                        " 6   latitude      150346 non-null  float64\n",
                        " 7   longitude     150346 non-null  float64\n",
                        " 8   stars         150346 non-null  float64\n",
                        " 9   review_count  150346 non-null  int64  \n",
                        " 10  is_open       150346 non-null  int64  \n",
                        " 11  attributes    136602 non-null  object \n",
                        " 12  categories    150243 non-null  object \n",
                        " 13  hours         127123 non-null  object \n",
                        "dtypes: float64(3), int64(2), object(9)\n",
                        "memory usage: 16.1+ MB\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>business_id</th>\n",
                            "      <th>name</th>\n",
                            "      <th>address</th>\n",
                            "      <th>city</th>\n",
                            "      <th>state</th>\n",
                            "      <th>postal_code</th>\n",
                            "      <th>latitude</th>\n",
                            "      <th>longitude</th>\n",
                            "      <th>stars</th>\n",
                            "      <th>review_count</th>\n",
                            "      <th>is_open</th>\n",
                            "      <th>attributes</th>\n",
                            "      <th>categories</th>\n",
                            "      <th>hours</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Pns2l4eNsfO8kk83dixA6A</td>\n",
                            "      <td>Abby Rappoport, LAC, CMQ</td>\n",
                            "      <td>1616 Chapala St, Ste 2</td>\n",
                            "      <td>Santa Barbara</td>\n",
                            "      <td>CA</td>\n",
                            "      <td>93101</td>\n",
                            "      <td>34.426679</td>\n",
                            "      <td>-119.711197</td>\n",
                            "      <td>5.0</td>\n",
                            "      <td>7</td>\n",
                            "      <td>0</td>\n",
                            "      <td>{'ByAppointmentOnly': 'True'}</td>\n",
                            "      <td>Doctors, Traditional Chinese Medicine, Naturop...</td>\n",
                            "      <td>None</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>mpf3x-BjTdTEA3yCZrAYPw</td>\n",
                            "      <td>The UPS Store</td>\n",
                            "      <td>87 Grasso Plaza Shopping Center</td>\n",
                            "      <td>Affton</td>\n",
                            "      <td>MO</td>\n",
                            "      <td>63123</td>\n",
                            "      <td>38.551126</td>\n",
                            "      <td>-90.335695</td>\n",
                            "      <td>3.0</td>\n",
                            "      <td>15</td>\n",
                            "      <td>1</td>\n",
                            "      <td>{'BusinessAcceptsCreditCards': 'True'}</td>\n",
                            "      <td>Shipping Centers, Local Services, Notaries, Ma...</td>\n",
                            "      <td>{'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>tUFrWirKiKi_TAnsVWINQQ</td>\n",
                            "      <td>Target</td>\n",
                            "      <td>5255 E Broadway Blvd</td>\n",
                            "      <td>Tucson</td>\n",
                            "      <td>AZ</td>\n",
                            "      <td>85711</td>\n",
                            "      <td>32.223236</td>\n",
                            "      <td>-110.880452</td>\n",
                            "      <td>3.5</td>\n",
                            "      <td>22</td>\n",
                            "      <td>0</td>\n",
                            "      <td>{'BikeParking': 'True', 'BusinessAcceptsCredit...</td>\n",
                            "      <td>Department Stores, Shopping, Fashion, Home &amp; G...</td>\n",
                            "      <td>{'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>MTSW4McQd7CbVtyjqoe9mw</td>\n",
                            "      <td>St Honore Pastries</td>\n",
                            "      <td>935 Race St</td>\n",
                            "      <td>Philadelphia</td>\n",
                            "      <td>PA</td>\n",
                            "      <td>19107</td>\n",
                            "      <td>39.955505</td>\n",
                            "      <td>-75.155564</td>\n",
                            "      <td>4.0</td>\n",
                            "      <td>80</td>\n",
                            "      <td>1</td>\n",
                            "      <td>{'RestaurantsDelivery': 'False', 'OutdoorSeati...</td>\n",
                            "      <td>Restaurants, Food, Bubble Tea, Coffee &amp; Tea, B...</td>\n",
                            "      <td>{'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>mWMc6_wTdE0EUBKIGXDVfA</td>\n",
                            "      <td>Perkiomen Valley Brewery</td>\n",
                            "      <td>101 Walnut St</td>\n",
                            "      <td>Green Lane</td>\n",
                            "      <td>PA</td>\n",
                            "      <td>18054</td>\n",
                            "      <td>40.338183</td>\n",
                            "      <td>-75.471659</td>\n",
                            "      <td>4.5</td>\n",
                            "      <td>13</td>\n",
                            "      <td>1</td>\n",
                            "      <td>{'BusinessAcceptsCreditCards': 'True', 'Wheelc...</td>\n",
                            "      <td>Brewpubs, Breweries, Food</td>\n",
                            "      <td>{'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "              business_id                      name  \\\n",
                            "0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n",
                            "1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
                            "2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n",
                            "3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n",
                            "4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n",
                            "\n",
                            "                           address           city state postal_code  \\\n",
                            "0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n",
                            "1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n",
                            "2             5255 E Broadway Blvd         Tucson    AZ       85711   \n",
                            "3                      935 Race St   Philadelphia    PA       19107   \n",
                            "4                    101 Walnut St     Green Lane    PA       18054   \n",
                            "\n",
                            "    latitude   longitude  stars  review_count  is_open  \\\n",
                            "0  34.426679 -119.711197    5.0             7        0   \n",
                            "1  38.551126  -90.335695    3.0            15        1   \n",
                            "2  32.223236 -110.880452    3.5            22        0   \n",
                            "3  39.955505  -75.155564    4.0            80        1   \n",
                            "4  40.338183  -75.471659    4.5            13        1   \n",
                            "\n",
                            "                                          attributes  \\\n",
                            "0                      {'ByAppointmentOnly': 'True'}   \n",
                            "1             {'BusinessAcceptsCreditCards': 'True'}   \n",
                            "2  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n",
                            "3  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n",
                            "4  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n",
                            "\n",
                            "                                          categories  \\\n",
                            "0  Doctors, Traditional Chinese Medicine, Naturop...   \n",
                            "1  Shipping Centers, Local Services, Notaries, Ma...   \n",
                            "2  Department Stores, Shopping, Fashion, Home & G...   \n",
                            "3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n",
                            "4                          Brewpubs, Breweries, Food   \n",
                            "\n",
                            "                                               hours  \n",
                            "0                                               None  \n",
                            "1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n",
                            "2  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n",
                            "3  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n",
                            "4  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Load business data\n",
                "# Using lines=True as each line is a separate JSON object\n",
                "try:\n",
                "    df_business = pd.read_json(business_path, lines=True)\n",
                "    print(\"Business data loaded successfully.\")\n",
                "    df_business.info()\n",
                "    display(df_business.head())\n",
                "except Exception as e:\n",
                "    print(f\"Error loading business data: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Review data iterator created with chunksize=100000.\n"
                    ]
                }
            ],
            "source": [
                "# Load review data using chunking\n",
                "# This avoids loading the entire large file into memory at once\n",
                "chunk_size = 100000  # Process 100,000 reviews at a time\n",
                "review_iterator = None\n",
                "try:\n",
                "    review_iterator = pd.read_json(review_path, lines=True, chunksize=chunk_size)\n",
                "    print(f\"Review data iterator created with chunksize={chunk_size}.\")\n",
                "    # We won't display head() or info() here as it requires loading the first chunk\n",
                "except Exception as e:\n",
                "    print(f\"Error creating review data iterator: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load user data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     df_user \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser data loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     df_user\u001b[38;5;241m.\u001b[39minfo()\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_json.py:780\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    778\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_json.py:894\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    893\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m--> 894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\json\\_json.py:906\u001b[0m, in \u001b[0;36mJsonReader._preprocess_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 906\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n\u001b[0;32m    908\u001b[0m     data \u001b[38;5;241m=\u001b[39m StringIO(data)\n",
                        "File \u001b[1;32mc:\\Users\\wiztu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Load user data\n",
                "try:\n",
                "    df_user = pd.read_json(user_path, lines=True)\n",
                "    print(\"User data loaded successfully.\")\n",
                "    df_user.info()\n",
                "    display(df_user.head())\n",
                "except Exception as e:\n",
                "    print(f\"Error loading user data: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load checkin data\n",
                "try:\n",
                "    df_checkin = pd.read_json(checkin_path, lines=True)\n",
                "    print(\"Checkin data loaded successfully.\")\n",
                "    df_checkin.info()\n",
                "    display(df_checkin.head())\n",
                "except Exception as e:\n",
                "    print(f\"Error loading checkin data: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tip data\n",
                "try:\n",
                "    df_tip = pd.read_json(tip_path, lines=True)\n",
                "    print(\"Tip data loaded successfully.\")\n",
                "    df_tip.info()\n",
                "    display(df_tip.head())\n",
                "except Exception as e:\n",
                "    print(f\"Error loading tip data: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initial Exploration & Basic Cleaning\n",
                "\n",
                "Perform basic checks, look at distributions, and handle missing values if necessary."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Business Data (`df_business`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Re-check basic info and check for missing values\n",
                "print(\"Business Data Info:\")\n",
                "df_business.info()\n",
                "print(\"\\nMissing Values:\")\n",
                "print(df_business.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore numerical features\n",
                "print(\"\\nNumerical Features Description:\")\n",
                "display(df_business[['stars', 'review_count']].describe())\n",
                "\n",
                "# Plot distributions\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "sns.histplot(df_business['stars'], bins=9, kde=False)\n",
                "plt.title('Distribution of Business Stars')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "# Using log scale for review_count due to potential skewness\n",
                "sns.histplot(df_business['review_count'], bins=50, log_scale=True)\n",
                "plt.title('Distribution of Review Count (Log Scale)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore categorical features: City, State, is_open\n",
                "print(\"\\nTop 10 Cities:\")\n",
                "print(df_business['city'].value_counts().head(10))\n",
                "\n",
                "print(\"\\nTop 10 States:\")\n",
                "print(df_business['state'].value_counts().head(10))\n",
                "\n",
                "print(\"\\nIs Open Distribution:\")\n",
                "print(df_business['is_open'].value_counts(normalize=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Categories - focusing on restaurants\n",
                "# The 'categories' column contains comma-separated strings or None\n",
                "print(\"\\nHandling Categories:\")\n",
                "# Fill NaN values with an empty string to avoid errors\n",
                "df_business['categories'] = df_business['categories'].fillna('')\n",
                "\n",
                "# Check how many businesses are categorized as 'Restaurants'\n",
                "is_restaurant = df_business['categories'].str.contains('Restaurants', case=False, na=False)\n",
                "print(f\"Number of businesses categorized as Restaurants: {is_restaurant.sum()}\")\n",
                "print(f\"Percentage of businesses categorized as Restaurants: {is_restaurant.mean():.2%}\")\n",
                "\n",
                "# Look at the most common categories overall (requires splitting the string)\n",
                "all_categories = df_business['categories'].str.split(', ').explode()\n",
                "print(\"\\nTop 20 Most Common Categories Overall:\")\n",
                "print(all_categories.value_counts().head(20))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Review Data (`df_review`)\n",
                "\n",
                "Since the review data is loaded as an iterator, we need to process it chunk by chunk to get overall statistics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize variables for aggregation\n",
                "total_reviews = 0\n",
                "review_columns = []\n",
                "review_dtypes = {}\n",
                "review_non_null_counts = Counter()\n",
                "review_star_counts = Counter()\n",
                "review_useful_counts = Counter()\n",
                "review_funny_counts = Counter()\n",
                "review_cool_counts = Counter()\n",
                "review_year_counts = Counter()\n",
                "review_text_length_sum = 0\n",
                "review_text_length_sum_sq = 0\n",
                "review_text_length_min = float('inf')\n",
                "review_text_length_max = float('-inf')\n",
                "review_date_min = pd.Timestamp.max\n",
                "review_date_max = pd.Timestamp.min\n",
                "\n",
                "print(\"Processing review chunks...\")\n",
                "if review_iterator:\n",
                "    for i, chunk in enumerate(review_iterator):\n",
                "        print(f\"Processing chunk {i+1}...\", end='\\r')\n",
                "        if i == 0:\n",
                "            # Get column names and dtypes from the first chunk\n",
                "            review_columns = chunk.columns.tolist()\n",
                "            review_dtypes = chunk.dtypes.to_dict()\n",
                "        \n",
                "        # Aggregations\n",
                "        total_reviews += len(chunk)\n",
                "        review_non_null_counts.update(chunk.notnull().sum().to_dict())\n",
                "        review_star_counts.update(chunk['stars'].value_counts().to_dict())\n",
                "        review_useful_counts.update(chunk['useful'].value_counts().to_dict())\n",
                "        review_funny_counts.update(chunk['funny'].value_counts().to_dict())\n",
                "        review_cool_counts.update(chunk['cool'].value_counts().to_dict())\n",
                "        \n",
                "        # Date processing\n",
                "        chunk['date'] = pd.to_datetime(chunk['date'])\n",
                "        chunk['year'] = chunk['date'].dt.year\n",
                "        review_year_counts.update(chunk['year'].value_counts().to_dict())\n",
                "        chunk_date_min = chunk['date'].min()\n",
                "        chunk_date_max = chunk['date'].max()\n",
                "        if chunk_date_min < review_date_min:\n",
                "            review_date_min = chunk_date_min\n",
                "        if chunk_date_max > review_date_max:\n",
                "            review_date_max = chunk_date_max\n",
                "            \n",
                "        # Text length processing\n",
                "        chunk['text_length'] = chunk['text'].str.len().fillna(0) # Fill NaN text with 0 length\n",
                "        review_text_length_sum += chunk['text_length'].sum()\n",
                "        review_text_length_sum_sq += (chunk['text_length']**2).sum()\n",
                "        chunk_text_min = chunk['text_length'].min()\n",
                "        chunk_text_max = chunk['text_length'].max()\n",
                "        if chunk_text_min < review_text_length_min:\n",
                "            review_text_length_min = chunk_text_min\n",
                "        if chunk_text_max > review_text_length_max:\n",
                "            review_text_length_max = chunk_text_max\n",
                "            \n",
                "    print(\"\\nReview chunk processing complete.\")\n",
                "    \n",
                "    # Calculate derived statistics\n",
                "    review_text_length_mean = review_text_length_sum / total_reviews\n",
                "    review_text_length_var = (review_text_length_sum_sq / total_reviews) - (review_text_length_mean**2)\n",
                "    review_text_length_std = np.sqrt(review_text_length_var) if review_text_length_var >= 0 else 0\n",
                "    \n",
                "    print(f\"Total reviews processed: {total_reviews}\")\n",
                "else:\n",
                "    print(\"Review iterator not created. Cannot process reviews.\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display aggregated basic info and missing values\n",
                "print(\"Aggregated Review Data Info:\")\n",
                "print(f\"Total Reviews: {total_reviews}\")\n",
                "print(f\"Columns: {review_columns}\")\n",
                "# print(f\"Data Types: {review_dtypes}\") # Can be long, uncomment if needed\n",
                "\n",
                "print(\"\\nAggregated Non-Null Counts:\")\n",
                "for col, count in review_non_null_counts.items():\n",
                "    print(f\"{col}: {count}\")\n",
                "\n",
                "print(\"\\nAggregated Missing Value Counts:\")\n",
                "for col in review_columns:\n",
                "    missing_count = total_reviews - review_non_null_counts.get(col, 0)\n",
                "    if missing_count > 0:\n",
                "        print(f\"{col}: {missing_count}\")\n",
                "if all(total_reviews - review_non_null_counts.get(col, 0) == 0 for col in review_columns):\n",
                "    print(\"No missing values found in processed columns.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot distributions based on aggregated counts\n",
                "print(\"\\nPlotting Aggregated Distributions:\")\n",
                "\n",
                "plt.figure(figsize=(15, 8))\n",
                "\n",
                "# Stars Distribution\n",
                "plt.subplot(2, 2, 1)\n",
                "star_data = sorted(review_star_counts.items())\n",
                "stars = [item[0] for item in star_data]\n",
                "counts = [item[1] for item in star_data]\n",
                "sns.barplot(x=stars, y=counts, palette='viridis')\n",
                "plt.title('Distribution of Review Stars (Full Dataset)')\n",
                "plt.xlabel('Stars')\n",
                "plt.ylabel('Count')\n",
                "\n",
                "# Useful Votes Distribution (Log Scale)\n",
                "# Plotting histogram from counts requires care, especially with log scale.\n",
                "# Let's plot the counts for the most common values (e.g., 0 to 10 useful votes)\n",
                "plt.subplot(2, 2, 2)\n",
                "useful_data = sorted(review_useful_counts.items())\n",
                "useful_vals = [item[0] for item in useful_data if item[0] <= 10] # Limit for visibility\n",
                "useful_counts_val = [item[1] for item in useful_data if item[0] <= 10]\n",
                "sns.barplot(x=useful_vals, y=useful_counts_val, palette='Blues')\n",
                "plt.title('Distribution of Useful Votes (0-10, Full Dataset)')\n",
                "plt.xlabel('Useful Votes')\n",
                "plt.ylabel('Count (Log Scale)')\n",
                "plt.yscale('log') # Apply log scale to y-axis\n",
                "\n",
                "# Funny Votes Distribution\n",
                "plt.subplot(2, 2, 3)\n",
                "funny_data = sorted(review_funny_counts.items())\n",
                "funny_vals = [item[0] for item in funny_data if item[0] <= 10]\n",
                "funny_counts_val = [item[1] for item in funny_data if item[0] <= 10]\n",
                "sns.barplot(x=funny_vals, y=funny_counts_val, palette='Oranges')\n",
                "plt.title('Distribution of Funny Votes (0-10, Full Dataset)')\n",
                "plt.xlabel('Funny Votes')\n",
                "plt.ylabel('Count (Log Scale)')\n",
                "plt.yscale('log')\n",
                "\n",
                "# Cool Votes Distribution\n",
                "plt.subplot(2, 2, 4)\n",
                "cool_data = sorted(review_cool_counts.items())\n",
                "cool_vals = [item[0] for item in cool_data if item[0] <= 10]\n",
                "cool_counts_val = [item[1] for item in cool_data if item[0] <= 10]\n",
                "sns.barplot(x=cool_vals, y=cool_counts_val, palette='Greens')\n",
                "plt.title('Distribution of Cool Votes (0-10, Full Dataset)')\n",
                "plt.xlabel('Cool Votes')\n",
                "plt.ylabel('Count (Log Scale)')\n",
                "plt.yscale('log')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Date\n",
                "print(\"\\nExploring Review Dates:\")\n",
                "# Convert 'date' column to datetime objects\n",
                "df_review['date'] = pd.to_datetime(df_review['date'])\n",
                "\n",
                "# Extract year and month\n",
                "df_review['year'] = df_review['date'].dt.year\n",
                "df_review['month'] = df_review['date'].dt.month\n",
                "\n",
                "print(f\"Date range: {df_review['date'].min()} to {df_review['date'].max()}\")\n",
                "\n",
                "# Plot number of reviews over years\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.countplot(x='year', data=df_review, palette='magma')\n",
                "plt.title('Number of Reviews per Year (Subset)')\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Review Text Length\n",
                "df_review['text_length'] = df_review['text'].str.len()\n",
                "\n",
                "print(\"\\nReview Text Length Description:\")\n",
                "display(df_review['text_length'].describe())\n",
                "\n",
                "# Plot distribution of text length\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.histplot(df_review['text_length'], bins=100, kde=True)\n",
                "plt.title('Distribution of Review Text Length (Subset)')\n",
                "plt.xlabel('Text Length (Number of Characters)')\n",
                "plt.xlim(0, 5000) # Limit x-axis for better visibility, as max length is capped by Yelp\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 User Data (`df_user`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Re-check basic info and check for missing values\n",
                "print(\"User Data Info:\")\n",
                "df_user.info()\n",
                "print(\"\\nMissing Values:\")\n",
                "print(df_user.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore numerical features: review_count, fans, average_stars, votes, compliments\n",
                "print(\"\\nNumerical Features Description:\")\n",
                "cols_to_describe = ['review_count', 'useful', 'funny', 'cool', 'fans', 'average_stars', \n",
                "                    'compliment_hot', 'compliment_more', 'compliment_profile', 'compliment_cute', \n",
                "                    'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', \n",
                "                    'compliment_funny', 'compliment_writer', 'compliment_photos']\n",
                "display(df_user[cols_to_describe].describe())\n",
                "\n",
                "# Plot distributions for key numerical features (using log scale where needed)\n",
                "plt.figure(figsize=(15, 10))\n",
                "\n",
                "plt.subplot(2, 3, 1)\n",
                "sns.histplot(df_user['review_count'] + 1, bins=50, log_scale=True)\n",
                "plt.title('Distribution of User Review Count (Log Scale)')\n",
                "\n",
                "plt.subplot(2, 3, 2)\n",
                "sns.histplot(df_user['fans'] + 1, bins=50, log_scale=True)\n",
                "plt.title('Distribution of User Fans (Log Scale)')\n",
                "\n",
                "plt.subplot(2, 3, 3)\n",
                "sns.histplot(df_user['average_stars'], bins=20, kde=False)\n",
                "plt.title('Distribution of User Average Stars')\n",
                "\n",
                "# Sum of votes given by user\n",
                "df_user['total_votes'] = df_user['useful'] + df_user['funny'] + df_user['cool']\n",
                "plt.subplot(2, 3, 4)\n",
                "sns.histplot(df_user['total_votes'] + 1, bins=50, log_scale=True)\n",
                "plt.title('Distribution of Total Votes Given (Log Scale)')\n",
                "\n",
                "# Sum of compliments received by user\n",
                "compliment_cols = [col for col in df_user.columns if col.startswith('compliment_')]\n",
                "df_user['total_compliments'] = df_user[compliment_cols].sum(axis=1)\n",
                "plt.subplot(2, 3, 5)\n",
                "sns.histplot(df_user['total_compliments'] + 1, bins=50, log_scale=True)\n",
                "plt.title('Distribution of Total Compliments Received (Log Scale)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Yelping Since\n",
                "print(\"\\nExploring Yelping Since:\")\n",
                "df_user['yelping_since'] = pd.to_datetime(df_user['yelping_since'])\n",
                "df_user['join_year'] = df_user['yelping_since'].dt.year\n",
                "\n",
                "print(f\"Yelping since range: {df_user['yelping_since'].min()} to {df_user['yelping_since'].max()}\")\n",
                "\n",
                "# Plot number of users joined over years\n",
                "plt.figure(figsize=(12, 5))\n",
                "sns.countplot(x='join_year', data=df_user, palette='coolwarm')\n",
                "plt.title('Number of Users Joined per Year')\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Friends and Elite Status\n",
                "# Calculate number of friends (friends column is a list of user_ids)\n",
                "# The friends list can be very long and is stored as a string, needs careful parsing\n",
                "df_user['num_friends'] = df_user['friends'].apply(lambda x: len(x.split(', ')) if x != 'None' else 0)\n",
                "\n",
                "# Calculate number of elite years\n",
                "# The elite list is stored as a string\n",
                "df_user['num_elite_years'] = df_user['elite'].apply(lambda x: len(x.split(',')) if x != 'None' and x != '' else 0)\n",
                "\n",
                "print(\"\\nFriend Count and Elite Years Description:\")\n",
                "display(df_user[['num_friends', 'num_elite_years']].describe())\n",
                "\n",
                "# Plot distributions\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "sns.histplot(df_user['num_friends'] + 1, bins=50, log_scale=True)\n",
                "plt.title('Distribution of Number of Friends (Log Scale)')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "sns.histplot(df_user['num_elite_years'], bins=df_user['num_elite_years'].max() + 1, discrete=True)\n",
                "plt.title('Distribution of Number of Elite Years')\n",
                "plt.xticks(range(0, df_user['num_elite_years'].max() + 1))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Checkin Data (`df_checkin`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Re-check basic info and check for missing values\n",
                "print(\"Checkin Data Info:\")\n",
                "df_checkin.info()\n",
                "print(\"\\nMissing Values:\")\n",
                "print(df_checkin.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Checkin Counts\n",
                "# The 'date' column is a string of comma-separated timestamps\n",
                "# Calculate the number of checkins for each business\n",
                "df_checkin['num_checkins'] = df_checkin['date'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)\n",
                "\n",
                "print(\"\\nCheckin Count Description:\")\n",
                "display(df_checkin['num_checkins'].describe())\n",
                "\n",
                "# Plot distribution of checkin counts (log scale)\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.histplot(df_checkin['num_checkins'] + 1, bins=50, log_scale=True)\n",
                "plt.title('Distribution of Number of Checkins per Business (Log Scale)')\n",
                "plt.xlabel('Number of Checkins + 1')\n",
                "plt.show()\n",
                "\n",
                "# Example: Business with the most checkins\n",
                "print(\"\\nBusiness with most checkins:\")\n",
                "most_checkins_idx = df_checkin['num_checkins'].idxmax()\n",
                "display(df_checkin.loc[most_checkins_idx])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 Tip Data (`df_tip`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Re-check basic info and check for missing values\n",
                "print(\"Tip Data Info:\")\n",
                "df_tip.info()\n",
                "print(\"\\nMissing Values:\")\n",
                "print(df_tip.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore numerical features: compliment_count\n",
                "print(\"\\nCompliment Count Description:\")\n",
                "display(df_tip['compliment_count'].describe())\n",
                "\n",
                "# Plot distribution of compliment counts (log scale)\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.histplot(df_tip['compliment_count'] + 1, bins=50, log_scale=True)\n",
                "plt.title('Distribution of Tip Compliment Count (Log Scale)')\n",
                "plt.xlabel('Compliment Count + 1')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Tip Text Length\n",
                "df_tip['text_length'] = df_tip['text'].str.len()\n",
                "\n",
                "print(\"\\nTip Text Length Description:\")\n",
                "display(df_tip['text_length'].describe())\n",
                "\n",
                "# Plot distribution of text length\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.histplot(df_tip['text_length'], bins=50, kde=True)\n",
                "plt.title('Distribution of Tip Text Length')\n",
                "plt.xlabel('Text Length (Number of Characters)')\n",
                "# Tips are shorter, adjust xlim if needed based on describe()\n",
                "plt.xlim(0, df_tip['text_length'].quantile(0.99)) # Show up to 99th percentile\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore Tip Date\n",
                "print(\"\\nExploring Tip Dates:\")\n",
                "df_tip['date'] = pd.to_datetime(df_tip['date'])\n",
                "df_tip['year'] = df_tip['date'].dt.year\n",
                "\n",
                "print(f\"Date range: {df_tip['date'].min()} to {df_tip['date'].max()}\")\n",
                "\n",
                "# Plot number of tips over years\n",
                "plt.figure(figsize=(12, 5))\n",
                "sns.countplot(x='year', data=df_tip, palette='plasma')\n",
                "plt.title('Number of Tips per Year')\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Relationship Analysis\n",
                "\n",
                "Explore relationships between different dataframes, e.g., reviews and businesses."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Merging DataFrames\n",
                "\n",
                "Merge reviews with business and user data to facilitate combined analysis. We'll use the review subset `df_review` for this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge reviews with business data\n",
                "print(\"Merging reviews with business data...\")\n",
                "df_review_business = pd.merge(\n",
                "    df_review, \n",
                "    df_business[['business_id', 'name', 'city', 'state', 'categories', 'stars', 'review_count']], \n",
                "    on='business_id', \n",
                "    how='left',\n",
                "    suffixes=('_review', '_business') # Add suffixes to distinguish columns like 'stars'\n",
                ")\n",
                "\n",
                "print(\"Merge with business data complete.\")\n",
                "display(df_review_business.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge the result with user data\n",
                "print(\"\\nMerging with user data...\")\n",
                "df_merged = pd.merge(\n",
                "    df_review_business, \n",
                "    df_user[['user_id', 'name', 'average_stars', 'fans', 'review_count', 'yelping_since', 'num_friends', 'num_elite_years']],\n",
                "    on='user_id',\n",
                "    how='left',\n",
                "    suffixes=('', '_user') # Suffix for user columns like 'name', 'review_count'\n",
                ")\n",
                "\n",
                "print(\"Merge with user data complete.\")\n",
                "df_merged.info()\n",
                "display(df_merged.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Exploring Relationships\n",
                "\n",
                "Analyze correlations and patterns in the merged data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Relationship between business stars and review stars\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.boxplot(x='stars_business', y='stars_review', data=df_merged, palette='coolwarm')\n",
                "plt.title('Review Stars vs. Average Business Stars')\n",
                "plt.xlabel('Average Business Stars')\n",
                "plt.ylabel('Individual Review Stars')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Relationship between user's average stars and their review stars\n",
                "plt.figure(figsize=(8, 6))\n",
                "# Calculate the difference for better visualization if needed, or just plot directly\n",
                "# df_merged['star_diff'] = df_merged['stars_review'] - df_merged['average_stars']\n",
                "sns.boxplot(x='stars_review', y='average_stars', data=df_merged, palette='viridis')\n",
                "plt.title('User Average Stars vs. Individual Review Stars Given')\n",
                "plt.xlabel('Individual Review Stars Given')\n",
                "plt.ylabel('User Average Stars (Overall)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap for selected numerical features\n",
                "cols_for_corr = [\n",
                "    'stars_review', 'useful', 'funny', 'cool', 'text_length', \n",
                "    'stars_business', 'review_count_business', \n",
                "    'average_stars', 'fans', 'review_count_user', 'num_friends', 'num_elite_years'\n",
                "]\n",
                "\n",
                "# Ensure columns exist before calculating correlation\n",
                "existing_cols_for_corr = [col for col in cols_for_corr if col in df_merged.columns]\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "correlation_matrix = df_merged[existing_cols_for_corr].corr()\n",
                "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
                "plt.title('Correlation Matrix of Selected Features')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Text Analysis (Reviews & Tips)\n",
                "\n",
                "Perform basic text analysis like word frequency and word clouds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries for text analysis\n",
                "from wordcloud import WordCloud, STOPWORDS\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from collections import Counter\n",
                "import string\n",
                "\n",
                "# Download stopwords if not already downloaded\n",
                "try:\n",
                "    nltk.data.find('corpora/stopwords')\n",
                "except nltk.downloader.DownloadError:\n",
                "    nltk.download('stopwords')\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "# Add custom stopwords if needed\n",
                "# stop_words.update(['place', 'food', 'service', 'good', 'great', 'like', 'one', 'get', 'go', 'also'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Review Text Analysis\n",
                "\n",
                "Analyze the text content of the reviews (using the subset `df_review`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function for basic text cleaning\n",
                "def clean_text(text):\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    # Lowercase\n",
                "    text = text.lower()\n",
                "    # Remove punctuation\n",
                "    text = ''.join([char for char in text if char not in string.punctuation])\n",
                "    # Remove stopwords\n",
                "    words = text.split()\n",
                "    words = [word for word in words if word not in stop_words]\n",
                "    return ' '.join(words)\n",
                "\n",
                "# Apply cleaning (optional, can be memory intensive on large datasets)\n",
                "# For word cloud, we can process text directly\n",
                "# df_review['cleaned_text'] = df_review['text'].apply(clean_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Word Cloud for all reviews (using subset)\n",
                "print(\"Generating Word Cloud for all reviews (subset)...\")\n",
                "\n",
                "# Concatenate all review texts from the subset\n",
                "all_review_text = ' '.join(df_review['text'].astype(str).tolist())\n",
                "\n",
                "wordcloud_all = WordCloud(\n",
                "    width=800, \n",
                "    height=400, \n",
                "    background_color='white', \n",
                "    stopwords=stop_words, \n",
                "    max_words=150,\n",
                "    colormap='viridis'\n",
                ").generate(all_review_text)\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.imshow(wordcloud_all, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Word Cloud for All Reviews (Subset)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Word Clouds for positive (4-5 stars) and negative (1-2 stars) reviews\n",
                "positive_reviews = ' '.join(df_review[df_review['stars'] >= 4]['text'].astype(str).tolist())\n",
                "negative_reviews = ' '.join(df_review[df_review['stars'] <= 2]['text'].astype(str).tolist())\n",
                "\n",
                "wordcloud_positive = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words, max_words=100, colormap='Greens').generate(positive_reviews)\n",
                "wordcloud_negative = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words, max_words=100, colormap='Reds').generate(negative_reviews)\n",
                "\n",
                "plt.figure(figsize=(15, 8))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Word Cloud for Positive Reviews (4-5 Stars)')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Word Cloud for Negative Reviews (1-2 Stars)')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Tip Text Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Word Cloud for all tips\n",
                "print(\"Generating Word Cloud for all tips...\")\n",
                "\n",
                "# Concatenate all tip texts\n",
                "all_tip_text = ' '.join(df_tip['text'].astype(str).tolist())\n",
                "\n",
                "wordcloud_tip = WordCloud(\n",
                "    width=800, \n",
                "    height=400, \n",
                "    background_color='white', \n",
                "    stopwords=stop_words, \n",
                "    max_words=150,\n",
                "    colormap='plasma'\n",
                ").generate(all_tip_text)\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.imshow(wordcloud_tip, interpolation='bilinear')\n",
                "plt.axis('off')\n",
                "plt.title('Word Cloud for All Tips')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary & Next Steps\n",
                "\n",
                "Summarize key findings and outline potential next steps for data cleaning, feature engineering, and modeling."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Key Findings:\n",
                "*   **Data Overview:** The dataset contains rich information across businesses, user interactions (reviews, tips, check-ins), and user profiles. The review and user files are particularly large.\n",
                "*   **Businesses:** Skewed distribution for `review_count` (many businesses with few reviews, few with many). `stars` distribution shows common ratings around 3.5-4.5. A significant portion (~30% in the full dataset) are restaurants. Missing values exist, notably in `attributes` and `hours`.\n",
                "*   **Reviews:** Star ratings are heavily skewed towards positive (4-5 stars). `useful`, `funny`, `cool` votes are also skewed, with most reviews receiving few votes. Review text length varies significantly. Review volume increased over the years (based on the subset).\n",
                "*   **Users:** User activity (`review_count`, `fans`, votes, compliments) is highly skewed. `average_stars` tends to be high. User join dates span a long period.\n",
                "*   **Checkins:** Number of check-ins per business is highly skewed.\n",
                "*   **Tips:** Tips are much shorter than reviews. `compliment_count` is low for most tips.\n",
                "*   **Relationships:** Business stars and review stars show a positive correlation, as expected. User average stars correlate with the stars they give in individual reviews. Review votes (`useful`, `funny`, `cool`) show some positive correlation with each other and with review text length.\n",
                "*   **Text Analysis:** Word clouds reveal common terms associated with reviews and tips, with differences between positive and negative reviews (e.g., 'amazing', 'delicious' vs. 'disappointed', 'worst'). Common words like 'food', 'place', 'service', 'good', 'great' dominate if not excluded as stopwords.\n",
                "\n",
                "### Potential Next Steps (Noise Cleaning & Filtering for Graph Analysis):\n",
                "1.  **Focus on Restaurants:** Filter `df_business` to include only businesses categorized as 'Restaurants' or related food categories, as per the project goal.\n",
                "2.  **Handle Missing Values:** Decide on strategies for missing `attributes`, `hours`, etc. (imputation, removal, or feature engineering).\n",
                "3.  **Filter by Location:** Potentially focus on a specific city or region (e.g., a major city like Las Vegas or Phoenix, which are prominent in the dataset) to create a more manageable and potentially denser graph.\n",
                "4.  **Filter by Activity/Recency:** Consider filtering out inactive users (low `review_count`) or very old reviews/businesses if focusing on recent trends.\n",
                "5.  **Create Subset:** Based on the filters above, create a final subset of businesses, users, and reviews/tips.\n",
                "6.  **Feature Engineering:** \n",
                "    *   Parse `attributes` and `hours` for businesses.\n",
                "    *   Extract features from `date` columns (day of week, month, year).\n",
                "    *   Perform more advanced text analysis (sentiment scores, topic modeling) on reviews/tips.\n",
                "    *   Calculate user tenure (`current_date - yelping_since`).\n",
                "7.  **Prepare for Neo4j:** Structure the chosen subset into nodes (Users, Businesses, Reviews/Tips) and relationships (WROTE, REVIEWED, FRIENDS_WITH, CHECKED_IN, TIPPED_ON) for loading into Neo4j."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
