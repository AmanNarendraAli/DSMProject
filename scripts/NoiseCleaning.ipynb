{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Data Cleaning: Noise Reduction (Chunking Refactor)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 0: Setup and Initial Load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "SyntaxError",
                    "evalue": "invalid syntax (1124650115.py, line 14)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    Potential plotting libraries (optional)\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import datetime\n",
                "import re\n",
                "import os\n",
                "import random\n",
                "import gc # Garbage Collector\n",
                "from sklearn.ensemble import IsolationForest\n",
                "import nltk # Uncomment if using stopwords in Phase 2\n",
                "from nltk.corpus import stopwords # Uncomment if using stopwords\n",
                "from nltk.tokenize import word_tokenize # Uncomment if using stopwords\n",
                "\n",
                "# Potential plotting libraries (optional)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Settings\n",
                "pd.set_option('display.max_columns', None) # Show all columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define File Paths\n",
                "input_dir = '../yelp_dataset/' # Relative to script location\n",
                "review_file = os.path.join(input_dir, 'yelp_academic_dataset_review.json')\n",
                "user_file = os.path.join(input_dir, 'yelp_academic_dataset_user.json')\n",
                "business_file = os.path.join(input_dir, 'yelp_academic_dataset_business.json')\n",
                "# tip_file = os.path.join(input_dir, 'yelp_academic_dataset_tip.json') # Optional\n",
                "\n",
                "# Define Output Directory\n",
                "output_dir = os.path.join(input_dir, 'cleaned_data')\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "print(f\"Output directory created/exists: {output_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Static Data (Users and Business)\n",
                "print(\"Loading user data...\")\n",
                "users_df = pd.read_json(user_file, lines=True)\n",
                "print(\"Loading business data...\")\n",
                "business_df = pd.read_json(business_file, lines=True)\n",
                "print(\"Static data loading complete.\")\n",
                "\n",
                "print(\"--- Initial Static Data Shapes ---\")\n",
                "print(f\"Users: {users_df.shape}\")     # Expected: (1987897, 22)\n",
                "print(f\"Business: {business_df.shape}\") # Expected: (150346, 14)\n",
                "\n",
                "print(\"\\n--- User Columns & Head ---\")\n",
                "print(users_df.columns)\n",
                "print(users_df.head(2))\n",
                "\n",
                "print(\"\\n--- Business Columns & Head ---\")\n",
                "print(business_df.columns)\n",
                "print(business_df.head(2))\n",
                "\n",
                "print(\"\\n--- Missing Values (Static) ---\")\n",
                "print(\"\\nUsers:\\n\", users_df.isnull().sum())\n",
                "print(\"\\nBusiness:\\n\", business_df.isnull().sum()) # Address/postal code NaNs expected"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define helper function (moved from original Phase 1)\n",
                "def count_friends(friends_str):\n",
                "    if friends_str is None or friends_str == 'None' or pd.isna(friends_str):\n",
                "        return 0\n",
                "    return len(friends_str.split(','))\n",
                "\n",
                "# Define URL pattern (moved from original Phase 2)\n",
                "url_pattern = r'http[s]?://|www\\\\.|\\\\S+\\\\.(com|net|org|edu|gov)\\\\S*|\\\\S+@\\\\S+'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chunk Processing Loop (Reviews)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chunk_size = 500000\n",
                "processed_chunks = []\n",
                "total_reviews_processed = 0\n",
                "total_reviews_kept_chunk_phase = 0\n",
                "\n",
                "print(f\"Starting review processing in chunks of {chunk_size}...\")\n",
                "\n",
                "for i, review_chunk in enumerate(pd.read_json(review_file, lines=True, chunksize=chunk_size)):\n",
                "    print(f\"\\n--- Processing Chunk {i+1} --- ({len(review_chunk)} reviews)\")\n",
                "    total_reviews_processed += len(review_chunk)\n",
                "\n",
                "    # 1. Merge with Users\n",
                "    print(\"  Merging with users...\")\n",
                "    chunk_merged = pd.merge(review_chunk, users_df, on='user_id', how='inner', suffixes=('_review', '_user'))\n",
                "    print(f\"  Reviews after merge: {len(chunk_merged)} (Removed {len(review_chunk) - len(chunk_merged)} due to missing users)\")\n",
                "    del review_chunk # Free memory\n",
                "    gc.collect()\n",
                "\n",
                "    # 2. Feature Engineering (Chunk-level)\n",
                "    print(\"  Performing feature engineering...\")\n",
                "    chunk_merged['date'] = pd.to_datetime(chunk_merged['date'], errors='coerce')\n",
                "    chunk_merged['yelping_since'] = pd.to_datetime(chunk_merged['yelping_since'], errors='coerce')\n",
                "    # Drop rows where date conversion failed\n",
                "    initial_chunk_count = len(chunk_merged)\n",
                "    chunk_merged.dropna(subset=['date', 'yelping_since'], inplace=True)\n",
                "    print(f\"    Dropped {initial_chunk_count - len(chunk_merged)} rows due to NaT dates.\")\n",
                "\n",
                "    chunk_merged['text_length'] = chunk_merged['text'].astype(str).str.len()\n",
                "    compliment_cols = [col for col in chunk_merged.columns if col.startswith('compliment_')]\n",
                "    chunk_merged['user_compliments_total'] = chunk_merged[compliment_cols].sum(axis=1)\n",
                "    chunk_merged['user_friends_count'] = chunk_merged['friends'].apply(count_friends)\n",
                "    if 'elite' in chunk_merged.columns:\n",
                "        chunk_merged['user_is_elite'] = chunk_merged['elite'].apply(lambda x: isinstance(x, str) and x != '' and x is not None)\n",
                "    else:\n",
                "        chunk_merged['user_is_elite'] = False\n",
                "\n",
                "    # 3. Apply Review-Level Filters (A, F, 2A)\n",
                "    print(\"  Applying review-level filters...\")\n",
                "    count_before_filters = len(chunk_merged)\n",
                "\n",
                "    # Filter A: Text Length\n",
                "    MIN_REVIEW_LENGTH = 50\n",
                "    chunk_merged = chunk_merged[chunk_merged['text_length'] >= MIN_REVIEW_LENGTH]\n",
                "    print(f\"    Filter A (Text Length >= {MIN_REVIEW_LENGTH}): Kept {len(chunk_merged)} reviews\")\n",
                "\n",
                "    # Filter F: Low Usefulness (Old/Short/0-useful)\n",
                "    MAX_REVIEW_AGE_YEARS = 2\n",
                "    cutoff_date = pd.Timestamp.now(tz='UTC') - pd.DateOffset(years=MAX_REVIEW_AGE_YEARS)\n",
                "    if chunk_merged['date'].dt.tz is not None:\n",
                "        cutoff_date = cutoff_date.tz_convert(chunk_merged['date'].dt.tz)\n",
                "    else:\n",
                "        cutoff_date = cutoff_date.tz_localize(None)\n",
                "    condition_f = (\n",
                "        (chunk_merged['date'] < cutoff_date) & \n",
                "        (chunk_merged['useful_review'] == 0) & \n",
                "        (chunk_merged['text_length'] < 150)\n",
                "    )\n",
                "    chunk_merged = chunk_merged[~condition_f]\n",
                "    print(f\"    Filter F (Low Usefulness): Kept {len(chunk_merged)} reviews\")\n",
                "\n",
                "    # Filter 2A: URLs/Emails\n",
                "    contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n",
                "    chunk_merged = chunk_merged[~contains_url_mask]\n",
                "    print(f\"    Filter 2A (URLs/Emails): Kept {len(chunk_merged)} reviews\")\n",
                "\n",
                "    reviews_removed_in_chunk = count_before_filters - len(chunk_merged)\n",
                "    print(f\"  Total reviews removed in this chunk by review-level filters: {reviews_removed_in_chunk}\")\n",
                "    total_reviews_kept_chunk_phase += len(chunk_merged)\n",
                "\n",
                "    # 4. Append to list (select necessary columns to save memory)\n",
                "    # Keep all columns needed for Phase 1 filters + final output\n",
                "    cols_to_keep_for_phase1 = [\n",
                "        'review_id', 'user_id', 'business_id', 'stars_review', \n",
                "        'useful_review', 'funny_review', 'cool_review', 'text', 'date', \n",
                "        'text_length', 'user_compliments_total', 'user_friends_count', \n",
                "        'user_is_elite', 'review_count' # Original user review count needed for Filter E\n",
                "        # 'yelping_since' # Not strictly needed after date drop\n",
                "    ]\n",
                "    # Ensure all needed columns exist before selecting\n",
                "    existing_cols_to_keep = [col for col in cols_to_keep_for_phase1 if col in chunk_merged.columns]\n",
                "    processed_chunks.append(chunk_merged[existing_cols_to_keep])\n",
                "    print(f\"  Chunk {i+1} processing complete. Appended {len(chunk_merged)} reviews.\")\n",
                "\n",
                "    # 5. Memory Cleanup\n",
                "    del chunk_merged\n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\nFinished processing all chunks. Total reviews processed: {total_reviews_processed}\")\n",
                "print(f\"Total reviews kept after chunk-level processing: {total_reviews_kept_chunk_phase}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Combine Processed Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nCombining processed chunks...\")\n",
                "if processed_chunks:\n",
                "    reviews_combined_df = pd.concat(processed_chunks, ignore_index=True)\n",
                "    print(f\"Combined DataFrame shape: {reviews_combined_df.shape}\")\n",
                "    # Clear chunk list from memory\n",
                "    del processed_chunks\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"No chunks were processed or kept. Exiting.\")\n",
                "    # Handle exit or create an empty DataFrame if needed\n",
                "    reviews_combined_df = pd.DataFrame() # Or raise an error\n",
                "\n",
                "# Ensure date is datetime type after concat\n",
                "if not reviews_combined_df.empty:\n",
                "    reviews_combined_df['date'] = pd.to_datetime(reviews_combined_df['date'], errors='coerce')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 1: Rule-Based Filtering (User Aggregation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Baseline Evaluation Metrics (After Chunking/Combining, Before Stage 1 Filters)\n",
                "print(\"--- Baseline Metrics (After Chunking/Combining) ---\")\n",
                "if not reviews_combined_df.empty:\n",
                "    baseline_review_count = len(reviews_combined_df)\n",
                "    baseline_user_ids = reviews_combined_df['user_id'].unique()\n",
                "    baseline_user_count = len(baseline_user_ids)\n",
                "\n",
                "    print(f\"Combined Review Count: {baseline_review_count}\")\n",
                "    print(f\"Combined Unique User Count: {baseline_user_count}\")\n",
                "\n",
                "    print(\"Combined Star Rating Distribution (%):\")\n",
                "    print(reviews_combined_df['stars_review'].value_counts(normalize=True).sort_index() * 100)\n",
                "\n",
                "    print(f\"Combined Avg Text Length: {reviews_combined_df['text_length'].mean():.2f}\")\n",
                "    print(f\"Combined Avg Useful Votes per Review: {reviews_combined_df['useful_review'].mean():.2f}\")\n",
                "\n",
                "    # Calculate baseline average user metrics (need to aggregate first)\n",
                "    baseline_user_agg = reviews_combined_df.drop_duplicates(subset=['user_id']).agg(\n",
                "        avg_compliments=('user_compliments_total', 'mean'),\n",
                "        avg_friends=('user_friends_count', 'mean'),\n",
                "        pct_elite=('user_is_elite', 'mean')\n",
                "    )\n",
                "    print(\"\\nBaseline Avg User Metrics (Unique Users):\")\n",
                "    print(f\"  Avg Compliments: {baseline_user_agg['avg_compliments']:.2f}\")\n",
                "    print(f\"  Avg Friends Count: {baseline_user_agg['avg_friends']:.2f}\")\n",
                "    print(f\"  % Elite Users: {baseline_user_agg['pct_elite'] * 100:.2f}%\")\n",
                "\n",
                "    # Keep a copy for filtering stages\n",
                "    reviews_stage1_df = reviews_combined_df.copy()\n",
                "    del reviews_combined_df # Free memory\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Combined DataFrame is empty, skipping Phase 1.\")\n",
                "    reviews_stage1_df = pd.DataFrame() # Ensure it exists"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Apply User-Level Filters (Iteratively)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Filters B, C, D, E are applied here on the combined data\n",
                "if not reviews_stage1_df.empty:\n",
                "    # (B) Filter by Review Bursts\n",
                "    print(\"\\n--- Filter B: Review Bursts ---\")\n",
                "    count_before = len(reviews_stage1_df)\n",
                "    users_before = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # Calculate reviews per user per day\n",
                "    reviews_per_day = reviews_stage1_df.groupby([reviews_stage1_df['user_id'], reviews_stage1_df['date'].dt.date])\\\n",
                "                                         .size().reset_index(name='reviews_per_day')\n",
                "\n",
                "    # Find max reviews per day for each user\n",
                "    max_reviews_per_user = reviews_per_day.groupby('user_id')['reviews_per_day'].max().reset_index(name='max_reviews_per_day')\n",
                "\n",
                "    # Merge max back to the main df\n",
                "    reviews_stage1_df = pd.merge(reviews_stage1_df, max_reviews_per_user, on='user_id', how='left')\n",
                "\n",
                "    avg_max_burst_before = reviews_stage1_df['max_reviews_per_day'].mean()\n",
                "\n",
                "    # Apply filter\n",
                "    MAX_BURST_RATE = 10 # Reviews per day\n",
                "    users_to_remove_burst = reviews_stage1_df[reviews_stage1_df['max_reviews_per_day'] > MAX_BURST_RATE]['user_id'].unique()\n",
                "    reviews_stage1_df = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove_burst)]\n",
                "\n",
                "    count_after = len(reviews_stage1_df)\n",
                "    users_after = reviews_stage1_df['user_id'].nunique()\n",
                "    # Recalculate mean only if df is not empty\n",
                "    avg_max_burst_after = reviews_stage1_df['max_reviews_per_day'].mean() if not reviews_stage1_df.empty else 0\n",
                "\n",
                "    print(f\"Users identified with burst rate > {MAX_BURST_RATE}: {len(users_to_remove_burst)}\")\n",
                "    print(f\"Reviews removed: {count_before - count_after}\")\n",
                "    print(f\"Users removed: {users_before - users_after}\")\n",
                "    print(f\"Avg max reviews/day before: {avg_max_burst_before:.2f}, after: {avg_max_burst_after:.2f}\")\n",
                "\n",
                "    # Drop the temporary column\n",
                "    reviews_stage1_df.drop(columns=['max_reviews_per_day'], inplace=True)\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Skipping Filter B: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "if not reviews_stage1_df.empty:\n",
                "    # (C) Filter by Activity Window\n",
                "    print(\"\\n--- Filter C: Activity Window ---\")\n",
                "    count_before = len(reviews_stage1_df)\n",
                "    users_before = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # Group by user to find min/max date and review count\n",
                "    user_activity = reviews_stage1_df.groupby('user_id').agg(\n",
                "        min_date=('date', 'min'),\n",
                "        max_date=('date', 'max'),\n",
                "        user_review_count_stage1=('review_id', 'count') # Count reviews remaining at this stage\n",
                "    ).reset_index()\n",
                "\n",
                "    # Calculate activity window\n",
                "    user_activity['activity_window_days'] = (user_activity['max_date'] - user_activity['min_date']).dt.days\n",
                "\n",
                "    # Merge back\n",
                "    reviews_stage1_df = pd.merge(reviews_stage1_df, user_activity[['user_id', 'activity_window_days', 'user_review_count_stage1']], on='user_id', how='left')\n",
                "\n",
                "    median_window_before = reviews_stage1_df['activity_window_days'].median()\n",
                "\n",
                "    # Apply filter\n",
                "    MIN_ACTIVITY_WINDOW_DAYS = 60\n",
                "    MIN_REVIEWS_FOR_WINDOW_FILTER = 3\n",
                "\n",
                "    condition = (\n",
                "        (reviews_stage1_df['activity_window_days'] < MIN_ACTIVITY_WINDOW_DAYS) & \n",
                "        (reviews_stage1_df['user_review_count_stage1'] >= MIN_REVIEWS_FOR_WINDOW_FILTER)\n",
                "    )\n",
                "    users_to_remove_window = reviews_stage1_df[condition]['user_id'].unique()\n",
                "    reviews_stage1_df = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove_window)]\n",
                "\n",
                "    count_after = len(reviews_stage1_df)\n",
                "    users_after = reviews_stage1_df['user_id'].nunique()\n",
                "    median_window_after = reviews_stage1_df['activity_window_days'].median() if not reviews_stage1_df.empty else 0\n",
                "\n",
                "    print(f\"Users identified with window < {MIN_ACTIVITY_WINDOW_DAYS} days & >= {MIN_REVIEWS_FOR_WINDOW_FILTER} reviews: {len(users_to_remove_window)}\")\n",
                "    print(f\"Reviews removed: {count_before - count_after}\")\n",
                "    print(f\"Users removed: {users_before - users_after}\")\n",
                "    print(f\"Median activity window (days) before: {median_window_before:.0f}, after: {median_window_after:.0f}\")\n",
                "\n",
                "    # Drop temporary columns\n",
                "    reviews_stage1_df.drop(columns=['activity_window_days', 'user_review_count_stage1'], inplace=True)\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Skipping Filter C: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "if not reviews_stage1_df.empty:\n",
                "    # (D) Filter by Rating Deviation\n",
                "    print(\"\\n--- Filter D: Rating Deviation ---\")\n",
                "    count_before = len(reviews_stage1_df)\n",
                "    users_before = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # Calculate average star rating per business (using current filtered reviews)\n",
                "    business_avg_stars = reviews_stage1_df.groupby('business_id')['stars_review'].mean().reset_index(name='business_avg_rating')\n",
                "\n",
                "    # Merge business average rating\n",
                "    reviews_stage1_df = pd.merge(reviews_stage1_df, business_avg_stars, on='business_id', how='left')\n",
                "\n",
                "    # Calculate deviation for each review\n",
                "    reviews_stage1_df['rating_deviation'] = (reviews_stage1_df['stars_review'] - reviews_stage1_df['business_avg_rating']).abs()\n",
                "\n",
                "    # Calculate average deviation per user\n",
                "    user_avg_deviation = reviews_stage1_df.groupby('user_id')['rating_deviation'].mean().reset_index(name='user_avg_deviation')\n",
                "\n",
                "    # Merge user average deviation\n",
                "    reviews_stage1_df = pd.merge(reviews_stage1_df, user_avg_deviation, on='user_id', how='left')\n",
                "\n",
                "    avg_dev_before = reviews_stage1_df['user_avg_deviation'].mean()\n",
                "\n",
                "    # Apply filter (excluding Elite users)\n",
                "    MAX_AVG_DEVIATION = 1.8\n",
                "    condition = (\n",
                "        (reviews_stage1_df['user_avg_deviation'] > MAX_AVG_DEVIATION) & \n",
                "        (reviews_stage1_df['user_is_elite'] == False)\n",
                "    )\n",
                "    users_to_remove_deviation = reviews_stage1_df[condition]['user_id'].unique()\n",
                "    reviews_stage1_df = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove_deviation)]\n",
                "\n",
                "    count_after = len(reviews_stage1_df)\n",
                "    users_after = reviews_stage1_df['user_id'].nunique()\n",
                "    avg_dev_after = reviews_stage1_df['user_avg_deviation'].mean() if not reviews_stage1_df.empty else 0\n",
                "\n",
                "    print(f\"Users identified with avg deviation > {MAX_AVG_DEVIATION} (non-elite): {len(users_to_remove_deviation)}\")\n",
                "    print(f\"Reviews removed: {count_before - count_after}\")\n",
                "    print(f\"Users removed: {users_before - users_after}\")\n",
                "    print(f\"Avg user rating deviation before: {avg_dev_before:.2f}, after: {avg_dev_after:.2f}\")\n",
                "\n",
                "    # Drop temporary columns\n",
                "    reviews_stage1_df.drop(columns=['business_avg_rating', 'rating_deviation', 'user_avg_deviation'], inplace=True)\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Skipping Filter D: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "if not reviews_stage1_df.empty:\n",
                "    # (E) Filter by Low Engagement (Combined Rule)\n",
                "    print(\"\\n--- Filter E: Low Engagement ---\")\n",
                "    count_before = len(reviews_stage1_df)\n",
                "    users_before = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # We kept 'review_count' (original user review count) from the chunk processing merge\n",
                "    # If it wasn't kept, merge it back from users_df here:\n",
                "    # if 'review_count' not in reviews_stage1_df.columns:\n",
                "    #     reviews_stage1_df = pd.merge(reviews_stage1_df, users_df[['user_id', 'review_count']], on='user_id', how='left', suffixes=('', '_original'))\n",
                "\n",
                "    condition = (\n",
                "        (reviews_stage1_df['user_compliments_total'] == 0) & \n",
                "        (reviews_stage1_df['user_friends_count'] == 0) & \n",
                "        (reviews_stage1_df['user_is_elite'] == False) & \n",
                "        (reviews_stage1_df['review_count'] <= 5) # Using original review count\n",
                "    )\n",
                "    users_to_remove_low_engagement = reviews_stage1_df[condition]['user_id'].unique()\n",
                "\n",
                "    # Evaluate stats for removed vs kept (before removing)\n",
                "    removed_stats = reviews_stage1_df[reviews_stage1_df['user_id'].isin(users_to_remove_low_engagement)].agg(\n",
                "        avg_compliments=('user_compliments_total', 'mean'),\n",
                "        avg_friends=('user_friends_count', 'mean')\n",
                "    )\n",
                "    kept_stats = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove_low_engagement)].agg(\n",
                "        avg_compliments=('user_compliments_total', 'mean'),\n",
                "        avg_friends=('user_friends_count', 'mean')\n",
                "    )\n",
                "\n",
                "    reviews_stage1_df = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove_low_engagement)]\n",
                "\n",
                "    count_after = len(reviews_stage1_df)\n",
                "    users_after = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    print(f\"Users identified as low engagement: {len(users_to_remove_low_engagement)}\")\n",
                "    # Handle potential NaN if removed_stats is empty\n",
                "    print(f\"  Removed Avg Compliments: {removed_stats['avg_compliments']:.2f}, Avg Friends: {removed_stats['avg_friends']:.2f}\" if not removed_stats.empty else \"  Removed Stats: N/A (no users removed)\")\n",
                "    print(f\"  Kept Avg Compliments: {kept_stats['avg_compliments']:.2f}, Avg Friends: {kept_stats['avg_friends']:.2f}\" if not kept_stats.empty else \"  Kept Stats: N/A (no users kept)\")\n",
                "    print(f\"Reviews removed: {count_before - count_after}\")\n",
                "    print(f\"Users removed: {users_before - users_after}\")\n",
                "\n",
                "    # Drop temporary column if it was merged in this step\n",
                "    # if 'review_count_original' in reviews_stage1_df.columns:\n",
                "    #     reviews_stage1_df.drop(columns=['review_count_original'], inplace=True)\n",
                "    # We keep 'review_count' as it's needed for Phase 3\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Skipping Filter E: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stage 1 Evaluation & Saving"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate final metrics after all Stage 1 filters\n",
                "print(\"\\n--- Stage 1 Final Metrics ---\")\n",
                "if not reviews_stage1_df.empty:\n",
                "    final_review_count_s1 = len(reviews_stage1_df)\n",
                "    final_user_ids_s1 = reviews_stage1_df['user_id'].unique()\n",
                "    final_user_count_s1 = len(final_user_ids_s1)\n",
                "\n",
                "    print(f\"Final Review Count (Stage 1): {final_review_count_s1}\")\n",
                "    print(f\"Final Unique User Count (Stage 1): {final_user_count_s1}\")\n",
                "\n",
                "    print(\"Final Star Rating Distribution (% - Stage 1):\")\n",
                "    print(reviews_stage1_df['stars_review'].value_counts(normalize=True).sort_index() * 100)\n",
                "\n",
                "    print(f\"Final Avg Text Length (Stage 1): {reviews_stage1_df['text_length'].mean():.2f}\")\n",
                "    print(f\"Final Avg Useful Votes per Review (Stage 1): {reviews_stage1_df['useful_review'].mean():.2f}\")\n",
                "\n",
                "    # Calculate final average user metrics (Stage 1)\n",
                "    final_user_agg_s1 = reviews_stage1_df.drop_duplicates(subset=['user_id']).agg(\n",
                "        avg_compliments=('user_compliments_total', 'mean'),\n",
                "        avg_friends=('user_friends_count', 'mean'),\n",
                "        pct_elite=('user_is_elite', 'mean')\n",
                "    )\n",
                "    print(\"\\nFinal Avg User Metrics (Unique Users - Stage 1):\")\n",
                "    print(f\"  Avg Compliments: {final_user_agg_s1['avg_compliments']:.2f}\")\n",
                "    print(f\"  Avg Friends Count: {final_user_agg_s1['avg_friends']:.2f}\")\n",
                "    print(f\"  % Elite Users: {final_user_agg_s1['pct_elite'] * 100:.2f}%\")\n",
                "\n",
                "    print(\"\\n--- Comparison: Baseline (Post-Chunking) vs Stage 1 ---\")\n",
                "    print(f\"Reviews: {baseline_review_count} -> {final_review_count_s1} ({final_review_count_s1 / baseline_review_count * 100:.1f}%) - Removed: {baseline_review_count - final_review_count_s1}\")\n",
                "    print(f\"Users: {baseline_user_count} -> {final_user_count_s1} ({final_user_count_s1 / baseline_user_count * 100:.1f}%) - Removed: {baseline_user_count - final_user_count_s1}\")\n",
                "    # Note: Baseline metrics are from the combined df before Stage 1 filters\n",
                "    print(f\"Avg Text Length: {baseline_user_agg.get('avg_text_length', 'N/A')} -> {reviews_stage1_df['text_length'].mean():.2f}\") # Baseline avg length not calculated earlier\n",
                "    print(f\"Avg Useful Votes: {baseline_user_agg.get('avg_useful_votes', 'N/A')} -> {reviews_stage1_df['useful_review'].mean():.2f}\") # Baseline avg useful not calculated earlier\n",
                "    print(f\"Avg User Compliments: {baseline_user_agg['avg_compliments']:.2f} -> {final_user_agg_s1['avg_compliments']:.2f}\")\n",
                "    print(f\"Avg User Friends: {baseline_user_agg['avg_friends']:.2f} -> {final_user_agg_s1['avg_friends']:.2f}\")\n",
                "    print(f\"% Elite Users: {baseline_user_agg['pct_elite'] * 100:.2f}% -> {final_user_agg_s1['pct_elite'] * 100:.2f}%\")\n",
                "else:\n",
                "    print(\"Stage 1 DataFrame is empty. No metrics to calculate.\")\n",
                "    final_review_count_s1 = 0\n",
                "    final_user_count_s1 = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Qualitative Check (Stage 1)\n",
                "print(\"\\n--- Qualitative Check (Stage 1) ---\")\n",
                "if not reviews_stage1_df.empty:\n",
                "    kept_reviews_s1 = reviews_stage1_df\n",
                "    # To find removed reviews, we need to compare against the state *before* Stage 1 filters\n",
                "    # This requires reloading the combined data or keeping a copy, which might use too much memory.\n",
                "    # Simplified check: Show samples of kept reviews.\n",
                "    print(f\"Total kept after Stage 1: {len(kept_reviews_s1)}\")\n",
                "\n",
                "    if len(kept_reviews_s1) >= 10:\n",
                "        print(\"\\nSample Kept Reviews (Stage 1):\")\n",
                "        print(kept_reviews_s1.sample(min(10, len(kept_reviews_s1)))[['review_id', 'user_id', 'stars_review', 'text']])\n",
                "    else:\n",
                "        print(\"\\nNot enough kept reviews to sample.\")\n",
                "    # Note: Comparing removed reviews is harder with chunking without reloading.\n",
                "else:\n",
                "    print(\"Stage 1 DataFrame is empty. No qualitative check possible.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Cleaned Data (Stage 1)\n",
                "print(\"\\n--- Saving Stage 1 Data ---\")\n",
                "if not reviews_stage1_df.empty:\n",
                "    # Select relevant columns for reviews output\n",
                "    review_cols_to_keep_output = ['review_id', 'user_id', 'business_id', 'stars_review', 'useful_review', 'funny_review', 'cool_review', 'text', 'date']\n",
                "    reviews_stage1_cleaned_df = reviews_stage1_df[review_cols_to_keep_output]\n",
                "\n",
                "    output_reviews_s1_path = os.path.join(output_dir, 'reviews_stage1_cleaned.json')\n",
                "    reviews_stage1_cleaned_df.to_json(output_reviews_s1_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved Stage 1 cleaned reviews ({len(reviews_stage1_cleaned_df)}) to: {output_reviews_s1_path}\")\n",
                "\n",
                "    # Filter original users_df to keep only those remaining\n",
                "    remaining_user_ids_s1 = reviews_stage1_cleaned_df['user_id'].unique()\n",
                "    users_stage1_cleaned_df = users_df[users_df['user_id'].isin(remaining_user_ids_s1)].copy() # Use copy to avoid SettingWithCopyWarning\n",
                "\n",
                "    output_users_s1_path = os.path.join(output_dir, 'users_stage1_cleaned.json')\n",
                "    users_stage1_cleaned_df.to_json(output_users_s1_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved Stage 1 cleaned users ({len(users_stage1_cleaned_df)}) to: {output_users_s1_path}\")\n",
                "\n",
                "    # Prepare for Stage 2 - Keep the filtered reviews DataFrame\n",
                "    reviews_stage2_df = reviews_stage1_df.copy()\n",
                "    del reviews_stage1_df # Free memory from the copy used for saving\n",
                "    del reviews_stage1_cleaned_df\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Stage 1 DataFrame is empty. Cannot save.\")\n",
                "    reviews_stage2_df = pd.DataFrame() # Ensure it exists for next phase check\n",
                "    users_stage1_cleaned_df = pd.DataFrame() # Ensure it exists"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2: Basic Text & Content Heuristics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Apply Text Filters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter 2A (URLs/Emails) was moved to the chunk processing loop.\n",
                "print(\"Filter 2A (URLs/Emails) was applied during chunk processing.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# (B) Filter by Repetition (Optional - requires NLTK)\n",
                "print(\"\\n--- Filter 2B: Text Repetition (Unique Ratio) ---\")\n",
                "\n",
                "SKIP_REPETITION_FILTER = False # Set to True to skip this filter\n",
                "\n",
                "if SKIP_REPETITION_FILTER:\n",
                "    print(\"Skipping Filter 2B (Repetition) as configured.\")\n",
                "elif not reviews_stage2_df.empty:\n",
                "    try:\n",
                "        import nltk\n",
                "        from nltk.corpus import stopwords\n",
                "        from nltk.tokenize import word_tokenize\n",
                "\n",
                "        try:\n",
                "            nltk.data.find('corpora/stopwords')\n",
                "        except nltk.downloader.DownloadError:\n",
                "            print(\"Downloading NLTK stopwords...\")\n",
                "            nltk.download('stopwords')\n",
                "        try:\n",
                "            nltk.data.find('tokenizers/punkt')\n",
                "        except nltk.downloader.DownloadError:\n",
                "            print(\"Downloading NLTK punkt tokenizer...\")\n",
                "            nltk.download('punkt')\n",
                "\n",
                "        stop_words = set(stopwords.words('english'))\n",
                "\n",
                "        def calculate_unique_ratio(text):\n",
                "            if pd.isna(text) or not isinstance(text, str) or len(text.strip()) == 0:\n",
                "                return 1.0 # Assign high ratio if no text\n",
                "            # Tokenize, lowercase, remove punctuation (simple regex), remove stopwords\n",
                "            text = text.lower()\n",
                "            text = re.sub(r'[^\\w\\s]', '', text) # Keep words and spaces\n",
                "            tokens = word_tokenize(text)\n",
                "            words = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
                "            if not words: # Handle case of no words after filtering\n",
                "                return 1.0\n",
                "            unique_words = set(words)\n",
                "            ratio = len(unique_words) / len(words)\n",
                "            return ratio\n",
                "\n",
                "        print(\"Calculating unique ratio (may take time)...\")\n",
                "        reviews_stage2_df['unique_ratio'] = reviews_stage2_df['text'].apply(calculate_unique_ratio)\n",
                "\n",
                "        avg_ratio_before = reviews_stage2_df['unique_ratio'].mean()\n",
                "\n",
                "        MIN_UNIQUE_RATIO = 0.4\n",
                "        count_before = len(reviews_stage2_df)\n",
                "        users_before = reviews_stage2_df['user_id'].nunique()\n",
                "\n",
                "        reviews_stage2_df = reviews_stage2_df[reviews_stage2_df['unique_ratio'] >= MIN_UNIQUE_RATIO]\n",
                "\n",
                "        count_after = len(reviews_stage2_df)\n",
                "        users_after = reviews_stage2_df['user_id'].nunique()\n",
                "        avg_ratio_after = reviews_stage2_df['unique_ratio'].mean() if not reviews_stage2_df.empty else 0\n",
                "\n",
                "        print(f\"Reviews removed with unique ratio < {MIN_UNIQUE_RATIO}: {count_before - count_after}\")\n",
                "        print(f\"Users removed: {users_before - users_after}\")\n",
                "        print(f\"Avg unique ratio before: {avg_ratio_before:.3f}, after: {avg_ratio_after:.3f}\")\n",
                "\n",
                "        # Drop temporary column\n",
                "        reviews_stage2_df.drop(columns=['unique_ratio'], inplace=True)\n",
                "        gc.collect()\n",
                "\n",
                "    except ImportError:\n",
                "        print(\"Skipping Filter 2B (Repetition) because NLTK is not installed.\")\n",
                "        print(\"Please install NLTK (`pip install nltk`) and run the cell again if needed.\")\n",
                "else:\n",
                "     print(\"Skipping Filter 2B: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stage 2 Evaluation & Saving"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate final metrics after Stage 2 filters\n",
                "print(\"\\n--- Stage 2 Final Metrics ---\")\n",
                "if not reviews_stage2_df.empty:\n",
                "    final_review_count_s2 = len(reviews_stage2_df)\n",
                "    final_user_ids_s2 = reviews_stage2_df['user_id'].unique()\n",
                "    final_user_count_s2 = len(final_user_ids_s2)\n",
                "\n",
                "    print(f\"Final Review Count (Stage 2): {final_review_count_s2}\")\n",
                "    print(f\"Final Unique User Count (Stage 2): {final_user_count_s2}\")\n",
                "\n",
                "    print(\"\\n--- Comparison: Stage 1 vs Stage 2 ---\")\n",
                "    print(f\"Reviews: {final_review_count_s1} -> {final_review_count_s2} - Removed in Stage 2: {final_review_count_s1 - final_review_count_s2}\")\n",
                "    print(f\"Users: {final_user_count_s1} -> {final_user_count_s2} - Removed in Stage 2: {final_user_count_s1 - final_user_count_s2}\")\n",
                "else:\n",
                "    print(\"Stage 2 DataFrame is empty. No metrics to calculate.\")\n",
                "    final_review_count_s2 = 0\n",
                "    final_user_count_s2 = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Qualitative Check (Stage 2)\n",
                "print(\"\\n--- Qualitative Check (Stage 2) ---\")\n",
                "if not reviews_stage2_df.empty:\n",
                "    kept_reviews_s2 = reviews_stage2_df\n",
                "    # To find removed reviews, compare Stage 1 saved file vs current df\n",
                "    try:\n",
                "        temp_reviews_s1_df = pd.read_json(os.path.join(output_dir, 'reviews_stage1_cleaned.json'), lines=True)\n",
                "        removed_review_ids_s2 = temp_reviews_s1_df[~temp_reviews_s1_df['review_id'].isin(kept_reviews_s2['review_id'])]['review_id']\n",
                "        removed_reviews_s2 = temp_reviews_s1_df[temp_reviews_s1_df['review_id'].isin(removed_review_ids_s2)]\n",
                "        print(f\"Total kept: {len(kept_reviews_s2)}, Total removed in S2: {len(removed_reviews_s2)}\")\n",
                "        if len(removed_reviews_s2) >= 10:\n",
                "            print(\"\\nSample Removed Reviews (by Stage 2 Filters):\")\n",
                "            print(removed_reviews_s2.sample(min(10, len(removed_reviews_s2)))[['review_id', 'user_id', 'stars_review', 'text']])\n",
                "        else:\n",
                "            print(\"\\nNot enough reviews removed in Stage 2 to sample.\")\n",
                "        del temp_reviews_s1_df, removed_reviews_s2 # cleanup\n",
                "        gc.collect()\n",
                "    except FileNotFoundError:\n",
                "        print(\"Could not perform removed qualitative check: Stage 1 file not found.\")\n",
                "    except Exception as e:\n",
                "         print(f\"Error during Stage 2 qualitative check: {e}\")\n",
                "else:\n",
                "    print(\"Stage 2 DataFrame is empty. No qualitative check possible.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Cleaned Data (Stage 2)\n",
                "print(\"\\n--- Saving Stage 2 Data ---\")\n",
                "if not reviews_stage2_df.empty:\n",
                "    # Select columns for output\n",
                "    reviews_stage2_cleaned_df = reviews_stage2_df[review_cols_to_keep_output]\n",
                "\n",
                "    output_reviews_s2_path = os.path.join(output_dir, 'reviews_stage2_cleaned.json')\n",
                "    reviews_stage2_cleaned_df.to_json(output_reviews_s2_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved Stage 2 cleaned reviews ({len(reviews_stage2_cleaned_df)}) to: {output_reviews_s2_path}\")\n",
                "\n",
                "    # Filter Stage 1 users based on remaining user_ids in Stage 2 reviews\n",
                "    remaining_user_ids_s2 = reviews_stage2_cleaned_df['user_id'].unique()\n",
                "    # Use the users_stage1_cleaned_df saved previously\n",
                "    if not users_stage1_cleaned_df.empty:\n",
                "        users_stage2_cleaned_df = users_stage1_cleaned_df[users_stage1_cleaned_df['user_id'].isin(remaining_user_ids_s2)].copy()\n",
                "        output_users_s2_path = os.path.join(output_dir, 'users_stage2_cleaned.json')\n",
                "        users_stage2_cleaned_df.to_json(output_users_s2_path, orient='records', lines=True, date_format='iso')\n",
                "        print(f\"Saved Stage 2 cleaned users ({len(users_stage2_cleaned_df)}) to: {output_users_s2_path}\")\n",
                "    else:\n",
                "        print(\"Warning: Stage 1 users DataFrame was empty, cannot save Stage 2 users.\")\n",
                "        users_stage2_cleaned_df = pd.DataFrame() # Ensure exists\n",
                "\n",
                "    # Clear Stage 2 DataFrames from memory\n",
                "    del reviews_stage2_df\n",
                "    del reviews_stage2_cleaned_df\n",
                "    del users_stage1_cleaned_df # No longer needed\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Stage 2 DataFrame is empty. Cannot save.\")\n",
                "    users_stage2_cleaned_df = pd.DataFrame() # Ensure exists for next phase"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 3: Advanced Filtering (Isolation Forest)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Stage 2 Data\n",
                "print(\"--- Loading Stage 2 Data for Phase 3 ---\")\n",
                "reviews_stage3_input_df = pd.DataFrame()\n",
                "users_stage3_input_df = pd.DataFrame()\n",
                "try:\n",
                "    reviews_stage3_input_path = os.path.join(output_dir, 'reviews_stage2_cleaned.json')\n",
                "    reviews_stage3_input_df = pd.read_json(reviews_stage3_input_path, lines=True)\n",
                "    reviews_stage3_input_df['date'] = pd.to_datetime(reviews_stage3_input_df['date'], errors='coerce') # Ensure date is datetime\n",
                "    print(f\"Loaded Stage 2 reviews: {reviews_stage3_input_df.shape}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: Stage 2 review file not found at {reviews_stage3_input_path}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading Stage 2 reviews: {e}\")\n",
                "\n",
                "try:\n",
                "    users_stage3_input_path = os.path.join(output_dir, 'users_stage2_cleaned.json')\n",
                "    users_stage3_input_df = pd.read_json(users_stage3_input_path, lines=True)\n",
                "    print(f\"Loaded Stage 2 users: {users_stage3_input_df.shape}\")\n",
                "except FileNotFoundError:\n",
                "    print(f\"Error: Stage 2 user file not found at {users_stage3_input_path}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading Stage 2 users: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Features for Isolation Forest\n",
                "print(\"\\nPreparing features for Isolation Forest...\")\n",
                "features_df = pd.DataFrame()\n",
                "reviews_stage3_merged_df = pd.DataFrame()\n",
                "\n",
                "if not reviews_stage3_input_df.empty and not users_stage3_input_df.empty:\n",
                "    # Merge user data back into reviews\n",
                "    reviews_stage3_merged_df = pd.merge(reviews_stage3_input_df, users_stage3_input_df, on='user_id', how='inner', suffixes=('_review', '_user'))\n",
                "    print(f\"Merged Stage 2 reviews and users: {reviews_stage3_merged_df.shape}\")\n",
                "\n",
                "    # Recalculate user_avg_deviation based on current (Stage 2) reviews\n",
                "    print(\"  Recalculating business/user average ratings...\")\n",
                "    # 1. Calculate business avg stars from Stage 2 reviews\n",
                "    business_avg_stars_s2 = reviews_stage3_merged_df.groupby('business_id')['stars_review'].mean().reset_index(name='business_avg_rating_s2')\n",
                "    # 2. Merge into the merged df\n",
                "    reviews_stage3_merged_df = pd.merge(reviews_stage3_merged_df, business_avg_stars_s2, on='business_id', how='left')\n",
                "    # 3. Calculate review deviation\n",
                "    reviews_stage3_merged_df['rating_deviation_s2'] = (reviews_stage3_merged_df['stars_review'] - reviews_stage3_merged_df['business_avg_rating_s2']).abs()\n",
                "    # 4. Calculate user average deviation\n",
                "    user_avg_deviation_s2 = reviews_stage3_merged_df.groupby('user_id')['rating_deviation_s2'].mean().reset_index(name='user_avg_deviation_s2')\n",
                "    # 5. Merge user avg deviation back\n",
                "    reviews_stage3_merged_df = pd.merge(reviews_stage3_merged_df, user_avg_deviation_s2, on='user_id', how='left')\n",
                "\n",
                "    # Add other features from user data\n",
                "    print(\"  Adding other features...\")\n",
                "    reviews_stage3_merged_df['text_length'] = reviews_stage3_merged_df['text'].astype(str).str.len()\n",
                "    compliment_cols_s3 = [col for col in reviews_stage3_merged_df.columns if col.startswith('compliment_') and col != 'compliment_count'] # Exclude tip compliment if present\n",
                "    reviews_stage3_merged_df['user_compliments_total'] = reviews_stage3_merged_df[compliment_cols_s3].sum(axis=1)\n",
                "    reviews_stage3_merged_df['user_friends_count'] = reviews_stage3_merged_df['friends'].apply(count_friends) # Use function defined earlier\n",
                "\n",
                "    # Select features for the model\n",
                "    feature_cols = [\n",
                "        'text_length',\n",
                "        'useful_review',\n",
                "        'stars_review',\n",
                "        'review_count', # User's total review count (from users_df)\n",
                "        'user_compliments_total',\n",
                "        'user_friends_count',\n",
                "        'user_avg_deviation_s2',\n",
                "        'average_stars' # User's average rating (from users_df)\n",
                "    ]\n",
                "\n",
                "    # Ensure all feature columns exist\n",
                "    missing_cols = [col for col in feature_cols if col not in reviews_stage3_merged_df.columns]\n",
                "    if missing_cols:\n",
                "        print(f\"Warning: Missing required feature columns: {missing_cols}. Isolation Forest cannot run.\")\n",
                "    else:\n",
                "        features_df = reviews_stage3_merged_df[feature_cols].copy()\n",
                "\n",
                "        # Handle NaNs (e.g., fill with median or mean)\n",
                "        print(\"  Handling NaNs in features...\")\n",
                "        for col in feature_cols:\n",
                "            if features_df[col].isnull().any():\n",
                "                median_val = features_df[col].median()\n",
                "                features_df[col].fillna(median_val, inplace=True)\n",
                "                print(f\"    Filled NaNs in '{col}' with median ({median_val:.2f})\")\n",
                "\n",
                "        print(\"Feature preparation complete.\")\n",
                "else:\n",
                "    print(\"Skipping feature preparation: Input DataFrames from Stage 2 are empty or failed to load.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train & Predict with Isolation Forest\n",
                "reviews_stage3_filtered_df = pd.DataFrame() # Initialize\n",
                "\n",
                "if not features_df.empty and not reviews_stage3_merged_df.empty:\n",
                "    print(\"\\nTraining Isolation Forest and predicting anomalies...\")\n",
                "    try:\n",
                "        # Initialize model\n",
                "        # contamination='auto' lets the algorithm estimate the proportion of outliers\n",
                "        # Or set explicitly, e.g., contamination=0.01 for 1%\n",
                "        iso_forest = IsolationForest(contamination='auto', random_state=42, n_jobs=-1) # Use all available cores\n",
                "\n",
                "        # Fit and predict\n",
                "        # fit_predict returns 1 for inliers, -1 for outliers\n",
                "        reviews_stage3_merged_df['anomaly_score'] = iso_forest.fit_predict(features_df)\n",
                "\n",
                "        n_outliers = (reviews_stage3_merged_df['anomaly_score'] == -1).sum()\n",
                "        print(f\"Identified {n_outliers} potential outliers (anomaly_score = -1).\")\n",
                "\n",
                "        # Filter out anomalies\n",
                "        count_before = len(reviews_stage3_merged_df)\n",
                "        users_before = reviews_stage3_merged_df['user_id'].nunique()\n",
                "\n",
                "        reviews_stage3_filtered_df = reviews_stage3_merged_df[reviews_stage3_merged_df['anomaly_score'] == 1].copy()\n",
                "\n",
                "        count_after = len(reviews_stage3_filtered_df)\n",
                "        users_after = reviews_stage3_filtered_df['user_id'].nunique()\n",
                "\n",
                "        print(f\"Reviews removed by Isolation Forest: {count_before - count_after}\")\n",
                "        print(f\"Users removed: {users_before - users_after}\")\n",
                "\n",
                "        # Optional: Examine features of outliers vs inliers\n",
                "        outliers_df = reviews_stage3_merged_df[reviews_stage3_merged_df['anomaly_score'] == -1]\n",
                "        if not outliers_df.empty and not reviews_stage3_filtered_df.empty:\n",
                "             print(\"\\nFeature Averages (Outliers vs Inliers):\")\n",
                "             print(pd.DataFrame({'Outliers': outliers_df[feature_cols].mean(), 'Inliers': reviews_stage3_filtered_df[feature_cols].mean()}))\n",
                "        del outliers_df\n",
                "        gc.collect()\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error during Isolation Forest execution: {e}\")\n",
                "        # Keep all reviews if IF fails\n",
                "        reviews_stage3_filtered_df = reviews_stage3_merged_df.copy()\n",
                "        if 'anomaly_score' in reviews_stage3_filtered_df.columns:\n",
                "             reviews_stage3_filtered_df.drop(columns=['anomaly_score'], inplace=True)\n",
                "else:\n",
                "    print(\"Skipping Isolation Forest: No features prepared or input data empty.\")\n",
                "    # If IF was skipped, the 'filtered' df is the 'merged' df from before\n",
                "    if not reviews_stage3_merged_df.empty:\n",
                "        reviews_stage3_filtered_df = reviews_stage3_merged_df.copy()\n",
                "    else:\n",
                "        reviews_stage3_filtered_df = pd.DataFrame() # Ensure it's an empty df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stage 3 Evaluation & Saving (Final)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Evaluation after Stage 3\n",
                "print(\"\\n--- Stage 3 Final Metrics (Overall Final) ---\")\n",
                "if not reviews_stage3_filtered_df.empty:\n",
                "    final_review_count_s3 = len(reviews_stage3_filtered_df)\n",
                "    final_user_ids_s3 = reviews_stage3_filtered_df['user_id'].unique()\n",
                "    final_user_count_s3 = len(final_user_ids_s3)\n",
                "    final_business_ids_s3 = reviews_stage3_filtered_df['business_id'].unique()\n",
                "    final_business_count_s3 = len(final_business_ids_s3)\n",
                "\n",
                "    print(f\"Final Review Count (End of Stage 3): {final_review_count_s3}\")\n",
                "    print(f\"Final Unique User Count (End of Stage 3): {final_user_count_s3}\")\n",
                "    print(f\"Final Unique Business Count (End of Stage 3): {final_business_count_s3}\")\n",
                "\n",
                "    print(\"\\n--- Comparison: Stage 2 vs Stage 3 ---\")\n",
                "    print(f\"Reviews: {final_review_count_s2} -> {final_review_count_s3} - Removed in Stage 3: {final_review_count_s2 - final_review_count_s3}\")\n",
                "    print(f\"Users: {final_user_count_s2} -> {final_user_count_s3} - Removed in Stage 3: {final_user_count_s2 - final_user_count_s3}\")\n",
                "else:\n",
                "    print(\"Stage 3 DataFrame is empty. No final metrics.\")\n",
                "    final_review_count_s3 = 0\n",
                "    final_user_count_s3 = 0\n",
                "    final_business_count_s3 = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Final Cleaned Data\n",
                "print(\"\\n--- Saving Final Cleaned Data ---\")\n",
                "\n",
                "if not reviews_stage3_filtered_df.empty:\n",
                "    # Final Reviews\n",
                "    reviews_final_cleaned_df = reviews_stage3_filtered_df[review_cols_to_keep_output].copy() # Select original review columns\n",
                "    output_reviews_final_path = os.path.join(output_dir, 'reviews_final_cleaned.json')\n",
                "    reviews_final_cleaned_df.to_json(output_reviews_final_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved final cleaned reviews ({len(reviews_final_cleaned_df)}) to: {output_reviews_final_path}\")\n",
                "\n",
                "    # Final Users (Filter the users loaded from Stage 2)\n",
                "    if not users_stage3_input_df.empty:\n",
                "        users_final_cleaned_df = users_stage3_input_df[users_stage3_input_df['user_id'].isin(final_user_ids_s3)].copy()\n",
                "        output_users_final_path = os.path.join(output_dir, 'users_final_cleaned.json')\n",
                "        users_final_cleaned_df.to_json(output_users_final_path, orient='records', lines=True, date_format='iso')\n",
                "        print(f\"Saved final cleaned users ({len(users_final_cleaned_df)}) to: {output_users_final_path}\")\n",
                "    else:\n",
                "        print(\"Warning: Stage 2 users input was empty, cannot save final users.\")\n",
                "        users_final_cleaned_df = pd.DataFrame()\n",
                "\n",
                "    # Final Businesses (Filter the original business_df)\n",
                "    if not business_df.empty:\n",
                "        business_final_cleaned_df = business_df[business_df['business_id'].isin(final_business_ids_s3)].copy()\n",
                "        output_business_final_path = os.path.join(output_dir, 'business_final_cleaned.json')\n",
                "        business_final_cleaned_df.to_json(output_business_final_path, orient='records', lines=True, date_format='iso')\n",
                "        print(f\"Saved final cleaned businesses ({len(business_final_cleaned_df)}) to: {output_business_final_path}\")\n",
                "    else:\n",
                "        print(\"Warning: Original business_df was empty, cannot save final businesses.\")\n",
                "        business_final_cleaned_df = pd.DataFrame()\n",
                "\n",
                "    # Final Memory Cleanup\n",
                "    del reviews_stage3_input_df\n",
                "    del users_stage3_input_df\n",
                "    del reviews_stage3_merged_df\n",
                "    del reviews_stage3_filtered_df\n",
                "    del features_df\n",
                "    del reviews_final_cleaned_df\n",
                "    del users_final_cleaned_df\n",
                "    del business_final_cleaned_df\n",
                "    del users_df # Original users no longer needed\n",
                "    del business_df # Original business no longer needed\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Stage 3 DataFrame is empty. Cannot save final files.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 4: Post-Cleaning Summary"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Summary of Cleaning Process (Chunking Refactor):**\n",
                "\n",
                "*   **Initial Data:** Started with Y users, Z businesses (Review count processed in chunks).\n",
                "*   **Chunk Processing:** Reviews loaded in chunks. Merged with users, feature engineered, and filtered by text length (A), low usefulness (F), and URLs/Emails (2A) within each chunk.\n",
                "*   **Combined Data (Baseline):** `baseline_review_count` reviews and `baseline_user_count` users remaining after combining chunks.\n",
                "*   **After Stage 1 (User Aggregation Rules):** Reduced to `final_review_count_s1` reviews and `final_user_count_s1` users.\n",
                "    *   Removed users/reviews based on burst rate (B), short activity windows (C), high rating deviation (D, non-elite), and low engagement (E).\n",
                "*   **After Stage 2 (Text Heuristics):** Reduced to `final_review_count_s2` reviews and `final_user_count_s2` users.\n",
                "    *   Removed reviews based on text repetition (B, optional/if NLTK present).\n",
                "*   **After Stage 3 (Isolation Forest):** Reduced to `final_review_count_s3` reviews and `final_user_count_s3` users.\n",
                "    *   Removed reviews identified as anomalies based on selected features.\n",
                "\n",
                "**Final Output:**\n",
                "\n",
                "*   `reviews_final_cleaned.json`: Contains `final_review_count_s3` cleaned reviews.\n",
                "*   `users_final_cleaned.json`: Contains `final_user_count_s3` users corresponding to the final reviews.\n",
                "*   `business_final_cleaned.json`: Contains `final_business_count_s3` businesses corresponding to the final reviews.\n",
                "\n",
                "These files are located in the `yelp_dataset/cleaned_data/` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n--- Cleaning Process Complete ---\")\n",
                "print(f\"Final review count: {final_review_count_s3 if 'final_review_count_s3' in locals() else 'N/A'}\")\n",
                "print(f\"Final user count: {final_user_count_s3 if 'final_user_count_s3' in locals() else 'N/A'}\")\n",
                "print(f\"Final business count: {final_business_count_s3 if 'final_business_count_s3' in locals() else 'N/A'}\")\n",
                "print(f\"Cleaned files saved in: {output_dir}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
