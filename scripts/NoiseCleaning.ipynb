{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Data Cleaning: Noise Reduction (Chunking Refactor)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 0: Setup and Initial Load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\wiztu\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import datetime\n",
                "import re\n",
                "import os\n",
                "import random\n",
                "import gc # Garbage Collector\n",
                "from sklearn.ensemble import IsolationForest\n",
                "import nltk # Uncomment if using stopwords in Phase 2\n",
                "from nltk.corpus import stopwords # Uncomment if using stopwords\n",
                "from nltk.tokenize import word_tokenize # Uncomment if using stopwords\n",
                "# Potential plotting libraries (optional)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Settings\n",
                "pd.set_option('display.max_columns', None) # Show all columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output directory created/exists: ../yelp_dataset/cleaned_data\n"
                    ]
                }
            ],
            "source": [
                "# Define File Paths\n",
                "input_dir = '../yelp_dataset/' # Relative to script location\n",
                "review_file = os.path.join(input_dir, 'yelp_academic_dataset_review.json')\n",
                "user_file = os.path.join(input_dir, 'yelp_academic_dataset_user.json')\n",
                "business_file = os.path.join(input_dir, 'yelp_academic_dataset_business.json')\n",
                "# tip_file = os.path.join(input_dir, 'yelp_academic_dataset_tip.json') # Optional\n",
                "\n",
                "# Define Output Directory\n",
                "output_dir = os.path.join(input_dir, 'cleaned_data')\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "print(f\"Output directory created/exists: {output_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading user data...\n",
                        "Loading business data...\n",
                        "Static data loading complete.\n",
                        "--- Initial Static Data Shapes ---\n",
                        "Users: (1987897, 22)\n",
                        "Business: (150346, 14)\n",
                        "\n",
                        "--- User Columns & Head ---\n",
                        "Index(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n",
                        "       'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot',\n",
                        "       'compliment_more', 'compliment_profile', 'compliment_cute',\n",
                        "       'compliment_list', 'compliment_note', 'compliment_plain',\n",
                        "       'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
                        "       'compliment_photos'],\n",
                        "      dtype='object')\n",
                        "                  user_id    name  review_count        yelping_since  useful  \\\n",
                        "0  qVc8ODYU5SZjKXVBgXdI7w  Walker           585  2007-01-25 16:47:26    7217   \n",
                        "1  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333  2009-01-25 04:35:42   43091   \n",
                        "\n",
                        "   funny   cool                                              elite  \\\n",
                        "0   1259   5994                                               2007   \n",
                        "1  13066  27281  2009,2010,2011,2012,2013,2014,2015,2016,2017,2...   \n",
                        "\n",
                        "                                             friends  fans  average_stars  \\\n",
                        "0  NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...   267           3.91   \n",
                        "1  ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  3138           3.74   \n",
                        "\n",
                        "   compliment_hot  compliment_more  compliment_profile  compliment_cute  \\\n",
                        "0             250               65                  55               56   \n",
                        "1            1145              264                 184              157   \n",
                        "\n",
                        "   compliment_list  compliment_note  compliment_plain  compliment_cool  \\\n",
                        "0               18              232               844              467   \n",
                        "1              251             1847              7054             3131   \n",
                        "\n",
                        "   compliment_funny  compliment_writer  compliment_photos  \n",
                        "0               467                239                180  \n",
                        "1              3131               1521               1946  \n",
                        "\n",
                        "--- Business Columns & Head ---\n",
                        "Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
                        "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
                        "       'attributes', 'categories', 'hours'],\n",
                        "      dtype='object')\n",
                        "              business_id                      name  \\\n",
                        "0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n",
                        "1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
                        "\n",
                        "                           address           city state postal_code  \\\n",
                        "0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n",
                        "1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n",
                        "\n",
                        "    latitude   longitude  stars  review_count  is_open  \\\n",
                        "0  34.426679 -119.711197    5.0             7        0   \n",
                        "1  38.551126  -90.335695    3.0            15        1   \n",
                        "\n",
                        "                               attributes  \\\n",
                        "0           {'ByAppointmentOnly': 'True'}   \n",
                        "1  {'BusinessAcceptsCreditCards': 'True'}   \n",
                        "\n",
                        "                                          categories  \\\n",
                        "0  Doctors, Traditional Chinese Medicine, Naturop...   \n",
                        "1  Shipping Centers, Local Services, Notaries, Ma...   \n",
                        "\n",
                        "                                               hours  \n",
                        "0                                               None  \n",
                        "1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n",
                        "\n",
                        "--- Missing Values (Static) ---\n",
                        "\n",
                        "Users:\n",
                        " user_id               0\n",
                        "name                  0\n",
                        "review_count          0\n",
                        "yelping_since         0\n",
                        "useful                0\n",
                        "funny                 0\n",
                        "cool                  0\n",
                        "elite                 0\n",
                        "friends               0\n",
                        "fans                  0\n",
                        "average_stars         0\n",
                        "compliment_hot        0\n",
                        "compliment_more       0\n",
                        "compliment_profile    0\n",
                        "compliment_cute       0\n",
                        "compliment_list       0\n",
                        "compliment_note       0\n",
                        "compliment_plain      0\n",
                        "compliment_cool       0\n",
                        "compliment_funny      0\n",
                        "compliment_writer     0\n",
                        "compliment_photos     0\n",
                        "dtype: int64\n",
                        "\n",
                        "Business:\n",
                        " business_id         0\n",
                        "name                0\n",
                        "address             0\n",
                        "city                0\n",
                        "state               0\n",
                        "postal_code         0\n",
                        "latitude            0\n",
                        "longitude           0\n",
                        "stars               0\n",
                        "review_count        0\n",
                        "is_open             0\n",
                        "attributes      13744\n",
                        "categories        103\n",
                        "hours           23223\n",
                        "dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# Load Static Data (Users and Business)\n",
                "print(\"Loading user data...\")\n",
                "users_df = pd.read_json(user_file, lines=True)\n",
                "print(\"Loading business data...\")\n",
                "business_df = pd.read_json(business_file, lines=True)\n",
                "print(\"Static data loading complete.\")\n",
                "\n",
                "print(\"--- Initial Static Data Shapes ---\")\n",
                "print(f\"Users: {users_df.shape}\")     # Expected: (1987897, 22)\n",
                "print(f\"Business: {business_df.shape}\") # Expected: (150346, 14)\n",
                "\n",
                "print(\"\\n--- User Columns & Head ---\")\n",
                "print(users_df.columns)\n",
                "print(users_df.head(2))\n",
                "\n",
                "print(\"\\n--- Business Columns & Head ---\")\n",
                "print(business_df.columns)\n",
                "print(business_df.head(2))\n",
                "\n",
                "print(\"\\n--- Missing Values (Static) ---\")\n",
                "print(\"\\nUsers:\\n\", users_df.isnull().sum())\n",
                "print(\"\\nBusiness:\\n\", business_df.isnull().sum()) # Address/postal code NaNs expected"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define helper function (moved from original Phase 1)\n",
                "def count_friends(friends_str):\n",
                "    if friends_str is None or friends_str == 'None' or pd.isna(friends_str):\n",
                "        return 0\n",
                "    return len([f for f in friends_str.split(',') if f])\n",
                "\n",
                "# Define URL pattern (moved from original Phase 2)\n",
                "url_pattern = r'http[s]?://|www\\\\.|\\\\S+\\\\.(com|net|org|edu|gov)\\\\S*|\\\\S+@\\\\S+'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chunk Processing Loop (Reviews)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting review processing in chunks of 500000...\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Processing Chunk 1 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 500000 (Removed 0 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499398 reviews\n",
                        "    Filter F (Low Usefulness): Kept 455494 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 454744 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 45256\n",
                        "  Chunk 1 processing complete. Appended 454744 reviews.\n",
                        "\n",
                        "--- Processing Chunk 2 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499996 (Removed 4 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499469 reviews\n",
                        "    Filter F (Low Usefulness): Kept 457006 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 456239 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 43757\n",
                        "  Chunk 2 processing complete. Appended 456239 reviews.\n",
                        "\n",
                        "--- Processing Chunk 3 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499998 (Removed 2 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499482 reviews\n",
                        "    Filter F (Low Usefulness): Kept 456688 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 455951 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 44047\n",
                        "  Chunk 3 processing complete. Appended 455951 reviews.\n",
                        "\n",
                        "--- Processing Chunk 4 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 500000 (Removed 0 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499385 reviews\n",
                        "    Filter F (Low Usefulness): Kept 455198 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 454463 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 45537\n",
                        "  Chunk 4 processing complete. Appended 454463 reviews.\n",
                        "\n",
                        "--- Processing Chunk 5 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499997 (Removed 3 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499400 reviews\n",
                        "    Filter F (Low Usefulness): Kept 457206 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 456390 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 43607\n",
                        "  Chunk 5 processing complete. Appended 456390 reviews.\n",
                        "\n",
                        "--- Processing Chunk 6 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499997 (Removed 3 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499500 reviews\n",
                        "    Filter F (Low Usefulness): Kept 457474 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 456685 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 43312\n",
                        "  Chunk 6 processing complete. Appended 456685 reviews.\n",
                        "\n",
                        "--- Processing Chunk 7 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499997 (Removed 3 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499443 reviews\n",
                        "    Filter F (Low Usefulness): Kept 458107 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 457371 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 42626\n",
                        "  Chunk 7 processing complete. Appended 457371 reviews.\n",
                        "\n",
                        "--- Processing Chunk 8 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 500000 (Removed 0 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499395 reviews\n",
                        "    Filter F (Low Usefulness): Kept 455751 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 455034 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 44966\n",
                        "  Chunk 8 processing complete. Appended 455034 reviews.\n",
                        "\n",
                        "--- Processing Chunk 9 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499998 (Removed 2 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499477 reviews\n",
                        "    Filter F (Low Usefulness): Kept 457176 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 456405 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 43593\n",
                        "  Chunk 9 processing complete. Appended 456405 reviews.\n",
                        "\n",
                        "--- Processing Chunk 10 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499995 (Removed 5 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499460 reviews\n",
                        "    Filter F (Low Usefulness): Kept 456983 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 456185 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 43810\n",
                        "  Chunk 10 processing complete. Appended 456185 reviews.\n",
                        "\n",
                        "--- Processing Chunk 11 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 500000 (Removed 0 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499446 reviews\n",
                        "    Filter F (Low Usefulness): Kept 454704 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 453956 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 46044\n",
                        "  Chunk 11 processing complete. Appended 453956 reviews.\n",
                        "\n",
                        "--- Processing Chunk 12 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499999 (Removed 1 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499454 reviews\n",
                        "    Filter F (Low Usefulness): Kept 456245 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 455486 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 44513\n",
                        "  Chunk 12 processing complete. Appended 455486 reviews.\n",
                        "\n",
                        "--- Processing Chunk 13 --- (500000 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 499995 (Removed 5 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 499424 reviews\n",
                        "    Filter F (Low Usefulness): Kept 456983 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 456227 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 43768\n",
                        "  Chunk 13 processing complete. Appended 456227 reviews.\n",
                        "\n",
                        "--- Processing Chunk 14 --- (490280 reviews)\n",
                        "  Merging with users...\n",
                        "  Reviews after merge: 490275 (Removed 5 due to missing users)\n",
                        "  Performing feature engineering...\n",
                        "    Dropped 0 rows due to NaT dates.\n",
                        "  Applying review-level filters...\n",
                        "    Filter A (Text Length >= 50): Kept 489805 reviews\n",
                        "    Filter F (Low Usefulness): Kept 448357 reviews\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1114296421.py:62: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
                        "  contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "    Filter 2A (URLs/Emails): Kept 447620 reviews\n",
                        "  Total reviews removed in this chunk by review-level filters: 42655\n",
                        "  Chunk 14 processing complete. Appended 447620 reviews.\n",
                        "\n",
                        "Finished processing all chunks. Total reviews processed: 6990280\n",
                        "Total reviews kept after chunk-level processing: 6372756\n"
                    ]
                }
            ],
            "source": [
                "chunk_size = 500000\n",
                "processed_chunks = []\n",
                "total_reviews_processed = 0\n",
                "total_reviews_kept_chunk_phase = 0\n",
                "\n",
                "print(f\"Starting review processing in chunks of {chunk_size}...\")\n",
                "\n",
                "for i, review_chunk in enumerate(pd.read_json(review_file, lines=True, chunksize=chunk_size)):\n",
                "    print(f\"\\n--- Processing Chunk {i+1} --- ({len(review_chunk)} reviews)\")\n",
                "    total_reviews_processed += len(review_chunk)\n",
                "\n",
                "    # 1. Merge with Users\n",
                "    print(\"  Merging with users...\")\n",
                "    chunk_merged = pd.merge(review_chunk, users_df, on='user_id', how='inner', suffixes=('_review', '_user'))\n",
                "    print(f\"  Reviews after merge: {len(chunk_merged)} (Removed {len(review_chunk) - len(chunk_merged)} due to missing users)\")\n",
                "    del review_chunk # Free memory\n",
                "    gc.collect()\n",
                "\n",
                "    # 2. Feature Engineering (Chunk-level)\n",
                "    print(\"  Performing feature engineering...\")\n",
                "    chunk_merged['date'] = pd.to_datetime(chunk_merged['date'], errors='coerce')\n",
                "    chunk_merged['yelping_since'] = pd.to_datetime(chunk_merged['yelping_since'], errors='coerce')\n",
                "    # Drop rows where date conversion failed\n",
                "    initial_chunk_count = len(chunk_merged)\n",
                "    chunk_merged.dropna(subset=['date', 'yelping_since'], inplace=True)\n",
                "    print(f\"    Dropped {initial_chunk_count - len(chunk_merged)} rows due to NaT dates.\")\n",
                "\n",
                "    chunk_merged['text_length'] = chunk_merged['text'].astype(str).str.len()\n",
                "    compliment_cols = [col for col in chunk_merged.columns if col.startswith('compliment_')]\n",
                "    chunk_merged['user_compliments_total'] = chunk_merged[compliment_cols].sum(axis=1)\n",
                "    chunk_merged['user_friends_count'] = chunk_merged['friends'].apply(count_friends)\n",
                "    if 'elite' in chunk_merged.columns:\n",
                "        chunk_merged['user_is_elite'] = chunk_merged['elite'].apply(lambda x: isinstance(x, str) and x != '' and x is not None)\n",
                "    else:\n",
                "        chunk_merged['user_is_elite'] = False\n",
                "\n",
                "    # 3. Apply Review-Level Filters (A, F, 2A)\n",
                "    print(\"  Applying review-level filters...\")\n",
                "    count_before_filters = len(chunk_merged)\n",
                "\n",
                "    # Filter A: Text Length\n",
                "    MIN_REVIEW_LENGTH = 50\n",
                "    chunk_merged = chunk_merged[chunk_merged['text_length'] >= MIN_REVIEW_LENGTH]\n",
                "    print(f\"    Filter A (Text Length >= {MIN_REVIEW_LENGTH}): Kept {len(chunk_merged)} reviews\")\n",
                "\n",
                "    # Filter F: Low Usefulness (Old/Short/0-useful)\n",
                "    MAX_REVIEW_AGE_YEARS = 2\n",
                "    cutoff_date = pd.Timestamp.now(tz='UTC') - pd.DateOffset(years=MAX_REVIEW_AGE_YEARS)\n",
                "    if chunk_merged['date'].dt.tz is not None:\n",
                "        cutoff_date = cutoff_date.tz_convert(chunk_merged['date'].dt.tz)\n",
                "    else:\n",
                "        cutoff_date = cutoff_date.tz_localize(None)\n",
                "    condition_f = (\n",
                "        (chunk_merged['date'] < cutoff_date) & \n",
                "        (chunk_merged['useful_review'] == 0) & \n",
                "        (chunk_merged['text_length'] < 150)\n",
                "    )\n",
                "    chunk_merged = chunk_merged[~condition_f]\n",
                "    print(f\"    Filter F (Low Usefulness): Kept {len(chunk_merged)} reviews\")\n",
                "\n",
                "    # Filter 2A: URLs/Emails\n",
                "    contains_url_mask = chunk_merged['text'].str.contains(url_pattern, regex=True, na=False)\n",
                "    chunk_merged = chunk_merged[~contains_url_mask]\n",
                "    print(f\"    Filter 2A (URLs/Emails): Kept {len(chunk_merged)} reviews\")\n",
                "\n",
                "    reviews_removed_in_chunk = count_before_filters - len(chunk_merged)\n",
                "    print(f\"  Total reviews removed in this chunk by review-level filters: {reviews_removed_in_chunk}\")\n",
                "    total_reviews_kept_chunk_phase += len(chunk_merged)\n",
                "\n",
                "    # 4. Append to list (select necessary columns to save memory)\n",
                "    # Keep all columns needed for Phase 1 filters + final output\n",
                "    cols_to_keep_for_phase1 = [\n",
                "        'review_id', 'user_id', 'business_id', 'stars', \n",
                "        'useful_review', 'funny_review', 'cool_review', 'text', 'date', \n",
                "        'text_length', 'user_compliments_total', 'user_friends_count', \n",
                "        'user_is_elite', 'review_count' # Original user review count needed for Filter E\n",
                "        # 'yelping_since' # Not strictly needed after date drop\n",
                "    ]\n",
                "    # Ensure all needed columns exist before selecting\n",
                "    existing_cols_to_keep = [col for col in cols_to_keep_for_phase1 if col in chunk_merged.columns]\n",
                "    processed_chunks.append(chunk_merged[existing_cols_to_keep])\n",
                "    print(f\"  Chunk {i+1} processing complete. Appended {len(chunk_merged)} reviews.\")\n",
                "\n",
                "    # 5. Memory Cleanup\n",
                "    del chunk_merged\n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\nFinished processing all chunks. Total reviews processed: {total_reviews_processed}\")\n",
                "print(f\"Total reviews kept after chunk-level processing: {total_reviews_kept_chunk_phase}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Combine Processed Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Combining processed chunks...\n",
                        "Combined DataFrame shape: (6372756, 14)\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nCombining processed chunks...\")\n",
                "if processed_chunks:\n",
                "    reviews_combined_df = pd.concat(processed_chunks, ignore_index=True)\n",
                "    print(f\"Combined DataFrame shape: {reviews_combined_df.shape}\")\n",
                "    # Clear chunk list from memory\n",
                "    del processed_chunks\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"No chunks were processed or kept. Exiting.\")\n",
                "    # Handle exit or create an empty DataFrame if needed\n",
                "    reviews_combined_df = pd.DataFrame() # Or raise an error\n",
                "\n",
                "# Ensure date is datetime type after concat\n",
                "if not reviews_combined_df.empty:\n",
                "    reviews_combined_df['date'] = pd.to_datetime(reviews_combined_df['date'], errors='coerce')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 1: Rule-Based Filtering (User Aggregation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Baseline Metrics (After Chunking/Combining) ---\n",
                        "Combined Unique User Count: 1836520\n",
                        "Combined Review Count: 6372756\n",
                        "Star-Rating Distribution (%):\n",
                        "stars\n",
                        "1    15.981986\n",
                        "2     8.171818\n",
                        "3    10.227977\n",
                        "4    20.934867\n",
                        "5    44.683352\n",
                        "Name: proportion, dtype: float64\n",
                        "Combined Avg Text Length: 609.63\n",
                        "Combined Avg Useful Votes / Review: 1.29\n",
                        "\n",
                        "Baseline Avg User Metrics (Unique Users):\n",
                        "  Avg Compliments: 15.81\n",
                        "  Avg Friends    : 53.67\n",
                        "  % Elite Users  : 4.92%\n"
                    ]
                }
            ],
            "source": [
                "print(\"--- Baseline Metrics (After Chunking/Combining) ---\")\n",
                "\n",
                "# scalar baselines you’ll need later\n",
                "baseline_review_count   = 0\n",
                "baseline_user_count     = 0\n",
                "baseline_avg_text_len   = np.nan\n",
                "baseline_avg_useful     = np.nan\n",
                "baseline_user_agg       = pd.Series(dtype=float)   # user-level means\n",
                "\n",
                "# -------- sanity-check --------\n",
                "if \"reviews_combined_df\" not in locals():\n",
                "    print(\"ERROR: reviews_combined_df not found.\")\n",
                "    reviews_combined_df = pd.DataFrame()\n",
                "\n",
                "# -------- metrics --------\n",
                "if not reviews_combined_df.empty:\n",
                "    baseline_review_count = len(reviews_combined_df)\n",
                "    baseline_user_count   = reviews_combined_df[\"user_id\"].nunique()\n",
                "    print(f\"Combined Unique User Count: {baseline_user_count}\")\n",
                "    print(f\"Combined Review Count: {baseline_review_count}\")\n",
                "\n",
                "    # quick check for required columns\n",
                "    req_cols = [\n",
                "        \"stars\", \"text_length\", \"useful_review\",\n",
                "        \"user_compliments_total\", \"user_friends_count\", \"user_is_elite\"\n",
                "    ]\n",
                "    missing_cols = [c for c in req_cols if c not in reviews_combined_df.columns]\n",
                "    if missing_cols:\n",
                "        print(f\"WARNING: Missing cols for baseline metrics: {missing_cols}\")\n",
                "    else:\n",
                "        print(\"Star-Rating Distribution (%):\")\n",
                "        print(reviews_combined_df[\"stars\"]\n",
                "              .value_counts(normalize=True).sort_index()*100)\n",
                "\n",
                "        # ---------- store scalar review-level baselines ----------\n",
                "        baseline_avg_text_len = reviews_combined_df[\"text_length\"].mean()\n",
                "        baseline_avg_useful   = reviews_combined_df[\"useful_review\"].mean()\n",
                "\n",
                "        print(f\"Combined Avg Text Length: {baseline_avg_text_len:.2f}\")\n",
                "        print(f\"Combined Avg Useful Votes / Review: {baseline_avg_useful:.2f}\")\n",
                "\n",
                "        # ---------- user-level means ----------\n",
                "        df_for_agg = reviews_combined_df.drop_duplicates(\"user_id\").copy()\n",
                "        baseline_user_agg = (\n",
                "            df_for_agg[[\"user_compliments_total\",\n",
                "                        \"user_friends_count\",\n",
                "                        \"user_is_elite\"]]\n",
                "            .rename(columns={\n",
                "                \"user_compliments_total\": \"avg_compliments\",\n",
                "                \"user_friends_count\"   : \"avg_friends\",\n",
                "                \"user_is_elite\"        : \"pct_elite\"\n",
                "            })\n",
                "            .mean()\n",
                "        )\n",
                "\n",
                "        print(\"\\nBaseline Avg User Metrics (Unique Users):\")\n",
                "        print(f\"  Avg Compliments: {baseline_user_agg['avg_compliments']:.2f}\")\n",
                "        print(f\"  Avg Friends    : {baseline_user_agg['avg_friends']:.2f}\")\n",
                "        print(f\"  % Elite Users  : {baseline_user_agg['pct_elite']*100:.2f}%\")\n",
                "\n",
                "    # hand off for Stage-1 filters (no .copy() to save RAM)\n",
                "    reviews_stage1_df = reviews_combined_df\n",
                "\n",
                "else:\n",
                "    print(\"reviews_combined_df is empty – nothing to summarise\")\n",
                "    reviews_stage1_df = pd.DataFrame()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Apply User-Level Filters (Iteratively)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Filter B: Review Bursts ---\n",
                        "Users identified with burst rate > 10: 4329\n",
                        "Reviews removed: 359159\n",
                        "Users removed: 4329\n",
                        "Avg max reviews/day before: 3.60, after: 2.81\n"
                    ]
                }
            ],
            "source": [
                "# Filters B, C, D, E are applied here on the combined data\n",
                "if not reviews_stage1_df.empty:\n",
                "    # (B) Filter by Review Bursts\n",
                "    print(\"\\n--- Filter B: Review Bursts ---\")\n",
                "    count_before = len(reviews_stage1_df)\n",
                "    users_before = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # Calculate reviews per user per day\n",
                "    reviews_per_day = reviews_stage1_df.groupby([reviews_stage1_df['user_id'], reviews_stage1_df['date'].dt.date])\\\n",
                "                                         .size().reset_index(name='reviews_per_day')\n",
                "\n",
                "    # Find max reviews per day for each user\n",
                "    max_reviews_per_user = reviews_per_day.groupby('user_id')['reviews_per_day'].max().reset_index(name='max_reviews_per_day')\n",
                "\n",
                "    # Merge max back to the main df\n",
                "    reviews_stage1_df = pd.merge(reviews_stage1_df, max_reviews_per_user, on='user_id', how='left')\n",
                "\n",
                "    avg_max_burst_before = reviews_stage1_df['max_reviews_per_day'].mean()\n",
                "\n",
                "    # Apply filter\n",
                "    MAX_BURST_RATE = 10 # Reviews per day\n",
                "    users_to_remove_burst = reviews_stage1_df[reviews_stage1_df['max_reviews_per_day'] > MAX_BURST_RATE]['user_id'].unique()\n",
                "    reviews_stage1_df = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove_burst)]\n",
                "\n",
                "    count_after = len(reviews_stage1_df)\n",
                "    users_after = reviews_stage1_df['user_id'].nunique()\n",
                "    # Recalculate mean only if df is not empty\n",
                "    avg_max_burst_after = reviews_stage1_df['max_reviews_per_day'].mean() if not reviews_stage1_df.empty else 0\n",
                "\n",
                "    print(f\"Users identified with burst rate > {MAX_BURST_RATE}: {len(users_to_remove_burst)}\")\n",
                "    print(f\"Reviews removed: {count_before - count_after}\")\n",
                "    print(f\"Users removed: {users_before - users_after}\")\n",
                "    print(f\"Avg max reviews/day before: {avg_max_burst_before:.2f}, after: {avg_max_burst_after:.2f}\")\n",
                "\n",
                "    # Drop the temporary column\n",
                "    reviews_stage1_df.drop(columns=['max_reviews_per_day'], inplace=True)\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Skipping Filter B: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Baseline Metrics (After Chunking/Combining) ---\n",
                        "Combined Unique User Count: 1836520\n",
                        "Combined Review Count: 6372756\n",
                        "Combined Star Rating Distribution (%):\n",
                        "stars\n",
                        "1    15.981986\n",
                        "2     8.171818\n",
                        "3    10.227977\n",
                        "4    20.934867\n",
                        "5    44.683352\n",
                        "Name: proportion, dtype: float64\n",
                        "Combined Avg Text Length: 609.63\n",
                        "Combined Avg Useful Votes / Review: 1.29\n",
                        "\n",
                        "Baseline Avg User Metrics (Unique Users):\n",
                        "  Avg Compliments: 15.81\n",
                        "  Avg Friends Count: 53.67\n",
                        "  % Elite Users: 4.92%\n",
                        "\n",
                        "(Freed memory from reviews_combined_df)\n"
                    ]
                }
            ],
            "source": [
                "print(\"--- Baseline Metrics (After Chunking/Combining) ---\")\n",
                "baseline_review_count = baseline_user_count = 0\n",
                "baseline_user_agg = pd.Series(dtype=float)          # will become a Series later\n",
                "\n",
                "# ---------- sanity-check that the combined reviews exist ----------\n",
                "if \"reviews_combined_df\" not in locals():\n",
                "    print(\"ERROR: reviews_combined_df not found.  Did the previous cell run?\")\n",
                "    reviews_combined_df = pd.DataFrame()            # placeholder so the rest doesn’t crash\n",
                "\n",
                "# ---------- basic counts ----------\n",
                "if not reviews_combined_df.empty:\n",
                "    baseline_review_count = len(reviews_combined_df)\n",
                "    if \"user_id\" in reviews_combined_df.columns:\n",
                "        baseline_user_count = reviews_combined_df[\"user_id\"].nunique()\n",
                "        print(f\"Combined Unique User Count: {baseline_user_count}\")\n",
                "    else:\n",
                "        print(\"WARNING: 'user_id' missing – cannot count users\")\n",
                "\n",
                "    print(f\"Combined Review Count: {baseline_review_count}\")\n",
                "\n",
                "    # ---------- quick univariate stats ----------\n",
                "    required_cols_baseline = [\n",
                "        \"stars\", \"text_length\", \"useful_review\", \"user_compliments_total\",\n",
                "        \"user_friends_count\", \"user_is_elite\"\n",
                "    ]\n",
                "    missing_cols = [c for c in required_cols_baseline if c not in reviews_combined_df.columns]\n",
                "\n",
                "    if missing_cols:\n",
                "        print(f\"\\nWARNING: Missing columns needed for baseline metrics: {missing_cols}\")\n",
                "    else:\n",
                "        print(\"Combined Star Rating Distribution (%):\")\n",
                "        print(reviews_combined_df[\"stars\"].value_counts(normalize=True).sort_index()*100)\n",
                "        print(f\"Combined Avg Text Length: {reviews_combined_df['text_length'].mean():.2f}\")\n",
                "        print(f\"Combined Avg Useful Votes / Review: {reviews_combined_df['useful_review'].mean():.2f}\")\n",
                "\n",
                "        # ---------- aggregate per-user metrics ----------\n",
                "        df_for_agg = reviews_combined_df.drop_duplicates(\"user_id\").copy()\n",
                "        numeric_fix = {\n",
                "            \"user_compliments_total\": \"avg_compliments\",\n",
                "            \"user_friends_count\": \"avg_friends\",\n",
                "            \"user_is_elite\": \"pct_elite\",\n",
                "        }\n",
                "\n",
                "        # make sure all three cols are numeric / boolean\n",
                "        for col in numeric_fix:\n",
                "            if not pd.api.types.is_numeric_dtype(df_for_agg[col]):\n",
                "                df_for_agg[col] = pd.to_numeric(df_for_agg[col], errors=\"coerce\").fillna(0)\n",
                "\n",
                "        # mean() returns a Series we can rename\n",
                "        baseline_user_agg = (\n",
                "            df_for_agg[list(numeric_fix)]\n",
                "            .mean()\n",
                "            .rename(numeric_fix)\n",
                "        )\n",
                "\n",
                "        # ---------- pretty-print ----------\n",
                "        print(\"\\nBaseline Avg User Metrics (Unique Users):\")\n",
                "        print(f\"  Avg Compliments: {baseline_user_agg['avg_compliments']:.2f}\")\n",
                "        print(f\"  Avg Friends Count: {baseline_user_agg['avg_friends']:.2f}\")\n",
                "        print(f\"  % Elite Users: {baseline_user_agg['pct_elite']*100:.2f}%\")\n",
                "\n",
                "    # ---------- keep copy for Stage 1 filters ----------\n",
                "    reviews_stage1_df = reviews_combined_df\n",
                "\n",
                "    # free memory once baseline metrics are safely stored\n",
                "    if not missing_cols and not baseline_user_agg.empty:\n",
                "        del reviews_combined_df\n",
                "        gc.collect()\n",
                "        print(\"\\n(Freed memory from reviews_combined_df)\")\n",
                "else:\n",
                "    print(\"reviews_combined_df is empty – nothing to summarise\")\n",
                "    reviews_stage1_df = pd.DataFrame()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Filter D: Rating Deviation ---\n",
                        "Users identified with avg deviation > 1.8 (non-elite): 324410\n",
                        "Reviews removed: 491430\n",
                        "Users removed: 324410\n",
                        "Avg user rating deviation before: 1.03, after: 0.92\n"
                    ]
                }
            ],
            "source": [
                "if not reviews_stage1_df.empty:\n",
                "    # (D) Filter by Rating Deviation\n",
                "    print(\"\\n--- Filter D: Rating Deviation ---\")\n",
                "    count_before = len(reviews_stage1_df)\n",
                "    users_before = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # Calculate average star rating per business (using current filtered reviews)\n",
                "    business_avg_stars = reviews_stage1_df.groupby('business_id')['stars'].mean().reset_index(name='business_avg_rating')\n",
                "\n",
                "    # Merge business average rating\n",
                "    reviews_stage1_df = pd.merge(reviews_stage1_df, business_avg_stars, on='business_id', how='left')\n",
                "\n",
                "    # Calculate deviation for each review\n",
                "    reviews_stage1_df['rating_deviation'] = (reviews_stage1_df['stars'] - reviews_stage1_df['business_avg_rating']).abs()\n",
                "\n",
                "    # Calculate average deviation per user\n",
                "    user_avg_deviation = reviews_stage1_df.groupby('user_id')['rating_deviation'].mean().reset_index(name='user_avg_deviation')\n",
                "\n",
                "    # Merge user average deviation\n",
                "    reviews_stage1_df = pd.merge(reviews_stage1_df, user_avg_deviation, on='user_id', how='left')\n",
                "\n",
                "    avg_dev_before = reviews_stage1_df['user_avg_deviation'].mean()\n",
                "\n",
                "    # Apply filter (excluding Elite users)\n",
                "    MAX_AVG_DEVIATION = 1.8\n",
                "    condition = (\n",
                "        (reviews_stage1_df['user_avg_deviation'] > MAX_AVG_DEVIATION) & \n",
                "        (reviews_stage1_df['user_is_elite'] == False)\n",
                "    )\n",
                "    users_to_remove_deviation = reviews_stage1_df[condition]['user_id'].unique()\n",
                "    reviews_stage1_df = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove_deviation)]\n",
                "\n",
                "    count_after = len(reviews_stage1_df)\n",
                "    users_after = reviews_stage1_df['user_id'].nunique()\n",
                "    avg_dev_after = reviews_stage1_df['user_avg_deviation'].mean() if not reviews_stage1_df.empty else 0\n",
                "\n",
                "    print(f\"Users identified with avg deviation > {MAX_AVG_DEVIATION} (non-elite): {len(users_to_remove_deviation)}\")\n",
                "    print(f\"Reviews removed: {count_before - count_after}\")\n",
                "    print(f\"Users removed: {users_before - users_after}\")\n",
                "    print(f\"Avg user rating deviation before: {avg_dev_before:.2f}, after: {avg_dev_after:.2f}\")\n",
                "\n",
                "    # Drop temporary columns\n",
                "    reviews_stage1_df.drop(columns=['business_avg_rating', 'rating_deviation', 'user_avg_deviation'], inplace=True)\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Skipping Filter D: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Filter E: Low Engagement ---\n",
                        "Users identified as low engagement: 369006\n",
                        "  Removed Avg Compliments: 0.00, Avg Friends: 0.00\n",
                        "  Kept Avg Compliments: 212.50, Avg Friends: 147.50\n",
                        "Reviews removed: 503899\n",
                        "Users   removed: 369006\n"
                    ]
                }
            ],
            "source": [
                "if not reviews_stage1_df.empty:\n",
                "    # (E) Filter by Low Engagement (Combined Rule)\n",
                "    print(\"\\n--- Filter E: Low Engagement ---\")\n",
                "\n",
                "    count_before  = len(reviews_stage1_df)\n",
                "    users_before  = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # ------------------------------------------------------------------\n",
                "    # 1. make sure the user-level review_count column exists\n",
                "    # ------------------------------------------------------------------\n",
                "    if 'review_count' not in reviews_stage1_df.columns:\n",
                "        reviews_stage1_df = reviews_stage1_df.merge(\n",
                "            users_df[['user_id', 'review_count']],\n",
                "            on='user_id', how='left', suffixes=('', '_user')\n",
                "        )\n",
                "\n",
                "    # ------------------------------------------------------------------\n",
                "    # 2. build mask & list of users to drop\n",
                "    # ------------------------------------------------------------------\n",
                "    low_engagement_mask = (\n",
                "        (reviews_stage1_df['user_compliments_total'] == 0) &\n",
                "        (reviews_stage1_df['user_friends_count']    == 0) &\n",
                "        (~reviews_stage1_df['user_is_elite']) &                       # faster than == False\n",
                "        (reviews_stage1_df['review_count'].fillna(0) <= 5)\n",
                "    )\n",
                "    users_to_remove = reviews_stage1_df.loc[low_engagement_mask, 'user_id'].unique()\n",
                "\n",
                "    # ------------------------------------------------------------------\n",
                "    # 3. quick stats – guard against “no rows” first\n",
                "    # ------------------------------------------------------------------\n",
                "    if users_to_remove.size:\n",
                "        removed_stats = (\n",
                "            reviews_stage1_df\n",
                "            .loc[reviews_stage1_df['user_id'].isin(users_to_remove),\n",
                "                 ['user_compliments_total', 'user_friends_count']]\n",
                "            .mean()\n",
                "            .rename({'user_compliments_total': 'avg_compliments',\n",
                "                     'user_friends_count'    : 'avg_friends'})\n",
                "        )\n",
                "    else:  # create a Series of NaNs so .loc below never errors\n",
                "        removed_stats = pd.Series({'avg_compliments': np.nan, 'avg_friends': np.nan})\n",
                "\n",
                "    kept_stats = (\n",
                "        reviews_stage1_df\n",
                "        .loc[~reviews_stage1_df['user_id'].isin(users_to_remove),\n",
                "             ['user_compliments_total', 'user_friends_count']]\n",
                "        .mean()\n",
                "        .rename({'user_compliments_total': 'avg_compliments',\n",
                "                 'user_friends_count'    : 'avg_friends'})\n",
                "    )\n",
                "\n",
                "    # ------------------------------------------------------------------\n",
                "    # 4. apply the filter\n",
                "    # ------------------------------------------------------------------\n",
                "    reviews_stage1_df = reviews_stage1_df[~reviews_stage1_df['user_id'].isin(users_to_remove)]\n",
                "\n",
                "    count_after = len(reviews_stage1_df)\n",
                "    users_after = reviews_stage1_df['user_id'].nunique()\n",
                "\n",
                "    # ------------------------------------------------------------------\n",
                "    # 5. reporting\n",
                "    # ------------------------------------------------------------------\n",
                "    print(f\"Users identified as low engagement: {users_to_remove.size}\")\n",
                "    print(f\"  Removed Avg Compliments: {removed_stats['avg_compliments']:.2f}, \"\n",
                "          f\"Avg Friends: {removed_stats['avg_friends']:.2f}\")\n",
                "    print(f\"  Kept Avg Compliments: {kept_stats['avg_compliments']:.2f}, \"\n",
                "          f\"Avg Friends: {kept_stats['avg_friends']:.2f}\")\n",
                "    print(f\"Reviews removed: {count_before - count_after}\")\n",
                "    print(f\"Users   removed: {users_before  - users_after}\")\n",
                "\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Skipping Filter E: Input DataFrame is empty.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stage 1 Evaluation & Saving"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Stage 1 Final Metrics ---\n",
                        "Final Review Count (Stage 1): 5377427\n",
                        "Final Unique User Count (Stage 1): 1143104\n",
                        "Star-Rating Distribution (% - Stage 1):\n",
                        "stars\n",
                        "1    10.879199\n",
                        "2     7.975301\n",
                        "3    11.481458\n",
                        "4    23.774790\n",
                        "5    45.889251\n",
                        "Name: proportion, dtype: float64\n",
                        "Final Avg Text Length            : 614.66\n",
                        "Final Avg Useful Votes per Review: 1.38\n",
                        "\n",
                        "Final Avg User Metrics (Unique Users - Stage 1):\n",
                        "  Avg Compliments: 25.19\n",
                        "  Avg Friends    : 74.83\n",
                        "  % Elite Users  : 7.91%\n",
                        "\n",
                        "--- Comparison: Baseline vs Stage 1 ---\n",
                        "Reviews : 6372756 ➔ 5377427 (84.4%)\n",
                        "Users   : 1836520 ➔ 1143104 (62.2%)\n",
                        "Avg Text Length  : 609.63 ➔ 522.83\n",
                        "Avg Useful Votes : 1.29 ➔ 0.88\n",
                        "Avg Compliments  : 15.81 ➔ 25.19\n",
                        "Avg Friends      : 53.67 ➔ 74.83\n",
                        "% Elite Users    : 4.92% ➔ 7.91%\n"
                    ]
                }
            ],
            "source": [
                "# ------------------------------------------------------------\n",
                "# Stage-1 Final Metrics\n",
                "# ------------------------------------------------------------\n",
                "print(\"\\n--- Stage 1 Final Metrics ---\")\n",
                "\n",
                "if not reviews_stage1_df.empty:\n",
                "    final_review_count_s1 = len(reviews_stage1_df)\n",
                "    final_user_count_s1   = reviews_stage1_df[\"user_id\"].nunique()\n",
                "\n",
                "    print(f\"Final Review Count (Stage 1): {final_review_count_s1}\")\n",
                "    print(f\"Final Unique User Count (Stage 1): {final_user_count_s1}\")\n",
                "\n",
                "    print(\"Star-Rating Distribution (% - Stage 1):\")\n",
                "    print(reviews_stage1_df[\"stars\"]\n",
                "          .value_counts(normalize=True).sort_index()*100)\n",
                "\n",
                "    print(f\"Final Avg Text Length            : \"\n",
                "          f\"{reviews_stage1_df['text_length'].mean():.2f}\")\n",
                "    print(f\"Final Avg Useful Votes per Review: \"\n",
                "          f\"{reviews_stage1_df['useful_review'].mean():.2f}\")\n",
                "\n",
                "    # ---------- per-user metrics ----------\n",
                "    final_user_agg_s1 = (\n",
                "        reviews_stage1_df.drop_duplicates(\"user_id\")\n",
                "        .agg({\n",
                "            \"user_compliments_total\": \"mean\",\n",
                "            \"user_friends_count\"   : \"mean\",\n",
                "            \"text_length\"          : \"mean\",\n",
                "            \"useful_review\"        : \"mean\",\n",
                "            \"user_is_elite\"        : \"mean\"\n",
                "        })\n",
                "        .rename({\n",
                "            \"user_compliments_total\": \"avg_compliments\",\n",
                "            \"user_friends_count\"   : \"avg_friends\",\n",
                "            \"text_length\"          : \"avg_text_length\",\n",
                "            \"useful_review\"        : \"avg_useful_votes\",\n",
                "            \"user_is_elite\"        : \"pct_elite\"\n",
                "        })\n",
                "    )\n",
                "\n",
                "    print(\"\\nFinal Avg User Metrics (Unique Users - Stage 1):\")\n",
                "    print(f\"  Avg Compliments: {final_user_agg_s1['avg_compliments']:.2f}\")\n",
                "    print(f\"  Avg Friends    : {final_user_agg_s1['avg_friends']:.2f}\")\n",
                "    print(f\"  % Elite Users  : {final_user_agg_s1['pct_elite']*100:.2f}%\")\n",
                "\n",
                "    # ---------- baseline vs Stage-1 comparison ----------\n",
                "    print(\"\\n--- Comparison: Baseline vs Stage 1 ---\")\n",
                "    print(f\"Reviews : {baseline_review_count} ➔ {final_review_count_s1} \"\n",
                "          f\"({final_review_count_s1 / baseline_review_count * 100:.1f}%)\")\n",
                "    print(f\"Users   : {baseline_user_count} ➔ {final_user_count_s1} \"\n",
                "          f\"({final_user_count_s1 / baseline_user_count * 100:.1f}%)\")\n",
                "    print(f\"Avg Text Length  : {baseline_avg_text_len:.2f} \"\n",
                "          f\"➔ {final_user_agg_s1['avg_text_length']:.2f}\")\n",
                "    print(f\"Avg Useful Votes : {baseline_avg_useful:.2f} \"\n",
                "          f\"➔ {final_user_agg_s1['avg_useful_votes']:.2f}\")\n",
                "    print(f\"Avg Compliments  : {baseline_user_agg['avg_compliments']:.2f} \"\n",
                "          f\"➔ {final_user_agg_s1['avg_compliments']:.2f}\")\n",
                "    print(f\"Avg Friends      : {baseline_user_agg['avg_friends']:.2f} \"\n",
                "          f\"➔ {final_user_agg_s1['avg_friends']:.2f}\")\n",
                "    print(f\"% Elite Users    : {baseline_user_agg['pct_elite']*100:.2f}% \"\n",
                "          f\"➔ {final_user_agg_s1['pct_elite']*100:.2f}%\")\n",
                "\n",
                "else:\n",
                "    print(\"Stage 1 DataFrame is empty. No metrics to calculate.\")\n",
                "    final_review_count_s1 = final_user_count_s1 = 0\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Qualitative Check (Stage 1) ---\n",
                        "Total kept after Stage 1: 5377427\n",
                        "\n",
                        "Sample Kept Reviews (Stage 1):\n",
                        "                      review_id                 user_id  stars  \\\n",
                        "5308559  2AZFGhEn4dXscJeJPR1IDg  UsQgl-9GE70Dga7Dvu6DUg      5   \n",
                        "3888991  K1lqJvBUiSibv15PeeoVFQ  lDfLMSRQmuG4pihc456pbQ      4   \n",
                        "3684099  AuY3dtRDkyWGhJAzDPHHaw  qQ4v3-TykvfHF7YmpfUifg      4   \n",
                        "3238320  p7UH-OFyvufxbtfgTaJ0CA  iJE3RnVcfHM3jsM4I6WFCw      3   \n",
                        "2635374  MTPF000pWZmNxEZFxAinPA  3ToghYK-MNVVx43wC-iGhA      1   \n",
                        "3154023  YwG8JPZ9LjdUnNLR0Yb7rw  t6QnunqfeTDNFPG311DFoA      3   \n",
                        "2592239  aYJrtHBRjVpVHL11P_saEw  b5IQGA8fXby6CLjeEELUgQ      5   \n",
                        "5555453  0xPdrxVXVqEeJO2AacppeA  m8q0X8bt7UAlawSjbG0c7Q      4   \n",
                        "6022162  rDy1enDTqAXMKqrK0z0eUg  UZ8_xqhiguIYb9Lu2Wu8og      3   \n",
                        "3659898  DVXaagL-hKPDYMQT0xQZ6w  22Y8hc4NYWPa2D6ffodddg      2   \n",
                        "\n",
                        "                                                      text  \n",
                        "5308559  This was our first time here and we wanted to ...  \n",
                        "3888991  i've been to fork for lunch and dinner.  if yo...  \n",
                        "3684099  We popped in looking for a bite on a beautiful...  \n",
                        "3238320  The old Sansom Street Oyster House is now know...  \n",
                        "2635374  Worst experience dealing with management/\"owne...  \n",
                        "3154023  Ate here, and thought the service was great, o...  \n",
                        "2592239  Amazing breakfast! Tried the seafood Benedict ...  \n",
                        "5555453  Call ahead to make sure they're not closed for...  \n",
                        "6022162  Can't ask for anything more convenient than th...  \n",
                        "3659898  What was great about this place is how extensi...  \n"
                    ]
                }
            ],
            "source": [
                "# Qualitative Check (Stage 1)\n",
                "print(\"\\n--- Qualitative Check (Stage 1) ---\")\n",
                "if not reviews_stage1_df.empty:\n",
                "    kept_reviews_s1 = reviews_stage1_df\n",
                "    # To find removed reviews, we need to compare against the state *before* Stage 1 filters\n",
                "    # This requires reloading the combined data or keeping a copy, which might use too much memory.\n",
                "    # Simplified check: Show samples of kept reviews.\n",
                "    print(f\"Total kept after Stage 1: {len(kept_reviews_s1)}\")\n",
                "\n",
                "    if len(kept_reviews_s1) >= 10:\n",
                "        print(\"\\nSample Kept Reviews (Stage 1):\")\n",
                "        print(kept_reviews_s1.sample(min(10, len(kept_reviews_s1)))[['review_id', 'user_id', 'stars', 'text']])\n",
                "    else:\n",
                "        print(\"\\nNot enough kept reviews to sample.\")\n",
                "    # Note: Comparing removed reviews is harder with chunking without reloading.\n",
                "else:\n",
                "    print(\"Stage 1 DataFrame is empty. No qualitative check possible.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Saving Stage 1 Data ---\n",
                        "Saved Stage 1 cleaned reviews (5377427) to: ../yelp_dataset/cleaned_data\\reviews_stage1_cleaned.json\n",
                        "Saved Stage 1 cleaned users (1143104) to: ../yelp_dataset/cleaned_data\\users_stage1_cleaned.json\n"
                    ]
                }
            ],
            "source": [
                "# Save Cleaned Data (Stage 1)\n",
                "print(\"\\n--- Saving Stage 1 Data ---\")\n",
                "if not reviews_stage1_df.empty:\n",
                "    # Select relevant columns for reviews output\n",
                "    review_cols_to_keep_output = ['review_id', 'user_id', 'business_id', 'stars', 'useful_review', 'funny_review', 'cool_review', 'text', 'date']\n",
                "    reviews_stage1_cleaned_df = reviews_stage1_df[review_cols_to_keep_output]\n",
                "\n",
                "    output_reviews_s1_path = os.path.join(output_dir, 'reviews_stage1_cleaned.json')\n",
                "    reviews_stage1_cleaned_df.to_json(output_reviews_s1_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved Stage 1 cleaned reviews ({len(reviews_stage1_cleaned_df)}) to: {output_reviews_s1_path}\")\n",
                "\n",
                "    # Filter original users_df to keep only those remaining\n",
                "    remaining_user_ids_s1 = reviews_stage1_cleaned_df['user_id'].unique()\n",
                "    users_stage1_cleaned_df = users_df[users_df['user_id'].isin(remaining_user_ids_s1)].copy() # Use copy to avoid SettingWithCopyWarning\n",
                "\n",
                "    output_users_s1_path = os.path.join(output_dir, 'users_stage1_cleaned.json')\n",
                "    users_stage1_cleaned_df.to_json(output_users_s1_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved Stage 1 cleaned users ({len(users_stage1_cleaned_df)}) to: {output_users_s1_path}\")\n",
                "\n",
                "    # Prepare for Stage 2 - Keep the filtered reviews DataFrame\n",
                "    reviews_stage2_df = reviews_stage1_df.copy()\n",
                "    del reviews_stage1_df # Free memory from the copy used for saving\n",
                "    del reviews_stage1_cleaned_df\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Stage 1 DataFrame is empty. Cannot save.\")\n",
                "    reviews_stage2_df = pd.DataFrame() # Ensure it exists for next phase check\n",
                "    users_stage1_cleaned_df = pd.DataFrame() # Ensure it exists"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2: Basic Text & Content Heuristics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Apply Text Filters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Filter 2A (URLs/Emails) was applied during chunk processing.\n"
                    ]
                }
            ],
            "source": [
                "# Filter 2A (URLs/Emails) was moved to the chunk processing loop.\n",
                "print(\"Filter 2A (URLs/Emails) was applied during chunk processing.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Filter 2B: Text Repetition (Unique Ratio) ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\wiztu\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     C:\\Users\\wiztu\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calculating unique ratio (may take time)...\n",
                        "Reviews removed with unique ratio < 0.4: 136\n",
                        "Users removed: 25\n",
                        "Avg unique ratio before: 0.896, after: 0.896\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Local\\Temp\\ipykernel_13968\\1677047546.py:62: SettingWithCopyWarning: \n",
                        "A value is trying to be set on a copy of a slice from a DataFrame\n",
                        "\n",
                        "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
                        "  reviews_stage2_df.drop(columns=['unique_ratio'], inplace=True)\n"
                    ]
                }
            ],
            "source": [
                "# (B) Filter by Repetition (Optional - requires NLTK)\n",
                "print(\"\\n--- Filter 2B: Text Repetition (Unique Ratio) ---\")\n",
                "nltk.download('punkt') # Ensure punkt tokenizer is downloaded\n",
                "nltk.download('punkt_tab')\n",
                "SKIP_REPETITION_FILTER = True # Set to True to skip this filter\n",
                "\n",
                "if SKIP_REPETITION_FILTER:\n",
                "    print(\"Skipping Filter 2B (Repetition) as configured.\")\n",
                "elif not reviews_stage2_df.empty:\n",
                "    try:\n",
                "        import nltk\n",
                "        from nltk.corpus import stopwords\n",
                "        from nltk.tokenize import word_tokenize\n",
                "\n",
                "        try:\n",
                "            nltk.data.find('corpora/stopwords')\n",
                "        except nltk.downloader.DownloadError:\n",
                "            print(\"Downloading NLTK stopwords...\")\n",
                "            nltk.download('stopwords')\n",
                "        try:\n",
                "            nltk.data.find('tokenizers/punkt')\n",
                "        except nltk.downloader.DownloadError:\n",
                "            print(\"Downloading NLTK punkt tokenizer...\")\n",
                "            nltk.download('punkt')\n",
                "\n",
                "        stop_words = set(stopwords.words('english'))\n",
                "\n",
                "        def calculate_unique_ratio(text):\n",
                "            if pd.isna(text) or not isinstance(text, str) or len(text.strip()) == 0:\n",
                "                return 1.0 # Assign high ratio if no text\n",
                "            # Tokenize, lowercase, remove punctuation (simple regex), remove stopwords\n",
                "            text = text.lower()\n",
                "            text = re.sub(r'[^\\w\\s]', '', text) # Keep words and spaces\n",
                "            tokens = word_tokenize(text)\n",
                "            words = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
                "            if not words: # Handle case of no words after filtering\n",
                "                return 1.0\n",
                "            unique_words = set(words)\n",
                "            ratio = len(unique_words) / len(words)\n",
                "            return ratio\n",
                "\n",
                "        print(\"Calculating unique ratio (may take time)...\")\n",
                "        reviews_stage2_df['unique_ratio'] = reviews_stage2_df['text'].apply(calculate_unique_ratio)\n",
                "\n",
                "        avg_ratio_before = reviews_stage2_df['unique_ratio'].mean()\n",
                "\n",
                "        MIN_UNIQUE_RATIO = 0.4\n",
                "        count_before = len(reviews_stage2_df)\n",
                "        users_before = reviews_stage2_df['user_id'].nunique()\n",
                "\n",
                "        reviews_stage2_df = reviews_stage2_df[reviews_stage2_df['unique_ratio'] >= MIN_UNIQUE_RATIO]\n",
                "\n",
                "        count_after = len(reviews_stage2_df)\n",
                "        users_after = reviews_stage2_df['user_id'].nunique()\n",
                "        avg_ratio_after = reviews_stage2_df['unique_ratio'].mean() if not reviews_stage2_df.empty else 0\n",
                "\n",
                "        print(f\"Reviews removed with unique ratio < {MIN_UNIQUE_RATIO}: {count_before - count_after}\")\n",
                "        print(f\"Users removed: {users_before - users_after}\")\n",
                "        print(f\"Avg unique ratio before: {avg_ratio_before:.3f}, after: {avg_ratio_after:.3f}\")\n",
                "\n",
                "        # Drop temporary column\n",
                "        reviews_stage2_df.drop(columns=['unique_ratio'], inplace=True)\n",
                "        gc.collect()\n",
                "\n",
                "    except ImportError:\n",
                "        print(\"Skipping Filter 2B (Repetition) because NLTK is not installed.\")\n",
                "        print(\"Please install NLTK (`pip install nltk`) and run the cell again if needed.\")\n",
                "else:\n",
                "     print(\"Skipping Filter 2B: Input DataFrame is empty.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stage 2 Evaluation & Saving"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Stage 2 Final Metrics ---\n",
                        "Final Review Count (Stage 2): 5377291\n",
                        "Final Unique User Count (Stage 2): 1143079\n",
                        "\n",
                        "--- Comparison: Stage 1 vs Stage 2 ---\n",
                        "Reviews: 5377427 -> 5377291 - Removed in Stage 2: 136\n",
                        "Users: 1143104 -> 1143079 - Removed in Stage 2: 25\n"
                    ]
                }
            ],
            "source": [
                "# Calculate final metrics after Stage 2 filters\n",
                "print(\"\\n--- Stage 2 Final Metrics ---\")\n",
                "if not reviews_stage2_df.empty:\n",
                "    final_review_count_s2 = len(reviews_stage2_df)\n",
                "    final_user_ids_s2 = reviews_stage2_df['user_id'].unique()\n",
                "    final_user_count_s2 = len(final_user_ids_s2)\n",
                "\n",
                "    print(f\"Final Review Count (Stage 2): {final_review_count_s2}\")\n",
                "    print(f\"Final Unique User Count (Stage 2): {final_user_count_s2}\")\n",
                "\n",
                "    print(\"\\n--- Comparison: Stage 1 vs Stage 2 ---\")\n",
                "    print(f\"Reviews: {final_review_count_s1} -> {final_review_count_s2} - Removed in Stage 2: {final_review_count_s1 - final_review_count_s2}\")\n",
                "    print(f\"Users: {final_user_count_s1} -> {final_user_count_s2} - Removed in Stage 2: {final_user_count_s1 - final_user_count_s2}\")\n",
                "else:\n",
                "    print(\"Stage 2 DataFrame is empty. No metrics to calculate.\")\n",
                "    final_review_count_s2 = 0\n",
                "    final_user_count_s2 = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "collapsed": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Qualitative Check (Stage 2) ---\n",
                        "Total kept: 5377291, Total removed in S2: 136\n",
                        "\n",
                        "Sample Removed Reviews (by Stage 2 Filters):\n",
                        "                      review_id                 user_id  stars  \\\n",
                        "309188   bSYueJW2QfnbfCbnwtGj6Q  zq-_GEZbFLKsDuvlJ1t2kA      5   \n",
                        "524267   MKNDWij0YIqpqP8l-iGT0A  bzREcvyIB1n1sozIHbrdNw      1   \n",
                        "2668224  95FSLmXd2KjeRXVLxTfFNQ  _1XrHcVtxeVWQHYVfKHKIQ      5   \n",
                        "19237    Qx1q2UxbsDgGzMo7dpNy0w  Jl4kC3eiGlEgEylXpOj7-Q      3   \n",
                        "1096800  GJiRo6Fn7korFtdNVrYb1g  cBQYOYNa6OtJxN5mZOqDkQ      5   \n",
                        "1454270  E9758dNn_831GI8PuXC_IA  pZQDbsJy9eJG8RyXwFcGNQ      5   \n",
                        "2305602  c3RU9GItoQR70T3hgUAGQw  gLzco4fFq3IFHn562zcKDA      4   \n",
                        "3003841  ZFys_wvnRPaDg3Q9oyNsvA  k0WffTIS1SJx64srck0VsQ      5   \n",
                        "775502   4NGiQ6fj6Mr7KQ9OW-NOXQ  XOjDtIL_Fs5b0Lm0486wTg      1   \n",
                        "2884672  2wf5pwxNcR1UW-S5S6X78Q  kQblyilW-V3k208BJg7mpQ      1   \n",
                        "\n",
                        "                                                      text  \n",
                        "309188   Quick service.  Good food for the price.  Quai...  \n",
                        "524267   If I could give them ZERO stars I would.  It i...  \n",
                        "2668224  It might seem crazy what I'm about to say\\nSun...  \n",
                        "19237    Deep dish pizza.  Deep dish pizza. Deep dish p...  \n",
                        "1096800  Massa has closed, but new restaurant in same l...  \n",
                        "1454270  Awesome sandwiches best in town. Better than p...  \n",
                        "2305602  Great Stylists.  Very Friendly.  I love my new...  \n",
                        "3003841  Cool place to attend!! Especially the art fair...  \n",
                        "775502   Too bad we can't leave zero stars.   Master Mo...  \n",
                        "2884672  I began having issues with my air conditioner ...  \n"
                    ]
                }
            ],
            "source": [
                "# Qualitative Check (Stage 2)\n",
                "print(\"\\n--- Qualitative Check (Stage 2) ---\")\n",
                "if not reviews_stage2_df.empty:\n",
                "    kept_reviews_s2 = reviews_stage2_df\n",
                "    # To find removed reviews, compare Stage 1 saved file vs current df\n",
                "    try:\n",
                "        temp_reviews_s1_df = pd.read_json(os.path.join(output_dir, 'reviews_stage1_cleaned.json'), lines=True)\n",
                "        removed_review_ids_s2 = temp_reviews_s1_df[~temp_reviews_s1_df['review_id'].isin(kept_reviews_s2['review_id'])]['review_id']\n",
                "        removed_reviews_s2 = temp_reviews_s1_df[temp_reviews_s1_df['review_id'].isin(removed_review_ids_s2)]\n",
                "        print(f\"Total kept: {len(kept_reviews_s2)}, Total removed in S2: {len(removed_reviews_s2)}\")\n",
                "        if len(removed_reviews_s2) >= 10:\n",
                "            print(\"\\nSample Removed Reviews (by Stage 2 Filters):\")\n",
                "            print(removed_reviews_s2.sample(min(10, len(removed_reviews_s2)))[['review_id', 'user_id', 'stars', 'text']])\n",
                "        else:\n",
                "            print(\"\\nNot enough reviews removed in Stage 2 to sample.\")\n",
                "        del temp_reviews_s1_df, removed_reviews_s2 # cleanup\n",
                "        gc.collect()\n",
                "    except FileNotFoundError:\n",
                "        print(\"Could not perform removed qualitative check: Stage 1 file not found.\")\n",
                "    except Exception as e:\n",
                "         print(f\"Error during Stage 2 qualitative check: {e}\")\n",
                "else:\n",
                "    print(\"Stage 2 DataFrame is empty. No qualitative check possible.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Saving Stage 2 Data ---\n",
                        "Saved Stage 2 cleaned reviews (5377291) to: ../yelp_dataset/cleaned_data\\reviews_stage2_cleaned.json\n",
                        "Saved Stage 2 cleaned users (1143079) to: ../yelp_dataset/cleaned_data\\users_stage2_cleaned.json\n"
                    ]
                }
            ],
            "source": [
                "# Save Cleaned Data (Stage 2)\n",
                "print(\"\\n--- Saving Stage 2 Data ---\")\n",
                "if not reviews_stage2_df.empty:\n",
                "    # Select columns for output\n",
                "    reviews_stage2_cleaned_df = reviews_stage2_df[review_cols_to_keep_output]\n",
                "\n",
                "    output_reviews_s2_path = os.path.join(output_dir, 'reviews_stage2_cleaned.json')\n",
                "    reviews_stage2_cleaned_df.to_json(output_reviews_s2_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved Stage 2 cleaned reviews ({len(reviews_stage2_cleaned_df)}) to: {output_reviews_s2_path}\")\n",
                "\n",
                "    # Filter Stage 1 users based on remaining user_ids in Stage 2 reviews\n",
                "    remaining_user_ids_s2 = reviews_stage2_cleaned_df['user_id'].unique()\n",
                "    # Use the users_stage1_cleaned_df saved previously\n",
                "    if not users_stage1_cleaned_df.empty:\n",
                "        users_stage2_cleaned_df = users_stage1_cleaned_df[users_stage1_cleaned_df['user_id'].isin(remaining_user_ids_s2)].copy()\n",
                "        output_users_s2_path = os.path.join(output_dir, 'users_stage2_cleaned.json')\n",
                "        users_stage2_cleaned_df.to_json(output_users_s2_path, orient='records', lines=True, date_format='iso')\n",
                "        print(f\"Saved Stage 2 cleaned users ({len(users_stage2_cleaned_df)}) to: {output_users_s2_path}\")\n",
                "    else:\n",
                "        print(\"Warning: Stage 1 users DataFrame was empty, cannot save Stage 2 users.\")\n",
                "        users_stage2_cleaned_df = pd.DataFrame() # Ensure exists\n",
                "\n",
                "    # Clear Stage 2 DataFrames from memory\n",
                "    del reviews_stage2_df\n",
                "    del reviews_stage2_cleaned_df\n",
                "    del users_stage1_cleaned_df # No longer needed\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Stage 2 DataFrame is empty. Cannot save.\")\n",
                "    users_stage2_cleaned_df = pd.DataFrame() # Ensure exists for next phase"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 3: Advanced Filtering (Isolation Forest)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Loading Stage 2 Data for Phase 3 ---\n",
                        "Loaded Stage 2 reviews: (5377427, 9)\n",
                        "Loaded Stage 2 users: (1143104, 22)\n"
                    ]
                }
            ],
            "source": [
                "# Load Stage 2 Data\n",
                "PHASE_3 = False\n",
                "if PHASE_3==False:\n",
                "    print(\"Skipping Phase 3\")\n",
                "else:\n",
                "    print(\"--- Loading Stage 2 Data for Phase 3 ---\")\n",
                "    reviews_stage3_input_df = pd.DataFrame()\n",
                "    users_stage3_input_df = pd.DataFrame()\n",
                "    try:\n",
                "        reviews_stage3_input_path = os.path.join(output_dir, 'reviews_stage1_cleaned.json')\n",
                "        reviews_stage3_input_df = pd.read_json(reviews_stage3_input_path, lines=True)\n",
                "        reviews_stage3_input_df['date'] = pd.to_datetime(reviews_stage3_input_df['date'], errors='coerce') # Ensure date is datetime\n",
                "        print(f\"Loaded Stage 2 reviews: {reviews_stage3_input_df.shape}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: Stage 2 review file not found at {reviews_stage3_input_path}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading Stage 2 reviews: {e}\")\n",
                "\n",
                "    try:\n",
                "        users_stage3_input_path = os.path.join(output_dir, 'users_stage1_cleaned.json')\n",
                "        users_stage3_input_df = pd.read_json(users_stage3_input_path, lines=True)\n",
                "        print(f\"Loaded Stage 2 users: {users_stage3_input_df.shape}\")\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Error: Stage 2 user file not found at {users_stage3_input_path}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading Stage 2 users: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Preparing features for Isolation Forest...\n",
                        "Merged Stage 2 reviews and users: (5377427, 30)\n",
                        "  Recalculating business/user average ratings...\n",
                        "  Adding other features...\n",
                        "  Handling NaNs in features...\n",
                        "Feature preparation complete.\n"
                    ]
                }
            ],
            "source": [
                "# Prepare Features for Isolation Forest\n",
                "if PHASE_3:\n",
                "    print(\"\\nPreparing features for Isolation Forest...\")\n",
                "    features_df = pd.DataFrame()\n",
                "    reviews_stage3_merged_df = pd.DataFrame()\n",
                "\n",
                "    if not reviews_stage3_input_df.empty and not users_stage3_input_df.empty:\n",
                "        # Merge user data back into reviews\n",
                "        reviews_stage3_merged_df = pd.merge(reviews_stage3_input_df, users_stage3_input_df, on='user_id', how='inner', suffixes=('_review', '_user'))\n",
                "        print(f\"Merged Stage 2 reviews and users: {reviews_stage3_merged_df.shape}\")\n",
                "\n",
                "        # Recalculate user_avg_deviation based on current (Stage 2) reviews\n",
                "        print(\"  Recalculating business/user average ratings...\")\n",
                "        # 1. Calculate business avg stars from Stage 2 reviews\n",
                "        business_avg_stars_s2 = reviews_stage3_merged_df.groupby('business_id')['stars'].mean().reset_index(name='business_avg_rating_s2')\n",
                "        # 2. Merge into the merged df\n",
                "        reviews_stage3_merged_df = pd.merge(reviews_stage3_merged_df, business_avg_stars_s2, on='business_id', how='left')\n",
                "        # 3. Calculate review deviation\n",
                "        reviews_stage3_merged_df['rating_deviation_s2'] = (reviews_stage3_merged_df['stars'] - reviews_stage3_merged_df['business_avg_rating_s2']).abs()\n",
                "        # 4. Calculate user average deviation\n",
                "        user_avg_deviation_s2 = reviews_stage3_merged_df.groupby('user_id')['rating_deviation_s2'].mean().reset_index(name='user_avg_deviation_s2')\n",
                "        # 5. Merge user avg deviation back\n",
                "        reviews_stage3_merged_df = pd.merge(reviews_stage3_merged_df, user_avg_deviation_s2, on='user_id', how='left')\n",
                "\n",
                "        # Add other features from user data\n",
                "        print(\"  Adding other features...\")\n",
                "        reviews_stage3_merged_df['text_length'] = reviews_stage3_merged_df['text'].astype(str).str.len()\n",
                "        compliment_cols_s3 = [col for col in reviews_stage3_merged_df.columns if col.startswith('compliment_') and col != 'compliment_count'] # Exclude tip compliment if present\n",
                "        reviews_stage3_merged_df['user_compliments_total'] = reviews_stage3_merged_df[compliment_cols_s3].sum(axis=1)\n",
                "        reviews_stage3_merged_df['user_friends_count'] = reviews_stage3_merged_df['friends'].apply(count_friends) # Use function defined earlier\n",
                "\n",
                "        # Select features for the model\n",
                "        feature_cols = [\n",
                "            'text_length',\n",
                "            'useful_review',\n",
                "            'stars',\n",
                "            'review_count', # User's total review count (from users_df)\n",
                "            'user_compliments_total',\n",
                "            'user_friends_count',\n",
                "            'user_avg_deviation_s2',\n",
                "            'average_stars' # User's average rating (from users_df)\n",
                "        ]\n",
                "\n",
                "        # Ensure all feature columns exist\n",
                "        missing_cols = [col for col in feature_cols if col not in reviews_stage3_merged_df.columns]\n",
                "        if missing_cols:\n",
                "            print(f\"Warning: Missing required feature columns: {missing_cols}. Isolation Forest cannot run.\")\n",
                "        else:\n",
                "            features_df = reviews_stage3_merged_df[feature_cols].copy()\n",
                "\n",
                "            # Handle NaNs (e.g., fill with median or mean)\n",
                "            print(\"  Handling NaNs in features...\")\n",
                "            for col in feature_cols:\n",
                "                if features_df[col].isnull().any():\n",
                "                    median_val = features_df[col].median()\n",
                "                    features_df[col].fillna(median_val, inplace=True)\n",
                "                    print(f\"    Filled NaNs in '{col}' with median ({median_val:.2f})\")\n",
                "\n",
                "            print(\"Feature preparation complete.\")\n",
                "    else:\n",
                "        print(\"Skipping feature preparation: Input DataFrames from Stage 2 are empty or failed to load.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Training Isolation Forest and predicting anomalies...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\wiztu\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
                        "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
                        "C:\\Users\\wiztu\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
                        "  if is_sparse(pd_dtype):\n",
                        "C:\\Users\\wiztu\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
                        "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
                        "C:\\Users\\wiztu\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
                        "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
                        "C:\\Users\\wiztu\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
                        "  if is_sparse(pd_dtype):\n",
                        "C:\\Users\\wiztu\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
                        "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Identified 449400 potential outliers (anomaly_score = -1).\n",
                        "Reviews removed by Isolation Forest: 449400\n",
                        "Users removed: 70117\n",
                        "\n",
                        "Feature Averages (Outliers vs Inliers):\n",
                        "                           Outliers     Inliers\n",
                        "text_length             1200.358104  561.243182\n",
                        "useful_review              5.967488    0.958524\n",
                        "stars                      2.863992    3.948860\n",
                        "review_count             714.281157  102.802536\n",
                        "user_compliments_total  2103.808358   40.022286\n",
                        "user_friends_count       795.661907   88.396274\n",
                        "user_avg_deviation_s2      0.924540    0.874446\n",
                        "average_stars              3.317155    3.889810\n"
                    ]
                }
            ],
            "source": [
                "if PHASE_3:\n",
                "    # Train & Predict with Isolation Forest\n",
                "    reviews_stage3_filtered_df = pd.DataFrame() # Initialize\n",
                "\n",
                "    if not features_df.empty and not reviews_stage3_merged_df.empty:\n",
                "        print(\"\\nTraining Isolation Forest and predicting anomalies...\")\n",
                "        try:\n",
                "            # Initialize model\n",
                "            # contamination='auto' lets the algorithm estimate the proportion of outliers\n",
                "            # Or set explicitly, e.g., contamination=0.01 for 1%\n",
                "            iso_forest = IsolationForest(contamination='auto', random_state=42, n_jobs=-1) # Use all available cores\n",
                "\n",
                "            # Fit and predict\n",
                "            # fit_predict returns 1 for inliers, -1 for outliers\n",
                "            reviews_stage3_merged_df['anomaly_score'] = iso_forest.fit_predict(features_df)\n",
                "\n",
                "            n_outliers = (reviews_stage3_merged_df['anomaly_score'] == -1).sum()\n",
                "            print(f\"Identified {n_outliers} potential outliers (anomaly_score = -1).\")\n",
                "\n",
                "            # Filter out anomalies\n",
                "            count_before = len(reviews_stage3_merged_df)\n",
                "            users_before = reviews_stage3_merged_df['user_id'].nunique()\n",
                "\n",
                "            reviews_stage3_filtered_df = reviews_stage3_merged_df[reviews_stage3_merged_df['anomaly_score'] == 1].copy()\n",
                "\n",
                "            count_after = len(reviews_stage3_filtered_df)\n",
                "            users_after = reviews_stage3_filtered_df['user_id'].nunique()\n",
                "\n",
                "            print(f\"Reviews removed by Isolation Forest: {count_before - count_after}\")\n",
                "            print(f\"Users removed: {users_before - users_after}\")\n",
                "\n",
                "            # Optional: Examine features of outliers vs inliers\n",
                "            outliers_df = reviews_stage3_merged_df[reviews_stage3_merged_df['anomaly_score'] == -1]\n",
                "            if not outliers_df.empty and not reviews_stage3_filtered_df.empty:\n",
                "                print(\"\\nFeature Averages (Outliers vs Inliers):\")\n",
                "                print(pd.DataFrame({'Outliers': outliers_df[feature_cols].mean(), 'Inliers': reviews_stage3_filtered_df[feature_cols].mean()}))\n",
                "            del outliers_df\n",
                "            gc.collect()\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"Error during Isolation Forest execution: {e}\")\n",
                "            # Keep all reviews if IF fails\n",
                "            reviews_stage3_filtered_df = reviews_stage3_merged_df.copy()\n",
                "            if 'anomaly_score' in reviews_stage3_filtered_df.columns:\n",
                "                reviews_stage3_filtered_df.drop(columns=['anomaly_score'], inplace=True)\n",
                "    else:\n",
                "        print(\"Skipping Isolation Forest: No features prepared or input data empty.\")\n",
                "        # If IF was skipped, the 'filtered' df is the 'merged' df from before\n",
                "        if not reviews_stage3_merged_df.empty:\n",
                "            reviews_stage3_filtered_df = reviews_stage3_merged_df.copy()\n",
                "        else:\n",
                "            reviews_stage3_filtered_df = pd.DataFrame() # Ensure it's an empty df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stage 3 Evaluation & Saving (Final)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Stage 3 Final Metrics (Overall Final) ---\n",
                        "Final Review Count (End of Stage 3): 4928027\n",
                        "Final Unique User Count (End of Stage 3): 1072987\n",
                        "Final Unique Business Count (End of Stage 3): 149570\n",
                        "\n",
                        "--- Comparison: Stage 2 vs Stage 3 ---\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'final_review_count_s2' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Unique Business Count (End of Stage 3): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_business_count_s3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Comparison: Stage 2 vs Stage 3 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviews: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfinal_review_count_s2\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_review_count_s3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Removed in Stage 3: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_review_count_s2\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mfinal_review_count_s3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_user_count_s2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_user_count_s3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Removed in Stage 3: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_user_count_s2\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mfinal_user_count_s3\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'final_review_count_s2' is not defined"
                    ]
                }
            ],
            "source": [
                "# Final Evaluation after Stage 3\n",
                "print(\"\\n--- Stage 3 Final Metrics (Overall Final) ---\")\n",
                "if not reviews_stage3_filtered_df.empty:\n",
                "    final_review_count_s3 = len(reviews_stage3_filtered_df)\n",
                "    final_user_ids_s3 = reviews_stage3_filtered_df['user_id'].unique()\n",
                "    final_user_count_s3 = len(final_user_ids_s3)\n",
                "    final_business_ids_s3 = reviews_stage3_filtered_df['business_id'].unique()\n",
                "    final_business_count_s3 = len(final_business_ids_s3)\n",
                "\n",
                "    print(f\"Final Review Count (End of Stage 3): {final_review_count_s3}\")\n",
                "    print(f\"Final Unique User Count (End of Stage 3): {final_user_count_s3}\")\n",
                "    print(f\"Final Unique Business Count (End of Stage 3): {final_business_count_s3}\")\n",
                "\n",
                "    print(\"\\n--- Comparison: Stage 2 vs Stage 3 ---\")\n",
                "    print(f\"Reviews: {final_review_count_s2} -> {final_review_count_s3} - Removed in Stage 3: {final_review_count_s2 - final_review_count_s3}\")\n",
                "    print(f\"Users: {final_user_count_s2} -> {final_user_count_s3} - Removed in Stage 3: {final_user_count_s2 - final_user_count_s3}\")\n",
                "else:\n",
                "    print(\"Stage 3 DataFrame is empty. No final metrics.\")\n",
                "    final_review_count_s3 = 0\n",
                "    final_user_count_s3 = 0\n",
                "    final_business_count_s3 = 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Saving Final Cleaned Data ---\n",
                        "Stage 3 DataFrame is empty. Cannot save final files.\n"
                    ]
                }
            ],
            "source": [
                "# Save Final Cleaned Data\n",
                "print(\"\\n--- Saving Final Cleaned Data ---\")\n",
                "\n",
                "if not reviews_stage3_filtered_df.empty:\n",
                "    # Final Reviews\n",
                "    reviews_final_cleaned_df = reviews_stage3_filtered_df[review_cols_to_keep_output].copy() # Select original review columns\n",
                "    output_reviews_final_path = os.path.join(output_dir, 'reviews_final_cleaned.json')\n",
                "    reviews_final_cleaned_df.to_json(output_reviews_final_path, orient='records', lines=True, date_format='iso')\n",
                "    print(f\"Saved final cleaned reviews ({len(reviews_final_cleaned_df)}) to: {output_reviews_final_path}\")\n",
                "\n",
                "    # Final Users (Filter the users loaded from Stage 2)\n",
                "    if not users_stage3_input_df.empty:\n",
                "        users_final_cleaned_df = users_stage3_input_df[users_stage3_input_df['user_id'].isin(final_user_ids_s3)].copy()\n",
                "        output_users_final_path = os.path.join(output_dir, 'users_final_cleaned.json')\n",
                "        users_final_cleaned_df.to_json(output_users_final_path, orient='records', lines=True, date_format='iso')\n",
                "        print(f\"Saved final cleaned users ({len(users_final_cleaned_df)}) to: {output_users_final_path}\")\n",
                "    else:\n",
                "        print(\"Warning: Stage 2 users input was empty, cannot save final users.\")\n",
                "        users_final_cleaned_df = pd.DataFrame()\n",
                "\n",
                "    # Final Businesses (Filter the original business_df)\n",
                "    if not business_df.empty:\n",
                "        business_final_cleaned_df = business_df[business_df['business_id'].isin(final_business_ids_s3)].copy()\n",
                "        output_business_final_path = os.path.join(output_dir, 'business_final_cleaned.json')\n",
                "        business_final_cleaned_df.to_json(output_business_final_path, orient='records', lines=True, date_format='iso')\n",
                "        print(f\"Saved final cleaned businesses ({len(business_final_cleaned_df)}) to: {output_business_final_path}\")\n",
                "    else:\n",
                "        print(\"Warning: Original business_df was empty, cannot save final businesses.\")\n",
                "        business_final_cleaned_df = pd.DataFrame()\n",
                "\n",
                "    # Final Memory Cleanup\n",
                "    del reviews_stage3_input_df\n",
                "    del users_stage3_input_df\n",
                "    del reviews_stage3_merged_df\n",
                "    del reviews_stage3_filtered_df\n",
                "    del features_df\n",
                "    del reviews_final_cleaned_df\n",
                "    del users_final_cleaned_df\n",
                "    del business_final_cleaned_df\n",
                "    del users_df # Original users no longer needed\n",
                "    del business_df # Original business no longer needed\n",
                "    gc.collect()\n",
                "else:\n",
                "    print(\"Stage 3 DataFrame is empty. Cannot save final files.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "3-day burst percentiles:\n",
                        " 90%    4.0\n",
                        "95%    5.0\n",
                        "98%    6.0\n",
                        "99%    7.0\n",
                        "Name: count, dtype: float64\n",
                        "\n",
                        "5-day burst percentiles:\n",
                        " 90%     6.0\n",
                        "95%     7.0\n",
                        "98%     9.0\n",
                        "99%    10.0\n",
                        "Name: count, dtype: float64\n",
                        "\n",
                        "7-day burst percentiles:\n",
                        " 90%     9.0\n",
                        "95%     9.0\n",
                        "98%    11.0\n",
                        "99%    13.0\n",
                        "Name: count, dtype: float64\n",
                        "\n",
                        "→ Using K_BURST=3, N_BURST=7\n"
                    ]
                },
                {
                    "ename": "KeyError",
                    "evalue": "'review_count'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
                        "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
                        "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
                        "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
                        "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
                        "\u001b[1;31mKeyError\u001b[0m: 'review_count'",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[16], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 5) baseline “slow biz” guard: require long-term mean < 1/day\u001b[39;00m\n\u001b[0;32m     42\u001b[0m biz_mean \u001b[38;5;241m=\u001b[39m daily\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m---> 43\u001b[0m slow_biz \u001b[38;5;241m=\u001b[39m biz_mean[\u001b[43mbiz_mean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 6) final drop set\u001b[39;00m\n\u001b[0;32m     46\u001b[0m to_drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(flagged_biz)\u001b[38;5;241m.\u001b[39mintersection(slow_biz)\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
                        "\u001b[1;31mKeyError\u001b[0m: 'review_count'"
                    ]
                }
            ],
            "source": [
                "# Business burst goes here\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 4: Post-Cleaning Summary"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Summary of Cleaning Process (Chunking Refactor):**\n",
                "\n",
                "*   **Initial Data:** Started with Y users, Z businesses (Review count processed in chunks).\n",
                "*   **Chunk Processing:** Reviews loaded in chunks. Merged with users, feature engineered, and filtered by text length (A), low usefulness (F), and URLs/Emails (2A) within each chunk.\n",
                "*   **Combined Data (Baseline):** `baseline_review_count` reviews and `baseline_user_count` users remaining after combining chunks.\n",
                "*   **After Stage 1 (User Aggregation Rules):** Reduced to `final_review_count_s1` reviews and `final_user_count_s1` users.\n",
                "    *   Removed users/reviews based on burst rate (B), short activity windows (C), high rating deviation (D, non-elite), and low engagement (E).\n",
                "*   **After Stage 2 (Text Heuristics):** Reduced to `final_review_count_s2` reviews and `final_user_count_s2` users.\n",
                "    *   Removed reviews based on text repetition (B, optional/if NLTK present).\n",
                "*   **After Stage 3 (Isolation Forest):** Reduced to `final_review_count_s3` reviews and `final_user_count_s3` users.\n",
                "    *   Removed reviews identified as anomalies based on selected features.\n",
                "\n",
                "**Final Output:**\n",
                "\n",
                "*   `reviews_final_cleaned.json`: Contains `final_review_count_s3` cleaned reviews.\n",
                "*   `users_final_cleaned.json`: Contains `final_user_count_s3` users corresponding to the final reviews.\n",
                "*   `business_final_cleaned.json`: Contains `final_business_count_s3` businesses corresponding to the final reviews.\n",
                "\n",
                "These files are located in the `yelp_dataset/cleaned_data/` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Cleaning Process Complete ---\n",
                        "Final review count: 0\n",
                        "Final user count: 0\n",
                        "Final business count: 0\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'output_dir' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal user count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_user_count_s3\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_user_count_s3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal business count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_business_count_s3\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_business_count_s3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned files saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'output_dir' is not defined"
                    ]
                }
            ],
            "source": [
                "print(\"\\n--- Cleaning Process Complete ---\")\n",
                "print(f\"Final review count: {final_review_count_s3 if 'final_review_count_s3' in locals() else 'N/A'}\")\n",
                "print(f\"Final user count: {final_user_count_s3 if 'final_user_count_s3' in locals() else 'N/A'}\")\n",
                "print(f\"Final business count: {final_business_count_s3 if 'final_business_count_s3' in locals() else 'N/A'}\")\n",
                "print(f\"Cleaned files saved in: {output_dir}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
